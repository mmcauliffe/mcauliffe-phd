@article{Ahissar1993,
abstract = {The performance of adult humans in simple visual tasks improves dramatically with practice. This improvement is highly specific to basic attributes of the trained stimulus, suggesting that the underlying changes occur at low-level processing stages in the brain, where different orientations and spatial frequencies are handled by separate channels. We asked whether these practice effects are determined solely by activity in stimulus-driven mechanisms or whether high-level attentional mechanisms, which are linked to the perceptual task, might control the learning process. We found that practicing one task did not improve performance in an alternative task, even though both tasks used exactly the same visual stimuli but depended on different stimulus attributes (either orientation of local elements or global shape). Moreover, even when the experiment was designed so that the same responses were associated with the same stimuli (although subjects were instructed to attend to the attribute underlying one task), learning did not transfer from one task to the other. These results suggest that specific high-level attentional mechanisms, controlling changes at early visual processing levels, are essential in perceptual learning.},
author = {Ahissar, Merav and Hochstein, Shaul},
doi = {10.1073/pnas.90.12.5718},
isbn = {0027-8424 1091-6490},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Adult,Attention,Brain,Brain: physiology,Humans,Learning,Orientation,Space Perception,Visual Perception},
number = {12},
pages = {5718--22},
pmid = {8516322},
title = {{Attentional control of early perceptual learning.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=46793\&tool=pmcentrez\&rendertype=abstract},
volume = {90},
year = {1993}
}
@article{Bacon1994,
abstract = {Theeuwes (1992) found a distracting effect of irrelevant-dimension singletons in a task involving search for a known target. He argued from this that selectivity is determined solely by stimulus salience; the parallel stage of visual processing cannot provide top-down guidance to the attentive stage sufficient to permit completely selective use of task-relevant information. We argue that in the task used by Theeuwes, subjects may have adopted the strategy of searching for an odd form even though the specific target form was known. In Experiment 1, we replicated Theeuwes's findings. Search for a circle target among diamond nontargets was disrupted by the presence of a diamond nontarget that was uniquely colored. In two subsequent experiments, we discouraged the singleton detection strategy, forcing subjects to search for the target feature. There was no distracting effect of a color singleton in these experiments, even with displays physically identical to those of Experiment 1, demonstrating that top-down selectivity is indeed possible during visual search. We conclude that goal-directed selection of a specific known featural identity may override stimulus-driven capture by salient featural singletons.},
author = {Bacon, W F and Egeth, H E},
institution = {Johns Hopkins University, Baltimore, Maryland.},
issn = {0031-5117},
journal = {Perception \& psychophysics},
number = {5},
pages = {485--496},
pmid = {8008550},
title = {{Overriding stimulus-driven attentional capture.}},
volume = {55},
year = {1994}
}
@phdthesis{Beckman1998,
abstract = {There are a variety of phonological asymmetries exhibited by segments which appear in perceptually or psycholinguistically prominent positions such as roots, root-initial syllables, stressed syllables, and syllable onsets. In such positions, segmental or featural contrasts are often maintained, though they may be neutralized in non-prominent positions. Segments in prominent positions frequently trigger phonological processes such as assimilation, dissimilation and vowel harmony; conversely, they often block or resist the application of these processes. The goal of this dissertation is to develop a theory of positional faithfulness which will both generate and explain the range of positional asymmetries attested in natural language phonology. Chapter 1 introduces the notion of positional privilege, as well as the fundamental aspects of Optimality Theory. Positional faithfulness constraints are introduced and demonstrated in an analysis of onset/coda asymmetries in Catalan voice assimilation. I argue that positional faithfulness provides an explanation for the attested onset/coda asymmetries that is not afforded by licensing alternatives. Faithfulness in root-initial syllables, a position in which prominence derives largely from psycholinguistic (rather than phonetic) properties, is considered in Chapter 2. Particular attention is given to the analysis of vowel harmony in Shona, and to the phonology of consonantal place in Tamil. Chapter 3 is devoted to the domain of stress, showing once again that positional faithfulness constraints unify and explain a wide range of phonological asymmetries associated with the positional prominence. The core of the chapter is an analysis of nasal harmony in Guarani; vowel reduction in Catalan is also examined. In Chapter 4, I turn to positional privilege effects which are sensitive to the distinction between root and affix. Such cases provide further support for positional faithfulness theory. Finally, in Chapter 5, a different type of positional faithfulness effect, that of positional maximization, is examined. I argue that constraints which favor maximal packing of prominent constituents are necessary. Such constraints are crucial in cases of prominence-driven ambisyllabicity, as in Ibibio. Positional MAX constraints also account for the appearance of complex syllable margins in prominent positions, though complex margins may be excluded elsewhere in the language.},
author = {Beckman, Jill N},
booktitle = {Optimality Theory in Phonology},
chapter = {16},
editor = {McCarthy, John J},
issn = {09526757},
number = {February},
pages = {1995--1995},
publisher = {University of Massachusetts, Amherst},
school = {University of Massachusetts Amherst},
title = {{Positional Faithfulness}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/9780470756171.ch16/summary},
volume = {14},
year = {1998}
}
@article{Bertelson2003,
abstract = {The kinds of aftereffects, indicative of cross-modal recalibration, that are observed after exposure to spatially incongruent inputs from different sensory modalities have not been demonstrated so far for identity incongruence. We show that exposure to incongruent audiovisual speech (producing the well-known McGurk effect) can recalibrate auditory speech identification. In Experiment 1, exposure to an ambiguous sound intermediate between /aba/ and /ada/ dubbed onto a video of a face articulating either /aba/ or /ada/ increased the proportion of /aba/ or /ada/ responses, respectively, during subsequent sound identification trials. Experiment 2 demonstrated the same recalibration effect or the opposite one, fewer /aba/ or /ada/ responses, revealing selective speech adaptation, depending on whether the ambiguous sound or a congruent nonambiguous one was used during exposure. In separate forced-choice identification trials, bimodal stimulus pairs producing these contrasting effects were identically categorized, which makes a role of postperceptual factors in the generation of the effects unlikely.},
author = {Bertelson, Paul and Vroomen, Jean and {De Gelder}, B\'{e}atrice},
journal = {Psychological Science},
number = {6},
pages = {592--597},
pmid = {14629691},
title = {{Visual Recalibration of Auditory Speech Identification: A McGurk Aftereffect}},
volume = {14},
year = {2003}
}
@article{Block2013,
abstract = {Clark advertises the predictive coding (PC) framework as applying to a wide range of phenomena, including attention. We argue that for many attentional phenomena, the predictive coding picture either makes false predictions, or else it offers no distinctive explanation of those phenomena, thereby reducing its explanatory power.},
author = {Block, Ned and Siegel, Susanna},
issn = {1469-1825},
journal = {The Behavioral and brain sciences},
keywords = {Attention,Attention: physiology,Brain,Brain: physiology,Cognition,Cognition: physiology,Cognitive Science,Cognitive Science: trends,Humans,Perception,Perception: physiology},
number = {3},
pages = {205--6},
pmid = {23663745},
title = {{Attention and perceptual adaptation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23663745},
volume = {36},
year = {2013}
}
@article{Borsky2000,
abstract = {The purpose of this study was to investigate the temporal unfolding of local acoustic information and sentence context using both cross-modal interference (CMI) and word-monitoring tasks. The timing of sentence context effects have important theoretical implications for models of language processing (e.g., initial context independence vs. initial interaction). Yet, different tasks tend to yield different results. For both experiments, stimuli from an acoustically manipulated "goat-to-coat" continuum were embedded in sentences whose interpretation was biased toward either "goat" or "coat." In experiment 1 (CMI), the primary task was listening to sentences for comprehension; the interference task was a word/nonword decision to an unrelated visual probe that appeared at one of three positions within the sentence. Results showed immediate effects of the acoustic manipulation, but only delayed effects of sentence context. These results were interpreted to indicate that phonological processing is initially context-independent but is followed by rapid context integration. Experiment 2 used a word-monitoring task: Response times were significantly longer when sentence context was incongruent with the monitoring target, showing an immediate effect of context. The apparently contradictory results of the two experiments together support an account of language processing in which phoneme categorization is initially independent of sentence context unless an explicit judgment about the identity of the target is required.},
author = {Borsky, S and Shapiro, L P and Tuller, B},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Borsky, Shapiro, Tuller - 2000 - The temporal unfolding of local acoustic information and sentence context.pdf:pdf},
institution = {University of North Florida, Department of Psychology, Jacksonville 32224, USA. sborsky@unf.edu},
journal = {Journal of Psycholinguistic Research},
keywords = {adult,humans,phonetics,random allocation,reaction time,speech,speech acoustics,speech perception,speech perception physiology,speech physiology},
number = {2},
pages = {155--168},
publisher = {Springer},
title = {{The temporal unfolding of local acoustic information and sentence context}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10709181},
volume = {29},
year = {2000}
}
@article{Borsky1998,
abstract = {This study examined the effect of sentence context and local acoustic structure on phoneme categorization. Target stimuli from a 10-step GOAT-COAT continuum, differing only on a temporal cue for voice onset time (VOT), were embedded in carrier sentences that biased interpretation toward either "goat" or "coat." While subjects listened to the sentences they also responded as quickly as possible to a visual probe by indicating whether the probe matched the target stimulus they heard. Results showed that the interaction of VOT and sentence context significantly affected both identification and RT for stimuli near the perceptual boundary; the identification function showed a boundary shift in favor of the biased context and peak response times for each context reflected the shifted identification boundaries. In addition, response times were faster for identification of stimuli near the category boundary when responses were congruent, rather than incongruent with the sentence context. The response time differences for congruent versus incongruent responses in the boundary region are interpreted as depending on the results of initial phonological analysis; potentially ambiguous categorizations may be subject to additional evaluation in which a context-congruent response is both preferred and available earlier.},
author = {Borsky, S and Tuller, B and Shapiro, L P},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Borsky, Tuller, Shapiro - 1998 - How to milk a coat the effects of semantic and acoustic information on phoneme categorization.pdf:pdf},
institution = {Program in Complex Systems and Brain Sciences, Florida Atlantic University, Boca Raton 33431, USA.},
journal = {Journal of the Acoustical Society of America},
number = {5 Pt 1},
pages = {2670--2676},
title = {{"How to milk a coat:" the effects of semantic and acoustic information on phoneme categorization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9604360},
volume = {103},
year = {1998}
}
@article{Bradlow2008,
abstract = {This study investigated talker-dependent and talker-independent perceptual adaptation to foreign-accent English. Experiment 1 investigated talker-dependent adaptation by comparing native English listeners' recognition accuracy for Chinese-accented English across single and multiple talker presentation conditions. Results showed that the native listeners adapted to the foreign-accented speech over the course of the single talker presentation condition with some variation in the rate and extent of this adaptation depending on the baseline sentence intelligibility of the foreign-accented talker. Experiment 2 investigated talker-independent perceptual adaptation to Chinese-accented English by exposing native English listeners to Chinese-accented English and then testing their perception of English produced by a novel Chinese-accented talker. Results showed that, if exposed to multiple talkers of Chinese-accented English during training, native English listeners could achieve talker-independent adaptation to Chinese-accented English. Taken together, these findings provide evidence for highly flexible speech perception processes that can adapt to speech that deviates substantially from the pronunciation norms in the native talker community along multiple acoustic-phonetic dimensions. Â© 2007 Elsevier B.V. All rights reserved.},
author = {Bradlow, Ann R. and Bent, Tessa},
journal = {Cognition},
keywords = {Foreign-accented speech,Perceptual learning,Speech perception},
number = {2},
pages = {707--729},
title = {{Perceptual adaptation to non-native speech}},
volume = {106},
year = {2008}
}
@article{Bradlow2007,
abstract = {Previous research has shown that speech recognition differences between native and proficient non-native listeners emerge under suboptimal conditions. Current evidence has suggested that the key deficit that underlies this disproportionate effect of unfavorable listening conditions for non-native listeners is their less effective use of compensatory information at higher levels of processing to recover from information loss at the phoneme identification level. The present study investigated whether this non-native disadvantage could be overcome if enhancements at various levels of processing were presented in combination. Native and non-native listeners were presented with English sentences in which the final word varied in predictability and which were produced in either plain or clear speech. Results showed that, relative to the low-predictability-plain-speech baseline condition, non-native listener final word recognition improved only when both semantic and acoustic enhancements were available (high-predictability-clear-speech). In contrast, the native listeners benefited from each source of enhancement separately and in combination. These results suggests that native and non-native listeners apply similar strategies for speech-in-noise perception: The crucial difference is in the signal clarity required for contextual information to be effective, rather than in an inability of non-native listeners to take advantage of this contextual information per se.},
author = {Bradlow, Ann R and Alexander, Jennifer a},
doi = {10.1121/1.2642103},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bradlow, Alexander - 2007 - Semantic and phonetic enhancements for speech-in-noise recognition by native and non-native listeners.pdf:pdf},
institution = {Department of Linguistics, Northwestern University, Evanston, Illinois 60208, USA. abradlow@northwestern.edu},
issn = {00014966},
journal = {Journal of the Acoustical Society of America},
number = {4},
pages = {2339--2349},
publisher = {Asa},
title = {{Semantic and phonetic enhancements for speech-in-noise recognition by native and non-native listeners.}},
url = {http://link.aip.org/link/JASMAN/v121/i4/p2339/s1\&Agg=doi},
volume = {121},
year = {2007}
}
@article{Brysbaert2009,
abstract = {Word frequency is the most important variable in research on word processing and memory. Yet, the main criterion for selecting word frequency norms has been the availability of the measure, rather than its quality. As a result, much research is still based on the old Kucera and Francis frequency norms. By using the lexical decision times of recently published megastudies, we show how bad this measure is and what must be done to improve it. In particular, we investigated the size of the corpus, the language register on which the corpus is based, and the definition of the frequency measure. We observed that corpus size is of practical importance for small sizes (depending on the frequency of the word), but not for sizes above 16-30 million words. As for the language register, we found that frequencies based on television and film subtitles are better than frequencies based on written sources, certainly for the monosyllabic and bisyllabic words used in psycholinguistic research. Finally, we found that lemma frequencies are not superior to word form frequencies in English and that a measure of contextual diversity is better than a measure based on raw frequency of occurrence. Part of the superiority of the latter is due to the words that are frequently used as names. Assembling a new frequency norm on the basis of these considerations turned out to predict word processing times much better than did the existing norms (including Kucera \& Francis and Celex). The new SUBTL frequency norms from the SUBTLEX(US) corpus are freely available for research purposes from http://brm.psychonomic-journals.org/content/supplemental, as well as from the University of Ghent and Lexique Web sites.},
author = {Brysbaert, Marc and New, Boris},
institution = {Ghent University, Ghent, Belgium. marc.brysbaert@ugent.be},
issn = {1554-3528},
journal = {Behavior research methods},
number = {4},
pages = {977--990},
pmid = {19897807},
title = {{Moving beyond Kucera and Francis: a critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English.}},
volume = {41},
year = {2009}
}
@misc{Clare2014,
author = {Clare, Emily},
title = {{Applying phonological knowledge to phonetic accommodation}},
year = {2014}
}
@article{Clark2013,
abstract = {Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this "hierarchical prediction machine" approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.},
author = {Clark, Andy},
issn = {1469-1825},
journal = {The Behavioral and brain sciences},
keywords = {Attention,Attention: physiology,Brain,Brain: physiology,Cognition,Cognition: physiology,Cognitive Science,Cognitive Science: trends,Humans,Learning,Learning: physiology,Models, Neurological,Perception,Perception: physiology},
number = {3},
pages = {181--204},
pmid = {23663408},
title = {{Whatever next? Predictive brains, situated agents, and the future of cognitive science.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23663408},
volume = {36},
year = {2013}
}
@article{Clarke2004,
abstract = {This study explored the perceptual benefits of brief exposure to non-native speech. Native English listeners were exposed to English sentences produced by non-native speakers. Perceptual processing speed was tracked by measuring reaction times to visual probe words following each sentence. Three experiments using Spanish- and Chinese-accented speech indicate that processing speed is initially slower for accented speech than for native speech but that this deficit diminishes within one minute of exposure. Control conditions rule out explanations for the adaptation effect based on practice with the task and general strategies for dealing with difficult speech. Further results suggest that adaptation can occur within as few as two to four sentence-length utterances. The findings emphasize the flexibility of human speech processing and require models of spoken word recognition that can rapidly accommodate significant acoustic-phonetic deviations from native language speech patterns.},
author = {Clarke, Constance M and Garrett, Merrill F},
institution = {Department of Psychology, University of Arizona, Tucson, Arizona 85721, USA. cclarke2@buffalo.edu},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {6},
pages = {3647--3658},
pmid = {15658715},
title = {{Rapid adaptation to foreign-accented English.}},
volume = {116},
year = {2004}
}
@article{Clarke-Davidson2008,
abstract = {Recent studies show that perceptual boundaries between phonetic categories are changeable with training (Norris, McQueen, \& Cutler, 2003). For example, Kraljic and Samuel (2005) exposed listeners in a lexical decision task to ambiguous /s-integral/ sounds in either s-word contexts (e.g., legacy) or integral-word contexts (e.g., parachute). In a subsequent /s/-/integral/ categorization test, listeners in the /s/ condition categorized more tokens as /s/ than did those in the /integral/ condition. The effect--termed perceptual learning in speech--is assumed to reflect a change in phonetic category representation. However, the result could be due to a decision bias resulting from the training task. In Experiment 1, we replicated the basic Kraljic and Samuel (2005) experiment and added an AXB discrimination test. In Experiment 2, we used a task that is less likely to induce a decision bias. Results of both experiments and signal detection analyses point to a true change in phonetic representation.},
author = {Clarke-Davidson, Constance M and Luce, Paul A and Sawusch, James R},
institution = {University at Buffalo, State University of New York, Buffalo, New York, USA. cmclarke@ualberta.ca},
issn = {0031-5117},
journal = {Perception \& psychophysics},
number = {4},
pages = {604--618},
pmid = {18556922},
title = {{Does perceptual learning in speech reflect changes in phonetic category representation or decision bias?}},
volume = {70},
year = {2008}
}
@article{Clopper2012,
abstract = {Words in semantically predictable sentences are more intelligible than words in less predictable or semantically anomalous sentences. However, the intelligibility benefit of semantically predictable words is reduced in noise and for non-native listeners, suggesting that semantic context contributes less to intelligibility under difficult listening conditions. The goal of the current study was to explore the effect of dialect variation on the semantic predictability benefit. Larger semantic predictability benefits were observed for more familiar dialects than less familiar dialects. In addition, more dialect differences in the size of the semantic predictability benefit were observed in conditions in which perceptual normalisation was more difficult. These results are consistent with a cue-weighting model in which dialect variation is comparable to energetic noise masking. Listeners attend less to semantic cues when perceptual normalisation for dialect variation is difficult and more to semantic cues when perceptual normalisation is easy. Words in semantically predictable sentences are more intelligible than words in less predictable or semantically anomalous sentences. However, the intelligibility benefit of semantically predictable words is reduced in noise and for non-native listeners, suggesting that semantic context contributes less to intelligibility under difficult listening conditions. The goal of the current study was to explore the effect of dialect variation on the semantic predictability benefit. Larger semantic predictability benefits were observed for more familiar dialects than less familiar dialects. In addition, more dialect differences in the size of the semantic predictability benefit were observed in conditions in which perceptual normalisation was more difficult. These results are consistent with a cue-weighting model in which dialect variation is comparable to energetic noise masking. Listeners attend less to semantic cues when perceptual normalisation for dialect variation is difficult and more to semantic cues when perceptual normalisation is easy.},
author = {Clopper, Cynthia G},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clopper - 2012 - Effects of dialect variation on the semantic predictability benefit.pdf:pdf},
journal = {Language \& Cognitive Processes},
number = {7-8},
pages = {1002--1020},
publisher = {Psychology Press},
title = {{Effects of dialect variation on the semantic predictability benefit}},
url = {http://dx.doi.org/10.1080/01690965.2011.558779},
volume = {27},
year = {2012}
}
@article{Clopper2008,
abstract = {This study explored the interaction between semantic predictability and regional dialect variation in an analysis of speech produced by college-aged female talkers from the Northern, Midland, and Southern dialects of American English. Previous research on the effects of semantic predictability has shown that vowels in high semantic predictability contexts are temporally and spectrally reduced compared to vowels in low semantic predictability contexts. In the current study, an analysis of vowel duration confirmed temporal reduction in the high predictability condition. An analysis of vowel formant structure and vowel space dispersion revealed overall spectral reduction for the Southern talkers. For the Northern talkers, more extreme Northern Cities shifting occurred in the high predictability condition than in the low predictability condition. No effects of semantic predictability were observed for the Midland talkers. These findings suggest an interaction between semantic and indexical factors in vowel reduction processes.},
author = {Clopper, Cynthia G and Pierrehumbert, Janet B},
doi = {10.1121/1.2953322},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clopper, Pierrehumbert - 2008 - Effects of semantic predictability and regional dialect on vowel space reduction.pdf:pdf},
institution = {Department of Linguistics, Northwestern University, Evanston, Illinois 60208, USA.},
issn = {1520-8524},
journal = {Journal of the Acoustical Society of America},
keywords = {Adolescent,Adult,Female,Humans,Residence Characteristics,Semantics,Speech Acoustics,Speech Intelligibility,Speech Perception,Speech Production Measurement,Time Factors,United States,Young Adult},
month = sep,
number = {3},
pages = {1682--1688},
pmid = {19045658},
publisher = {Acoustical Society of America},
title = {{Effects of semantic predictability and regional dialect on vowel space reduction}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2676620\&tool=pmcentrez\&rendertype=abstract},
volume = {124},
year = {2008}
}
@article{Connine1994,
abstract = {Four experiments investigated acoustic-phonetic similarity in the mapping process between the speech signal and lexical representations (vertical similarity). Auditory stimuli were used where ambiguous initial phonemes rendered a phoneme sequence lexically ambiguous (perceptual-lexical ambiguities). A cross-modal priming paradigm (Experiments 1, 2, and 3) showed facilitation for targets related to both interpretations of the ambiguities, indicating multiple activation. Experiment 4 investigated individual differences and the role of sentence context in vertical similarity mapping. The results support a model where spoken word recognition proceeds via goodness-of-fit mapping between speech and lexical representations that is not influenced by sentence context.},
author = {Connine, C M and Blasko, D G and Wang, J},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Connine, Blasko, Wang - 1994 - Vertical similarity in spoken word recognition multiple lexical activation, individual differences, and t.pdf:pdf},
institution = {State University of New York, Department of Psychology, Binghamton 13901.},
journal = {Perception And Psychophysics},
keywords = {connine bilingual},
number = {6},
pages = {624--636},
publisher = {Springer},
title = {{Vertical similarity in spoken word recognition: multiple lexical activation, individual differences, and the role of sentence context.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7816533},
volume = {56},
year = {1994}
}
@article{Connine1987a,
abstract = {Two experiments are reported that demonstrate contextual effects on identification of speech voicing continua. Experiment 1 demonstrated the influence of lexical knowledge on identification of ambiguous tokens from word-nonword and nonword-word continua. Reaction times for word and non-word responses showed a word advantage only for ambiguous stimulus tokens (at the category boundary); no word advantage was found for clear stimuli (at the continua endpoints). Experiment 2 demonstrated an effect of a postperceptual variable, monetary payoff, on nonword-nonword continua. Identification responses were influenced by monetary payoff, but reaction times for bias-consistent and bias-inconsistent responses did not differ at the category boundary. An advantage for bias-consistent responses was evident at the continua endpoints. The contrasting patterns of reaction-time data in the two experiments indicate different underlying mechanisms. We argue that the lexical status effect is attributable to a mechanism in which lexical knowledge directly influences perceptual processes.},
author = {Connine, C M and Clifton, C},
issn = {0096-1523},
journal = {Journal of experimental psychology. Human perception and performance},
number = {2},
pages = {291--299},
pmid = {2953858},
title = {{Interactive use of lexical information in speech perception.}},
volume = {13},
year = {1987}
}
@article{Connine1987,
author = {Connine, CM},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Connine - 1987 - Constraints on interactive processes in auditory word recognition The role of sentence context.pdf:pdf},
journal = {Journal of Memory and Language},
pages = {527--538},
title = {{Constraints on interactive processes in auditory word recognition: The role of sentence context}},
url = {http://www.sciencedirect.com/science/article/pii/0749596X87901380},
volume = {538},
year = {1987}
}
@article{Connine1991,
abstract = {Acoustic-phonetic ambiguity in word initial position was investigated in three experiments in which the ambiguous string was compatible with two lexical items. Perceptual/lexical ambiguities were embedded in sentences in which contextual information subsequent to the item served to bias the interpretation of the ambiguous string. Two experiments are reported that investigated temporal constraints on perceptual/lexical ambiguity resolution. The results suggest that commitment to a single lexical hypothesis occurs within a relatively small window of time. When information relevant to selection of the contextually appropriate word is not forthcoming within six syllables, commitment to a single lexical hypothesis is delayed but proceeds without benefit from sentence context. The results of Experiment 3 suggest that in addition to temporal constraints, a short independent clause promotes early commitment to a single lexical hypothesis. The findings are discussed in terms of assumptions of current auditory word recognition models.},
author = {Connine, Cynthia M and Blasko, Dawn G and Hall, Michael},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Connine, Blasko, Hall - 1991 - Effects of subsequent sentence context in auditory word recognition Temporal and linguistic constrainst.pdf:pdf},
journal = {Journal of Memory and Language},
number = {2},
pages = {234--250},
title = {{Effects of subsequent sentence context in auditory word recognition: Temporal and linguistic constrainst}},
url = {http://www.sciencedirect.com/science/article/B6WK4-4D7006D-5/2/52b72f93401eef7c379618e374ece032},
volume = {30},
year = {1991}
}
@misc{Cutler1987,
abstract = {In seven experiments reaction time to detect the initial phoneme of words and nonwords was measured. Reaction time advantages for words over nonwords come and go according to the particular characteristics of the experimental situation. One relevant characteristic is degree of task monotony, an effect which is most parsimoniously explained by attention shifting between levels of processing. General classes of models of the relationship between levels of processing in comprehension are discussed in light of the results. Serial models incorporate an attention shift explanation of the monotony effect more elegantly than do interactive models. Alternative serial models are available in the literature in this area. One recent model, which allows only a single outlet point for phoneme detection responses, and hence requires that apparent reaction time advantages for words are artefactual, can be unambiguously rejected on the basis of the present data. It is argued that a serial model involving competition between target detection based on a prelexical representation and detection based on a lexical representation most satisfactorily accounts for the overall pattern of results.},
author = {Cutler, Anne and Mehler, Jacques and Norris, Dennis and Segui, Juan},
booktitle = {Cognitive Psychology},
issn = {00100285},
number = {2},
pages = {141--177},
title = {{Phoneme identification and the lexicon}},
volume = {19},
year = {1987}
}
@article{Dahan2008,
abstract = {Past research has established that listeners can accommodate a wide range of talkers in understanding language. How this adjustment operates, however, is a matter of debate. Here, listeners were exposed to spoken words from a speaker of an American English dialect in which the vowel /??/ is raised before /g/, but not before /k/. Results from two experiments showed that listeners' identification of /k/-final words like back (which are unaffected by the dialect) was facilitated by prior exposure to their dialect-affected /g/-final counterparts, e.g., bag. This facilitation occurred because the competition between interpretations, e.g., bag or back, while hearing the initial portion of the input [b??], was mitigated by the reduced probability for the input to correspond to bag as produced by this talker. Thus, adaptation to an accent is not just a matter of adjusting the speech signal as it is being heard; adaptation involves dynamic adjustment of the representations stored in the lexicon, according to the characteristics of the speaker or the context. ?? 2008 Elsevier B.V. All rights reserved.},
author = {Dahan, Delphine and Drucker, Sarah J. and Scarborough, Rebecca A.},
journal = {Cognition},
keywords = {Eye movements,Speaker adaptation,Speech perception},
number = {3},
pages = {710--718},
title = {{Talker adaptation in speech perception: Adjusting the signal or the representations?}},
volume = {108},
year = {2008}
}
@phdthesis{Dilts2013,
author = {Dilts, Philip C},
school = {University of Alberta},
title = {{Modelling Phonetic Reduction in a Corpus of Spoken English Using Random Forests and Mixed-effects Regression}},
year = {2013}
}
@article{Egner2010,
abstract = {Visual cortex is traditionally viewed as a hierarchy of neural feature detectors, with neural population responses being driven by bottom-up stimulus features. Conversely, "predictive coding" models propose that each stage of the visual hierarchy harbors two computationally distinct classes of processing unit: representational units that encode the conditional probability of a stimulus and provide predictions to the next lower level; and error units that encode the mismatch between predictions and bottom-up evidence, and forward prediction error to the next higher level. Predictive coding therefore suggests that neural population responses in category-selective visual regions, like the fusiform face area (FFA), reflect a summation of activity related to prediction ("face expectation") and prediction error ("face surprise"), rather than a homogenous feature detection response. We tested the rival hypotheses of the feature detection and predictive coding models by collecting functional magnetic resonance imaging data from the FFA while independently varying both stimulus features (faces vs houses) and subjects' perceptual expectations regarding those features (low vs medium vs high face expectation). The effects of stimulus and expectation factors interacted, whereby FFA activity elicited by face and house stimuli was indistinguishable under high face expectation and maximally differentiated under low face expectation. Using computational modeling, we show that these data can be explained by predictive coding but not by feature detection models, even when the latter are augmented with attentional mechanisms. Thus, population responses in the ventral visual stream appear to be determined by feature expectation and surprise rather than by stimulus features per se.},
author = {Egner, Tobias and Monti, Jim M and Summerfield, Christopher},
institution = {Department of Psychology and Neuroscience, and Center for Cognitive Neuroscience, Duke University, Durham, North Carolina 27708, USA. tobias.egner@duke.edu},
issn = {0270-6474},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
number = {49},
pages = {16601--16608},
pmid = {21147999},
title = {{Expectation and surprise determine neural population responses in the ventral visual stream.}},
volume = {30},
year = {2010}
}
@misc{Eimas1973,
abstract = {Using a selective adaptation procedure, evidence was obtained for the existence of linguistic feature detectors, analogous to visual feature detectors. Thes detectors are each sensitive to a restricted range of voice onset times, the physical continuum underlying the perceived phonetic distinctions between voiced and voiceless stop consonants. The sensitivity of a particular detector can be reduced selectively by repetitive presentation of its adequate stimulus. This results in a shift in the locus of the phonetic boundary separating the voiced and voiceless stops.},
author = {Eimas, Peter D. and Cooper, William E. and Corbit, John D.},
booktitle = {Perception \& Psychophysics},
issn = {0031-5117},
number = {2},
pages = {247--252},
title = {{Some properties of linguistic feature detectors}},
volume = {13},
year = {1973}
}
@article{Eisner2005,
abstract = {We conducted four experiments to investigate the specificity of perceptual adjustments made to unusual speech sounds. Dutch listeners heard a female talker produce an ambiguous fricative [?] (between [f] and [s]) in [f]- or [s]-biased lexical contexts. Listeners with [f]-biased exposure (e.g., [witlo?]; from witlof, "chicory"; witlos is meaningless) subsequently categorized more sounds on an [epsilonf]-[epsilons] continuum as [f] than did listeners with [s]-biased exposure. This occurred when the continuum was based on the exposure talker's speech (Experiment 1), and when the same test fricatives appeared after vowels spoken by novel female and male talkers (Experiments 1 and 2). When the continuum was made entirely from a novel talker's speech, there was no exposure effect (Experiment 3) unless fricatives from that talker had been spliced into the exposure talker's speech during exposure (Experiment 4). We conclude that perceptual learning about idiosyncratic speech is applied at a segmental level and is, under these exposure conditions, talker specific.},
author = {Eisner, Frank and McQueen, James M},
institution = {Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands. frank.eisner@mpi.nl},
journal = {Perception \& psychophysics},
number = {2},
pages = {224--238},
title = {{The specificity of perceptual learning in speech processing.}},
volume = {67},
year = {2005}
}
@article{Eisner2006,
author = {Eisner, Frank and McQueen, James M.},
doi = {10.1121/1.2178721},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/Eisner\_McQueen\_LongLearning.pdf:pdf},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {4},
pages = {1950},
title = {{Perceptual learning in speech: Stability over time}},
url = {http://scitation.aip.org/content/asa/journal/jasa/119/4/10.1121/1.2178721},
volume = {119},
year = {2006}
}
@article{Fallon2002,
abstract = {Children 5 and 9 years of age and adults were required to identify the final words of low- and high-context sentences in background noise. Age-related differences in the audibility of speech signals were minimized by selecting signal-to-noise ratios (SNRs) that yielded 78\% correct performance for low-context sentences. As expected, children required more favorable SNRs than adults to achieve comparable levels of performance. A more difficult listening condition was generated by adding 2 dB of noise. In general, 5-year-olds performed more poorly than did 9-year-olds and adults. Listeners of all ages, however, showed comparable gains from context in both levels of noise, indicating that noise does not impede children's use of contextual cues.},
author = {Fallon, Marianne and Trehub, Sandra E and Schneider, Bruce A},
institution = {Department of Psychology, University of Toronto at Mississauga, Ontario, Canada.},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {5 Pt 1},
pages = {2242--2249},
pmid = {12051444},
title = {{Children's use of semantic cues in degraded listening environments.}},
volume = {111},
year = {2002}
}
@article{Ganong1980,
abstract = {To investigate the interaction in speech perception of auditory information and lexical knowledge (in particular, knowledge of which phonetic sequences are words), acoustic continua varying in voice onset time were constructed so that for each acoustic continuum, one of the two possible phonetic categorizations made a word and the other did not. For example, one continuum ranged between the word dash and the nonword tash; another used the nonword dask and the word task. In two experiments, subjects showed a significant lexical effect-that is, a tendency to make phonetic categorizations that make words. This lexical effect was greater at the phoneme boundary (where auditory information is ambiguous) than at the ends of the condinua. Hence the lexical effect must arise at a stage of processing sensitive to both lexical knowledge and auditory information.},
author = {Ganong, W F},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganong - 1980 - Phonetic categorization in auditory word perception.pdf:pdf},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
keywords = {discrimination learning,humans,phonetics,speech perception},
number = {1},
pages = {110--125},
publisher = {American Psychological Assn},
title = {{Phonetic categorization in auditory word perception.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6444985},
volume = {6},
year = {1980}
}
@article{Gibson1953,
abstract = {review of the effects of practice on vision, vision training,},
author = {Gibson, E J},
journal = {Psychological bulletin},
keywords = {LEARNING,PERCEPTION},
number = {6},
pages = {401--431},
title = {{Improvement in perceptual judgments as a function of controlled practice or training.}},
volume = {50},
year = {1953}
}
@article{Gilbert2001,
abstract = {Perceptual learning is a lifelong process. We begin by encoding information about the basic structure of the natural world and continue to assimilate information about specific patterns with which we become familiar. The specificity of the learning suggests that all areas of the cerebral cortex are plastic and can represent various aspects of learned information. The neural substrate of perceptual learning relates to the nature of the neural code itself, including changes in cortical maps, in the temporal characteristics of neuronal responses, and in modulation of contextual influences. Top-down control of these representations suggests that learning involves an interaction between multiple cortical areas.},
author = {Gilbert, CD and Sigman, Mariano and Crist, RE},
doi = {10.1016/S0896-6273(01)00424-X},
isbn = {0896-6273},
issn = {0896-6273},
journal = {Neuron},
pages = {681--697},
pmid = {11567610},
title = {{The neural basis of perceptual learning}},
url = {http://www.sciencedirect.com/science/article/pii/S089662730100424X},
volume = {31},
year = {2001}
}
@misc{Jesse2011,
abstract = {Listeners use lexical knowledge to adjust to speakers' idiosyncratic pronunciations. Dutch listeners learn to interpret an ambiguous sound between /s/ and /f/ as /f/ if they hear it word-finally in Dutch words normally ending in /f/, but as /s/ if they hear it in normally /s/-final words. Here, we examined two positional effects in lexically guided retuning. In Experiment 1, ambiguous sounds during exposure always appeared in word-initial position (replacing the first sounds of /f/- or /s/-initial words). No retuning was found. In Experiment 2, the same ambiguous sounds always appeared word-finally during exposure. Here, retuning was found. Lexically guided perceptual learning thus appears to emerge reliably only when lexical knowledge is available as the to-be-tuned segment is initially being processed. Under these conditions, however, lexically guided retuning was position independent: It generalized across syllabic positions. Lexical retuning can thus benefit future recognition of particular sounds wherever they appear in words.},
author = {Jesse, Alexandra and McQueen, James M.},
booktitle = {Psychonomic Bulletin \& Review},
issn = {1069-9384},
number = {5},
pages = {943--950},
pmid = {21735330},
title = {{Positional effects in the lexical retuning of speech perception}},
volume = {18},
year = {2011}
}
@inproceedings{Johnson2004,
author = {Johnson, Keith},
booktitle = {Spontaneous speech: Data and analysis. Proceedings of the 1st session of the 10th international symposium},
organization = {Citeseer},
pages = {29--54},
title = {{Massive reduction in conversational American English}},
year = {2004}
}
@article{Kalikow1977,
author = {Kalikow, DN and Stevens, KN and Elliott, LL},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalikow, Stevens, Elliott - 1977 - Development of a test of speech intelligibility in noise using sentence materials with controlled wor.pdf:pdf},
journal = {\ldots Journal of the Acoustical Society of \ldots},
number = {5},
title = {{Development of a test of speech intelligibility in noise using sentence materials with controlled word predictability}},
url = {http://link.aip.org/link/?JASMAN/61/1337/1},
volume = {61},
year = {1977}
}
@inproceedings{Kawahara2008,
abstract = {A simple new method for estimating temporally stable power spectra is introduced to provide a unified basis for computing an interference-free spectrum, the fundamental frequency (F0), as well as aperiodicity estimation. F0 adaptive spectral smoothing and cepstral liftering based on consistent sampling theory are employed for interference-free spectral estimation. A perturbation spectrum, calculated from temporally stable power and interference-free spectra, provides the basis for both F0 and aperiodicity estimation. The proposed approach eliminates ad-hoc parameter tuning and the heavy demand on computational power, from which STRAIGHT has suffered in the past.},
author = {Kawahara, H. and Morise, M. and Takahashi, T. and Nisimura, R. and Irino, T. and Banno, H.},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Consistent sampling,Periodic signal,Periodicity,Power spectrum,Speech processing},
pages = {3933--3936},
title = {{Tandem-straight: A temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, F0, and aperiodicity estimation}},
year = {2008}
}
@misc{Kleber2012,
abstract = {The present study is concerned with lax /Ê/-fronting in Standard British English and in particular with whether this sound change in progress can be attributed to a waning of the perceptual compensation for the coarticulatory effects of context. Younger and older speakers produced various monosyllables in which /Ê/ occurred in different symmetrical consonantal contexts. The same speakers participated in a forced-choice perception experiment in which they categorized a synthetic /Éª-Ê/ continuum embedded in fronting /s\_t/ and non-fronting /w\_l/ contexts. /Ê/ was shown to be fronted for the younger age group in both production and perception. Although there was no conclusive evidence that younger listeners compensated less for coarticulation than did older listeners, the size of the coarticulatory influence of consonantal context on /Ê/ in perception was found to be smaller than in production for the younger than for the older group. The findings are consistent with a model of sound change in which the perceptual compensation for coarticulation wanes ahead of changes that take place to coarticulatory relationships in speech production. As a result, the perception and production of coarticulation may be unusually misaligned with respect to each other for some speaker-listeners participating in a sound change in progress. Keywords},
author = {Kleber, F. and Harrington, J. and Reubold, U.},
booktitle = {Language and Speech},
issn = {0023-8309},
number = {3},
pages = {383--405},
title = {{The Relationship between the Perception and Production of Coarticulation during a Sound Change in Progress}},
volume = {55},
year = {2012}
}
@article{Kleinschmidt2011,
abstract = {The mapping from phonetic categories to acoustic cue values is highly flexible, and adapts rapidly in response to exposure. There is currently, however, no theoretical framework which captures the range of this adaptation. We develop a novel approach to modeling phonetic adaptation via a belief-updating model, and demonstrate that this model naturally unifies two adaptation phenomena traditionally considered to be distinct.},
author = {Kleinschmidt, Dave and Jaeger, T Florian},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kleinschmidt, Jaeger - 2011 - A Bayesian belief updating model of phonetic recalibration and selective adaptation.pdf:pdf},
journal = {Science},
number = {June},
pages = {10--19},
publisher = {Association for Computational Linguistics},
title = {{A Bayesian belief updating model of phonetic recalibration and selective adaptation}},
url = {http://www.aclweb.org/anthology-new/W/W11/W11-06.pdf\#page=20},
year = {2011}
}
@article{Kraljic2008a,
abstract = {Listeners are faced with enormous variation in pronunciation, yet they rarely have difficulty understanding speech. Although much research has been devoted to figuring out how listeners deal with variability, virtually none (outside of sociolinguistics) has focused on the source of the variation itself. The current experiments explore whether different kinds of variation lead to different cognitive and behavioral adjustments. Specifically, we compare adjustments to the same acoustic consequence when it is due to context-independent variation (resulting from articulatory properties unique to a speaker) versus context-conditioned variation (resulting from common articulatory properties of speakers who share a dialect). The contrasting results for these two cases show that the source of a particular acoustic-phonetic variation affects how that variation is handled by the perceptual system. We also show that changes in perceptual representations do not necessarily lead to changes in production.},
author = {Kraljic, Tanya and Brennan, Susan E and Samuel, Arthur G},
doi = {10.1016/j.cognition.2007.07.013},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/KraljicBrennanSamuel2008.pdf:pdf},
issn = {0010-0277},
journal = {Cognition},
keywords = {Adult,Culture,Female,Humans,Language,Learning,Male,Phonetics,Speech Perception,Speech Production Measurement},
month = apr,
number = {1},
pages = {54--81},
pmid = {17803986},
title = {{Accommodating variation: dialects, idiolects, and speech processing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2375975\&tool=pmcentrez\&rendertype=abstract},
volume = {107},
year = {2008}
}
@article{Kraljic2005,
abstract = {Recent work on perceptual learning shows that listeners' phonemic representations dynamically adjust to reflect the speech they hear (Norris, McQueen, \& Cutler, 2003). We investigate how the perceptual system makes such adjustments, and what (if anything) causes the representations to return to their pre-perceptual learning settings. Listeners are exposed to a speaker whose pronunciation of a particular sound (either /s/ or /integral/) is ambiguous (e.g., halfway between /s/ and /integral/). After exposure, participants are tested for perceptual learning on two continua that range from /s/ to /integral/, one in the Same voice they heard during exposure, and one in a Different voice. To assess how representations revert to their prior settings, half of Experiment 1's participants were tested immediately after exposure; the other half performed a 25-min silent intervening task. The perceptual learning effect was actually larger after such a delay, indicating that simply allowing time to pass does not cause learning to fade. The remaining experiments investigate different ways that the system might unlearn a person's pronunciations: listeners hear the Same or a Different speaker for 25 min with either: no relevant (i.e., 'good') /s/ or /integral/ input (Experiment 2), one of the relevant inputs (Experiment 3), or both relevant inputs (Experiment 4). The results support a view of phonemic representations as dynamic and flexible, and suggest that they interact with both higher- (e.g., lexical) and lower-level (e.g., acoustic) information in important ways.},
author = {Kraljic, Tanya and Samuel, Arthur G},
doi = {10.1016/j.cogpsych.2005.05.001},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/KraljicSamuel2005.pdf:pdf},
issn = {0010-0285},
journal = {Cognitive psychology},
keywords = {Adaptation, Psychological,Adult,Female,Humans,Learning,Male,Psycholinguistics,Retention (Psychology),Speech Acoustics,Speech Perception,Time Factors,Transfer (Psychology),Voice Quality},
month = sep,
number = {2},
pages = {141--78},
pmid = {16095588},
title = {{Perceptual learning for speech: Is there a return to normal?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16095588},
volume = {51},
year = {2005}
}
@article{Kraljic2007,
author = {Kraljic, Tanya and Samuel, Arthur G.},
doi = {10.1016/j.jml.2006.07.010},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/KraljicSamuel2006MultipleSpeakers.pdf:pdf},
issn = {0749596X},
journal = {Journal of Memory and Language},
keywords = {a speech-to-text system knows,adjustments,anyone who has used,multiple,multiple speakers,not equipped to handle,partner effects,perceptual learning,priming,speakers,speech perception,successful perfor-,that most systems are,to get even moderately},
month = jan,
number = {1},
pages = {1--15},
title = {{Perceptual adjustments to multiple speakers}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X06000842},
volume = {56},
year = {2007}
}
@article{Kraljic2006,
author = {Kraljic, Tanya and Samuel, Arthur G.},
doi = {10.3758/BF03193841},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/KraljicSamuel2006Generalization.pdf:pdf},
issn = {1069-9384},
journal = {Psychonomic Bulletin \& Review},
month = apr,
number = {2},
pages = {262--268},
title = {{Generalization in perceptual learning for speech}},
url = {http://link.springer.com/10.3758/BF03193841},
volume = {13},
year = {2006}
}
@article{Kraljic2008,
abstract = {Perceptual theories must explain how perceivers extract meaningful information from a continuously variable physical signal. In the case of speech, the puzzle is that little reliable acoustic invariance seems to exist. We tested the hypothesis that speech-perception processes recover invariants not about the signal, but rather about the source that produced the signal. Findings from two manipulations suggest that the system learns those properties of speech that result from idiosyncratic characteristics of the speaker; the same properties are not learned when they can be attributed to incidental factors. We also found evidence for how the system determines what is characteristic: In the absence of other information about the speaker, the system relies on episodic order, representing those properties present during early experience as characteristic of the speaker. This "first-impressions" bias can be overridden, however, when variation is an incidental consequence of a temporary state (a pen in the speaker's mouth), rather than characteristic of the speaker.},
author = {Kraljic, Tanya and Samuel, Arthur G and Brennan, Susan E},
doi = {10.1111/j.1467-9280.2008.02090.x},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/KraljicSamuelBrennan2008.pdf:pdf},
issn = {0956-7976},
journal = {Psychological science},
keywords = {Adaptation, Psychological,Adolescent,Adult,Attention,Attitude,Humans,Phonetics,Reaction Time,Social Perception,Speech Perception},
month = apr,
number = {4},
pages = {332--8},
pmid = {18399885},
title = {{First impressions and last resorts: how listeners adjust to speaker variability.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18399885},
volume = {19},
year = {2008}
}
@article{Kuhl1979,
abstract = {While numerous studies on infant perception demonstrate the infant's ability to discriminate individual speech-sound pairs, very few demonstrate the infant's ability to recognize the similarity among phonetic units when they occur in different phonetic contexts, in different positions in a syllable, or when they are spoken by different talkers. In two studies, six-month-old infants demonstrated the ability to distinguish two spectrally dissimilar vowel categories (/a/ and /i/) in which the vowel tokens were generated to simulate tokens produced by a male, a female, and a child talker. In experiment I, the infants were initially trained to discriminate the /a/ and /i/ tokens produced by the computer-simulated male voice. They were then gradually exposed to a number of novel tokens in a progressive transfer-of-learning task. In experiment II, the infants were initially trained to discriminate the same vowell contrast, but were then immediately tested with all of the tokens in both vowel categories. In both experiments the infants demonstrated rapid transfer of learning from the training tokens produced by the male talker to the tokens produced by female and child talkers. Both experiments provide strong evidence that the six-month-old infant recognizes acoustic categories that conform to the vowel categories perceived by adult speakers of English.},
author = {Kuhl, P K},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {6},
pages = {1668--1679},
pmid = {521551},
title = {{Speech perception in early infancy: perceptual constancy for spectrally dissimilar vowel categories.}},
volume = {66},
year = {1979}
}
@misc{Labov1972,
abstract = {This classic volume, by a well-known linguist, constitutes a systematic introduction to sociolinguistics, unmatched in the clarity and forcefulness of its approach, and to the study of language in its social setting.},
author = {Labov, William},
booktitle = {Language},
editor = {Anonymous},
number = {4},
pages = {344},
pmid = {771584},
publisher = {University of Pennsylvania Press},
series = {Conduct and communication 4},
title = {{Sociolinguistic Patterns}},
url = {http://books.google.com/books?id=hD0PNMu8CfQC\&pgis=1},
volume = {2},
year = {1972}
}
@article{Ladefoged1957,
abstract = {Most speech sounds may be said to convey three kinds of information: linguistic information which enables the listener to identify the words that are being used; socio-linguistic information, which enables him to appreciate something about the background of the speaker; and personal information which helps to identify the speaker. An e'qleriment has been carried out which shows that the linguistic information con- veyed by a vowel sound does not depend on the absolute values of its formant frequencies, but on the relationship between the formant frequencies for that vowel and the formant frequencies of other vowels pronounced by that speaker. Six versions of tile sentence Please say what this word is were synthesized on a Parametric Artificial Talking device. Four test words of the form b-(vowel)-t were also synthesized. It is shown that the identification of the test word depends on the formant structure of the introductory sentence. Some psychological implications of this e:tperiment are discussed, and hypotheses are put forward concerning the ways in which all three kinds of information are conveyed by vowels.},
author = {Ladefoged, Peter and Broadbent, D. E.},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {98--104},
title = {{Information conveyed by vowels}},
url = {http://scitation.aip.org/content/asa/journal/jasa/29/1/10.1121/1.1908694},
volume = {29},
year = {1957}
}
@misc{Leber2006,
abstract = {What factors determine the implementation of attentional set? It is often assumed that set is determined only by experimenter instructions and characteristics of the immediate stimulus environment, yet it is likely that other factors play a role. The present experiments were designed to evaluate the latter possibility; specifically, the role of past experience was probed. In a 320-trial training phase, observers could use one of two possible attentional sets (but not both) to find colour-defined targets in a rapid serial visual presentation (RSVP) stream of letters. In the subsequent 320-trial test phase, where either set could be used, observers persisted in using their pre-established sets through the remainder of the experiment, affirming a clear role of past experience in the implementation of attentional set. A second experiment revealed that sufficient experience with a given set was necessary to facilitate persistence with it. These results are consistent with models of executive control (e.g., Norman \& Shallice, 1986), in which ?top-down? behaviours are influenced by learned associations between tasks and the environment.$\backslash$nWhat factors determine the implementation of attentional set? It is often assumed that set is determined only by experimenter instructions and characteristics of the immediate stimulus environment, yet it is likely that other factors play a role. The present experiments were designed to evaluate the latter possibility; specifically, the role of past experience was probed. In a 320-trial training phase, observers could use one of two possible attentional sets (but not both) to find colour-defined targets in a rapid serial visual presentation (RSVP) stream of letters. In the subsequent 320-trial test phase, where either set could be used, observers persisted in using their pre-established sets through the remainder of the experiment, affirming a clear role of past experience in the implementation of attentional set. A second experiment revealed that sufficient experience with a given set was necessary to facilitate persistence with it. These results are consistent with models of executive control (e.g., Norman \& Shallice, 1986), in which ?top-down? behaviours are influenced by learned associations between tasks and the environment.},
author = {Leber, Andrew B. and Egeth, Howard E.},
booktitle = {Visual Cognition},
issn = {1350-6285},
number = {4-8},
pages = {565--583},
title = {{Attention on autopilot: Past experience and attentional set}},
volume = {14},
year = {2006}
}
@article{Levy2008,
abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159-166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Levy, Roger},
journal = {Cognition},
keywords = {Frequency,Information theory,Parsing,Prediction,Sentence processing,Syntactic complexity,Syntax,Word order},
number = {3},
pages = {1126--1177},
pmid = {17662975},
title = {{Expectation-based syntactic comprehension}},
volume = {106},
year = {2008}
}
@article{Lieberman1963,
abstract = {Three speakers read aloud meaningful grammatical English sentences at a fast rate. Some of these sentences contained common maxims and stereotyped phrases. Other sentences which were less familiar contained, in similar phonetic environments, certain test words that also occurred in the stereotyped sentences. The test words were âexcisedâ, i.e., gated out of all the sentences, and listening tests were performed by 43 listeners. Quite apart from these listening tests two operational measures of redundancy were derived. One measure was based on a Markovian model of perception and readers were asked to guess what word n would be after they had read all the words through word nâ1 in a sentence. The second measure was based on the hypothesis that people reserve their final decision on the recognition of each word until they perceive an entire sentence. Readers read an entire sentence in the midst of which a word was represented by a dash. They were then asked to guess what the missing word was. Two groups of thirty readers each performed these written tests on the sentences from which the test words had been excised. Indexes of redundancy were computed for each test word from the percent of correct guesses that occurred for the two written tests.The results of the listening tests show that the intelligibility of the excised words is inversely proportional to the redundancy index obtained from the total sentence context. Two acoustic correlates of linguistic stress (duration and amplitude) were measured for each excised word. Stress is also apparently inversely proportional to the sentence context redundancy measure. The results of this experiment support the hypothesis that both the acoustic realization and auditory perception of a given word in a meaningful sentence may be a function of the speaker's and listener's knowledge of the semantic and grammatical information contained in the entire sentence. The experimental results also indicate that, in some circumstances, a listener may be able to identify a word only after he adapts to the speaker's voice.},
author = {Lieberman, Philip},
journal = {Language and Speech},
number = {3},
pages = {172 --187},
title = {{Some Effects of Semantic and Grammatical Context on the Production and Perception of Speech}},
url = {http://las.sagepub.com/content/6/3/172.abstract},
volume = {6},
year = {1963}
}
@article{Magnuson2007,
abstract = {The sounds that make up spoken words are heard in a series and must be mapped rapidly onto words in memory because their elements, unlike those of visual words, cannot simultaneously exist or persist in time. Although theories agree that the dynamics of spoken word recognition are important, they differ in how they treat the nature of the competitor set-precisely which words are activated as an auditory word form unfolds in real time. This study used eye tracking to measure the impact over time of word frequency and 2 partially overlapping competitor set definitions: onset density and neighborhood density. Time course measures revealed early and continuous effects of frequency (facilitatory) and on set based similarity (inhibitory). Neighborhood density appears to have early facilitatory effects and late inhibitory effects. The late inhibitory effects are due to differences in the temporal distribution of similarity within neighborhoods. The early facilitatory effects are due to subphonemic cues that inform the listener about word length before the entire word is heard. The results support a new conception of lexical competition neighborhoods in which recognition occurs against a background of activated competitors that changes over time based on fine-grained goodness-of-fit and competition dynamics.},
author = {Magnuson, James S and Dixon, James A and Tanenhaus, Michael K and Aslin, Richard N},
institution = {Department of Psychology, University of ConnecticutHaskins Laboratories, New Haven, ConnecticutDepartment of Brain and Cognitive Sciences, University of Rochester.},
issn = {0364-0213},
journal = {Cognitive science},
number = {1},
pages = {133--156},
pmid = {21635290},
title = {{The dynamics of lexical competition during spoken word recognition.}},
volume = {31},
year = {2007}
}
@article{Mattys2009,
abstract = {Effects of perceptual and cognitive loads on spoken-word recognition have so far largely escaped investigation. This study lays the foundations of a psycholinguistic approach to speech recognition in adverse conditions that draws upon the distinction between energetic masking, i.e., listening environments leading to signal degradation, and informational masking, i.e., listening environments leading to depletion of higher-order, domain-general processing resources, independent of signal degradation. We show that severe energetic masking, such as that produced by background speech or noise, curtails reliance on lexical-semantic knowledge and increases relative reliance on salient acoustic detail. In contrast, informational masking, induced by a resource-depleting competing task (divided attention or a memory load), results in the opposite pattern. Based on this clear dissociation, we propose a model of speech recognition that addresses not only the mapping between sensory input and lexical representations, as traditionally advocated, but also the way in which this mapping interfaces with general cognition and non-linguistic processes. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Mattys, Sven L. and Brooks, Joanna and Cooke, Martin},
journal = {Cognitive Psychology},
keywords = {Energetic masking,Informational masking,Processing load,Psycholinguistics,Speech segmentation,Spoken-word recognition},
number = {3},
pages = {203--243},
pmid = {19423089},
title = {{Recognizing speech under a processing load: Dissociating energetic from informational factors}},
volume = {59},
year = {2009}
}
@article{Mattys2011,
abstract = {The effect of cognitive load (CL) on speech recognition has received little attention despite the prevalence of CL in everyday life, e.g., dual-tasking. To assess the effect of CL on the interaction between lexically-mediated and acoustically-mediated processes, we measured the magnitude of the "Ganong effect" (i.e., lexical bias on phoneme identification) under CL and no CL. CL consisted of a concurrent visual search task. Experiment 1 showed an increased Ganong effect under CL. A time-course analysis of this pattern (Experiments 2 and 3) revealed that the Ganong effect decreased over time under optimal conditions, but it did not under CL. Thus, CL appears to be delaying (and perhaps preventing) listeners' ability to rely on fine phonetic detail to perform the sub-lexical task. This finding, along with an absence of measurable effects at the post-lexical level (Experiment 4) or at the lexical level (Experiment 5) and a clear negative effect of CL on perceptual discrimination (Experiment 6), suggests that the increased reliance on lexically-mediated processes under CL is the cascaded effect of impoverished encoding of the sensory input. Ways of implementing a link between CL and sensory analysis into existing models of speech recognition are proposed. ?? 2011 Elsevier Inc.},
author = {Mattys, Sven L. and Wiget, Lukas},
journal = {Journal of Memory and Language},
keywords = {Cognitive load,Divided attention,Ganong,Speech perception,Spoken-word recognition},
number = {2},
pages = {145--160},
title = {{Effects of cognitive load on speech recognition}},
volume = {65},
year = {2011}
}
@article{Mayo1997,
abstract = {To determine how age of acquisition influences perception of second-language speech, the Speech Perception in Noise (SPIN) test was administered to native Mexican-Spanish-speaking listeners who learned fluent English before age 6 (early bilinguals) or after age 14 (late bilinguals) and monolingual American-English speakers (monolinguals). Results show that the levels of noise at which the speech was intelligible were significantly higher and the benefit from context was significantly greater for monolinguals and early bilinguals than for late bilinguals. These findings indicate that learning a second language at an early age is important for the acquisition of efficient high-level processing of it, at least in the presence of noise.},
author = {Mayo, L H and Florentine, M and Buus, S},
institution = {Northeastern University, Boston, MA, USA.},
issn = {1092-4388},
journal = {Journal of speech, language, and hearing research : JSLHR},
number = {3},
pages = {686--693},
pmid = {9210123},
title = {{Age of second-language acquisition and perception of speech in noise.}},
volume = {40},
year = {1997}
}
@misc{Mcauliffe2015,
author = {Mcauliffe, Michael},
title = {python-acoustic-similarity},
url = {https://github.com/mmcauliffe/python-acoustic-similarity},
year = {2015}
}
@article{Mitterer2013,
abstract = {Recent evidence shows that listeners use abstract prelexical units in speech perception. Using the phenomenon of lexical retuning in speech processing, we ask whether those units are necessarily phonemic. Dutch listeners were exposed to a Dutch speaker producing ambiguous phones between the Dutch syllable-final allophones approximant [r] and dark [l]. These ambiguous phones replaced either final /r/ or final /l/ in words in a lexical-decision task. This differential exposure affected perception of ambiguous stimuli on the same allophone continuum in a subsequent phonetic-categorization test: Listeners exposed to ambiguous phones in /r/-final words were more likely to perceive test stimuli as /r/ than listeners with exposure in /l/-final words. This effect was not found for test stimuli on continua using other allophones of /r/ and /l/. These results confirm that listeners use phonological abstraction in speech perception. They also show that context-sensitive allophones can play a role in this process, and hence that context-insensitive phonemes are not necessary. We suggest there may be no one unit of perception. ?? 2013 Elsevier B.V.},
author = {Mitterer, Holger and Scharenborg, Odette and McQueen, James M.},
journal = {Cognition},
keywords = {Allophones,Perceptual learning,Phonemes,Speech perception},
number = {2},
pages = {356--361},
pmid = {23973464},
title = {{Phonological abstraction without phonemes in speech perception}},
volume = {129},
year = {2013}
}
@article{Norris1988,
abstract = {Previous research comparing detection times for syllables and for phonemes has consistently found that syllables are responded to faster than phonemes. This finding poses theoretical problems for strictly hierarchical models of speech recognition, in which smaller units should be able to be identified faster than larger units. However, inspection of the characteristics of previous experiments' stimuli reveals that subjects have been able to respond to syllables on the basis of only a partial analysis of the stimulus. In the present experiment, five groups of subjects listened to identical stimulus material. Phoneme and syllable monitoring under standard conditions was compared with monitoring under conditions in which near matches of target and stimulus occurred on no-response trials. In the latter case, when subjects were forced to analyse each stimulus fully, phonemes were detected faster than syllables.},
author = {Norris, D and Cutler, A},
issn = {0031-5117},
journal = {Perception \& psychophysics},
number = {6},
pages = {541--550},
pmid = {3399352},
title = {{The relative accessibility of phonemes and syllables.}},
volume = {43},
year = {1988}
}
@misc{Norris2003,
abstract = {This study demonstrates that listeners use lexical knowledge in perceptual learning of speech sounds. Dutch listeners first made lexical decisions on Dutch words and nonwords. The final fricative of 20 critical words had been replaced by an ambiguous sound, between [f] and [s]. One group of listeners heard ambiguous [f]-final words (e.g., [witlo?], from witlof, chicory) and unambiguous [s]-final words (e.g., naaldbos, pine forest). Another group heard the reverse (e.g., ambiguous [na:ldbo?], unambiguous witlof ). Listeners who had heard [?] in [f]-final words were subsequently more likely to categorize ambiguous sounds on an [f]-[s] continuum as [f] than those who heard [?] in [s]-final words. Control conditions ruled out alternative explanations based on selective adaptation and contrast. Lexical information can thus be used to train categorization of speech. This use of lexical information differs from the on-line lexical feedback embodied in interactive models of speech perception. In contrast to on-line feedback, lexical feedback for learning is of benefit to spoken word recognition (e.g., in adapting to a newly encountered dialect). ?? 2003 Elsevier Science (USA). All rights reserved.},
author = {Norris, Dennis and McQueen, James M. and Cutler, Anne},
booktitle = {Cognitive Psychology},
number = {2},
pages = {204--238},
pmid = {12948518},
title = {{Perceptual learning in speech}},
volume = {47},
year = {2003}
}
@article{Pierrehumbert2002,
author = {Pierrehumbert, J},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Pierrehumbert\_word\_specific.pdf.pdf:pdf},
journal = {Laboratory phonology},
title = {{Word-specific phonetics}},
url = {http://books.google.com/books?hl=en\&lr=\&id=N5quZxfmpswC\&oi=fnd\&pg=PA101\&dq=Word-specific+phonetics\&ots=3kCkpgcbgI\&sig=kMSAYjSD1UDqQr8VOpP0cst4LDE},
year = {2002}
}
@article{Pierrehumbert2001,
abstract = {Exemplar theory was first developed as amodel of similarity and classification in perception. In this paper, the theory is extended to model speech production as well as speech perception. Straightforward extension of the model provides a formal framework for thinking about the quantitative predictions of usage-based phonology, as proposed by Bybee. A model is proposed which allows us to derive the finding that leniting historical changes are more advanced in frequent words than in rarer ones. Calculations using this model are presented which reveal the interaction of production noise, lenition and entrenchment. A realistic treatment is also provided for the time course of a phonological merger which originates from lenition of a marked category.},
author = {Pierrehumbert, Janet B},
issn = {01677373},
journal = {Frequency and the emergence of linguistic structure},
pages = {137--158},
title = {{Exemplar dynamics: Word frequency, lenition and contrast}},
year = {2001}
}
@article{Pitt1993,
abstract = {Recent studies that used Ganong's (1980) identification task have produced discrepant results. The present study sought to resolve these discrepancies by examining the influence of methodological factors on phoneme identification and differences in data analysis techniques. Three factors were examined across 2 experiments: position of target phoneme, phonetic contrast, and 2 task conditions in which stimulus quality (S/N ratio) or cognitive load varied. A meta-analysis was then performed on the results from all identification studies, including the present one, in an effort to obtain additional insight on factors that influence the task. The experiments and meta-analysis identified the importance of several methodological factors in affecting identification, most notably position of the target phoneme.},
author = {Pitt, M A and Samuel, A G},
institution = {Department of Psychology, Ohio State University, Columbus 43210-1222.},
issn = {0096-1523},
journal = {Journal of experimental psychology. Human perception and performance},
number = {4},
pages = {699--725},
pmid = {8409855},
title = {{An empirical and meta-analytic evaluation of the phoneme identification task.}},
volume = {19},
year = {1993}
}
@article{Pitt2012,
author = {Pitt, MA and Szostak, CM},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pitt, Szostak - 2012 - A lexically biased attentional set compensates for variable speech quality caused by pronunciation variation.pdf:pdf},
journal = {Language and Cognitive Processes},
number = {April 2013},
pages = {37--41},
title = {{A lexically biased attentional set compensates for variable speech quality caused by pronunciation variation}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2011.619370},
year = {2012}
}
@article{Pitt1990,
abstract = {260 undergraduates participated in 3 experiments in which a variant of the phoneme monitoring task was developed to investigate temporal selective attention during speech processing. The probable location of the target phoneme to be monitored for was varied to induce Ss to attend more closely to one location than to others. Exps 1 and 2 examined selective attention under normal listening conditions, and Exp 3 investigated attention under more difficult monitoring conditions. Data indicate that temporal selective attention is very flexible and precise: Benefits in performance were obtained at the attended location, and costs were observed at the unattended locations. Imposing extra processing demands on Ss resulted in a loss of attentional selectivity under some circumstances. (PsycINFO Database Record (c) 2003 APA, all rights reserved) Access: http://www.academicpress.com/jml http:$\backslash$$\backslash$www.elsevier.com SUBJECT(S)},
author = {Pitt, Mark A and Samuel, Arthur G},
issn = {0749596X},
journal = {Journal of Memory \& Language},
keywords = {(Major): Selective Attention,(Minor): Phonemes,Age Group: Adulthood (18 yrs \& older),Class Descriptors: 2326 Auditory \& Speech Percepti,Identifier: variability of probable location of ta,Population: Human,Speech Perception,Stimulus Variability},
number = {5},
pages = {611--632},
title = {{Attentional allocation during speech perception: How fine is the focus?}},
volume = {29},
year = {1990}
}
@article{Pitt2006,
abstract = {Many models of spoken word recognition posit the existence of lexical and sublexical representations, with excitatory and inhibitory mechanisms used to affect the activation levels of such representations. Bottom-up evidence provides excitatory input, and inhibition from phonetically similar representations leads to lexical competition. In such a system, long words should produce stronger lexical activation than short words, for 2 reasons: Long words provide more bottom-up evidence than short words, and short words are subject to greater inhibition due to the existence of more similar words. Four experiments provide evidence for this view. In addition, reaction-time-based partitioning of the data shows that long words generate greater activation that is available both earlier and for a longer time than is the case for short words. As a result, lexical influences on phoneme identification are extremely robust for long words but are quite fragile and condition-dependent for short words. Models of word recognition must consider words of all lengths to capture the true dynamics of lexical activation.},
author = {Pitt, Mark A and Samuel, Arthur G},
institution = {Department of Psychology, Ohio State University, Columbus, OH 43210-1222, USA. pitt.2@osu.edu},
issn = {0096-1523},
journal = {Journal of experimental psychology. Human perception and performance},
number = {5},
pages = {1120--1135},
pmid = {17002526},
title = {{Word length and lexical activation: longer is better.}},
volume = {32},
year = {2006}
}
@misc{PsychologySoftwareTools2012,
author = {{Psychology Software Tools}, Inc},
title = {{E-Prime}},
url = {http://www.pstnet.com},
year = {2012}
}
@article{Reinisch2013a,
abstract = {Listeners use lexical knowledge to retune phoneme categories. When hearing an ambiguous sound between /s/ and /f/ in lexically unambiguous contexts such as gira[s/f], listeners learn to interpret the sound as /f/ because gira[f] is a real word and gira[s] is not. Later, they apply this learning even in lexically ambiguous contexts (perceiving knife rather than nice). Although such retuning could help listeners adapt to foreign-accented speech, research has focused on single phonetic contrasts artificially manipulated to create ambiguous sounds; however, accented speech varies along many dimensions. It is therefore unclear whether analogies to adaptation to accented speech are warranted. In the present studies, the to-be-adapted ambiguous sound was embedded in a global foreign accent. In addition, conditions of cross-speaker generalization were tested with focus on the extent to which perceptual similarity between 2 speakers' fricatives is a condition for generalization to occur. Results showed that listeners retune phoneme categories manipulated within the context of a global foreign accent, and that they generalize this short-term learning to the perception of phonemes from previously unheard speakers. However, generalization was observed only when exposure and test speakers' fricatives were sampled across a similar perceptual space. (PsycINFO Database Record (c) 2013 APA, all rights reserved).},
author = {Reinisch, Eva and Holt, Lori L},
issn = {1939-1277},
journal = {Journal of experimental psychology. Human perception and performance},
keywords = {foreign accent,lexically-guided phonetic category,perceptual learning,retuning,speaker generalization,speech perception},
number = {2},
pages = {539--555},
pmid = {24059846},
title = {{Lexically Guided Phonetic Retuning of Foreign-Accented Speech and Its Generalization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24059846},
volume = {40},
year = {2013}
}
@article{Reinisch2013,
abstract = {Native listeners adapt to noncanonically produced speech by retuning phoneme boundaries by means of lexical knowledge. We asked whether a second language lexicon can also guide category retuning and whether perceptual learning transfers from a second language (L2) to the native language (L1). During a Dutch lexical-decision task, German and Dutch listeners were exposed to unusual pronunciation variants in which word-final /f/ or /s/ was replaced by an ambiguous sound. At test, listeners categorized Dutch minimal word pairs ending in sounds along an /f/-/s/ continuum. Dutch L1 and German L2 listeners showed boundary shifts of a similar magnitude. Moreover, following exposure to Dutch-accented English, Dutch listeners also showed comparable effects of category retuning when they heard the same speaker speak her native language (Dutch) during the test. The former result suggests that lexical representations in a second language are specific enough to support lexically guided retuning, and the latter implies that production patterns in a second language are deemed a stable speaker characteristic likely to transfer to the native language; thus retuning of phoneme categories applies across languages.},
author = {Reinisch, Eva and Weber, Andrea and Mitterer, Holger},
doi = {10.1037/a0027979},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/ReinischWeberMitterer\_JEPHPP2012.pdf:pdf},
issn = {1939-1277},
journal = {Journal of experimental psychology. Human perception and performance},
keywords = {Acoustic Stimulation,Adolescent,Adult,Humans,Learning,Multilingualism,Phonetics,Psycholinguistics,Speech,Speech Perception,Young Adult},
month = feb,
number = {1},
pages = {75--86},
pmid = {22545600},
title = {{Listeners retune phoneme categories across languages.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22545600},
volume = {39},
year = {2013}
}
@article{Reinisch2014,
abstract = {Listeners use lexical or visual context information to recalibrate auditory speech perception. After hearing an ambiguous auditory stimulus between /aba/ and /ada/ coupled with a clear visual stimulus (e.g., lip closure in /aba/), an ambiguous auditory-only stimulus is perceived in line with the previously seen visual stimulus. What remains unclear, however, is what exactly listeners are recalibrating: phonemes, phone sequences, or acoustic cues. To address this question we tested generalization of visually-guided auditory recalibration to (1) the same phoneme contrast cued differently (i.e., /aba/-/ada/ vs. /ibi/-/idi/ where the main cues are formant transitions in the vowels vs. burst and frication of the obstruent), (2) a different phoneme contrast cued identically (/aba/-/ada/ vs. /ama/-/ana/ both cued by formant transitions in the vowels), and (3) the same phoneme contrast with the same cues in a different acoustic context (/aba/-/ada/ vs. /ubu/-/udu/). Whereas recalibration was robust for all recalibration control trials, no generalization was found in any of the experiments. This suggests that perceptual recalibration may be more specific than previously thought as it appears to be restricted to the phoneme category experienced during exposure as well as to the specific manipulated acoustic cues. We suggest that recalibration affects context-dependent sub-lexical units.},
author = {Reinisch, Eva and Wozny, David R. and Mitterer, Holger and Holt, Lori L.},
doi = {10.1016/j.wocn.2014.04.002},
issn = {00954470},
journal = {Journal of Phonetics},
pages = {91--105},
pmid = {24932053},
title = {{Phonetic category recalibration: What are the categories?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S009544701400045X},
volume = {45},
year = {2014}
}
@article{Samuel1986,
abstract = {The selective adaptation paradigm was used extensively for about 5 years fol- lowing its introduction to speech research in 1973. During the next few years, its use dropped dramatically; it is now little used. Several reasons for the abandon- ment of the paradigm are discussed, and theoretical and empirical justification is provided for rejecting these reasons. Experiment 1 demonstrates that âacoustic similarityâ of an adapting sound and test items cannot account for the observed results. Experiments 2-4 demonstrate that adaptation effects are not equivalent to simple contrast effects. These experiments indicate that selective adaptation produces robust reaction time effects-items in the adapted category are identi- tied more slowly than unadapted items. The effects found in a simple paired-con- trast procedure differ from those found with selective adaptation. Most strikingly, contrast effects are extremely ear dependent-much larger effects occur if testing is conducted in the right ear than in the left; adaptation effects are relatively symmetrical with respect to ear. The empirical and theoretical analyses suggest that the selective adaptation paradigm can be a powerful tool for investigating the perception of Complex acoustic stimuli like speech.},
author = {Samuel, A G},
issn = {00100285},
journal = {Cognitive psychology},
number = {4},
pages = {452--499},
pmid = {3769426},
title = {{Red herring detectors and speech perception: in defense of selective adaptation.}},
volume = {18},
year = {1986}
}
@article{Samuel1981,
abstract = {Phonemic restoration is a powerful auditory illusion in which listeners "hear" parts of words that are not really there. In earlier studies of the illusion, segments of words (phonemes) were replaced by an extraneous sound; listeners were asked whether anything was missing and where the extraneous noise had occurred. Most listeners reported that the utterance was intact and mislocalized the noise, suggesting that they had restored the missing phoneme. In the present study, a second type of stimulus was also presented: items in which the extraneous sound was merely superimposed on the critical phoneme. On each trial, listeners were asked to report whether they thought a stimulus utterance was intact (noise superimposed) or not (noise replacing). Since this procedure yields both a miss rate P(intact/replaced), and a false alarm rate P(replaced/intact), signal detection parameters of discriminability and bias can be calculated. The discriminability parameter reflects how similar the two types of stimuli sound; perceptual restoration of replaced items should make them sound intact, producing low discriminability scores. The bias parameter measures the tendency of listeners to report utterances as intact; it reflects postperceptual decision processes. This improved methodology was used to test the hypothesis that restoration (and more generally, speech perception) depends upon the bottom-up confirmation of expectations generated at higher levels. Perceptual restoration varied greatly wih the phone class of the replaced segment and its acoustic similarity to the replacement sound, supporting a bottom-up component to the illusion. Increasing listeners' expectations of a phoneme increased perceptual restoration: missing segments in words were better restored than corresponding pieces in phonologically legal pseudowords; priming the words produced even more restoration. In contrast, sentential context affected the postperceptual decision stage, biasing listeners to report utterances as intact. A limited interactive model of speech perception, with both bottom-up and top-down components, is used to explain the results.},
author = {Samuel, A G},
issn = {0096-3445},
journal = {Journal of experimental psychology. General},
number = {4},
pages = {474--494},
pmid = {6459403},
title = {{Phonemic restoration: insights from a new methodology.}},
volume = {110},
year = {1981}
}
@article{Scarborough2010,
author = {Scarborough, R},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scarborough - 2010 - Lexical and contextual predictability Confluent effects on the production of vowels.pdf:pdf},
journal = {Laboratory phonology},
pages = {575--604},
title = {{Lexical and contextual predictability: Confluent effects on the production of vowels}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Lexical+and+contextual+predictability:+confluent+effects+on+the+production+of+vowels\#0},
year = {2010}
}
@article{Scharenborg2013,
abstract = {Numerous studies have shown that younger adults engage in lexically guided perceptual learning in speech perception. Here, we investigated whether older listeners are also able to retune their phonetic category boundaries. More specifically, in this research we tried to answer two questions. First, do older adults show perceptual-learning effects of similar size to those of younger adults? Second, do differences in lexical behavior predict the strength of the perceptual-learning effect? An age group comparison revealed that older listeners do engage in lexically guided perceptual learning, but there were two age-related differences: Younger listeners had a stronger learning effect right after exposure than did older listeners, but the effect was more stable for older than for younger listeners. Moreover, a clear link was shown to exist between individuals' lexical-decision performance during exposure and the magnitude of their perceptual-learning effects. A subsequent analysis on the results of the older participants revealed that, even within the older participant group, with increasing age the perceptual retuning effect became smaller but also more stable, mirroring the age group comparison results. These results could not be explained by differences in hearing loss. The age effect may be accounted for by decreased flexibility in the adjustment of phoneme categories or by age-related changes in the dynamics of spoken-word recognition, with older adults being more affected by competition from similar-sounding lexical competitors, resulting in less lexical guidance for perceptual retuning. In conclusion, our results clearly show that the speech perception system remains flexible over the life span.},
author = {Scharenborg, Odette and Janse, Esther},
issn = {1943-393X},
journal = {Attention, perception \& psychophysics},
keywords = {aging,can,highly educated listeners,ideal,individual differences,listeners,normal-hearing,numerous studies have shown,perceptual learning,speech perception,that,that is,young},
number = {3},
pages = {525--36},
pmid = {23354594},
title = {{Comparing lexically guided perceptual learning in younger and older listeners.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23354594},
volume = {75},
year = {2013}
}
@article{Scharenborg2014,
author = {Scharenborg, Odette and Weber, Andrea and Janse, Esther},
journal = {Attention, Perception, \& Psychophysics},
pages = {1--15},
publisher = {Springer},
title = {{The role of attentional abilities in lexically guided perceptual learning by older listeners}},
year = {2014}
}
@article{Shankweiler1977,
author = {Shankweiler, Donald and Strange, Winifred and Verbrugge, R R},
journal = {Perceiving, acting, and knowing: Toward an ecological psychology},
pages = {315--345},
title = {{Speech and the problem of perceptual constancy}},
year = {1977}
}
@inproceedings{Strand1996,
author = {Strand, Elizabeth A and Johnson, Keith},
booktitle = {KONVENS},
pages = {14--26},
title = {{Gradient and visual speaker normalization in the perception of fricatives.}},
year = {1996}
}
@article{Sumner2011,
author = {Sumner, Meghan},
doi = {10.1016/j.cognition.2010.10.018},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sumner - 2011 - The role of variation in the perception of accented speech.pdf:pdf},
issn = {0010-0277},
journal = {Cognition},
number = {1},
pages = {131--136},
publisher = {Elsevier B.V.},
title = {{The role of variation in the perception of accented speech}},
url = {http://dx.doi.org/10.1016/j.cognition.2010.10.018},
volume = {119},
year = {2011}
}
@article{Sumner2013,
abstract = {This study reports equivalence in recognition for variable productions of spoken words that differ greatly in frequency. General American (GA) listeners participated in either a semantic priming or a false-memory task, each with three talkers with different accents: GA, New York City (NYC), and Southern Standard British English (BE). GA/BE induced strong semantic priming and low false recall rates. NYC induced no semantic priming but high false recall rates. These results challenge current theory and illuminate encoding-based differences sensitive to phonetically-cued talker variation. The findings highlight the central role of phonetic variation in the spoken word recognition process.},
author = {Sumner, Meghan and Kataoka, Reiko},
doi = {10.1121/1.4826151},
issn = {1520-8524},
journal = {The Journal of the Acoustical Society of America},
month = dec,
number = {6},
pages = {EL485},
pmid = {25669293},
title = {{Effects of phonetically-cued talker variation on semantic encoding.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25669293},
volume = {134},
year = {2013}
}
@article{Sumner2009,
abstract = {The task of recognizing spoken words is notoriously difficult. Once dialectal variation is considered, the difficulty of this task increases. When living in a new dialect region, however, processing difficulties associated with dialectal variation dissipate over time. Through a series of primed lexical decision tasks (form priming, semantic priming, and long-term repetition priming), we examine the general issue of dialectal variation in spoken word recognition, while investigating the role of experience in perception and representation. The main questions we address are: (1) how are cross-dialect variants recognized and stored, and (2) how are these variants accommodated by listeners with different levels of exposure to the dialect? Three claims are made based on the results: (1) dialect production is not always representative of dialect perception and representation, (2) experience strongly affects a listener's ability to recognize and represent spoken words, and (3) there is a general benefit for variants that are not regionally-marked. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Sumner, Meghan and Samuel, Arthur G.},
issn = {0749596X},
journal = {Journal of Memory and Language},
keywords = {Dialect,Experience,Speech perception,Spoken word recognition,Variation,r-Dropping},
number = {4},
pages = {487--501},
title = {{The effect of experience on the perception and representation of dialect variants}},
volume = {60},
year = {2009}
}
@techreport{Sussman1999,
abstract = {There is uncertainty concerning the extent to which the auditory streaming effect is a function of attentive or preattentive mechanisms. The mismatch negativity (MMN), which indexes preattentive acoustic processing, was used to probe whether the segregation associated with the streaming effect occurs preattentively. In Experiment 1, alternating high and low those were presented at fast and slow paces while subjects ignored the stimuli. At the slow pace, tones were heard as alternating high and low pitches, and no MMN was elicited. At the fast pace a streaming effect was induced and an MMN was observed for the low stream, indicating a preattentive locus for the streaming effect. The high deviant did not elicit an MMN. MMNs were obtained to both the high and low deviants when the interval between the across-stream deviance was lengthened to more than 250 ms in Experiment 2, indicating that the MMN system is susceptible to processing constraints.},
author = {Sussman, E and Ritter, W and Vaughan, H G},
booktitle = {Psychophysiology},
institution = {Department of Neuroscience, Albert Einstein College of Medicine, Bronx, New York 10461, USA. esussman@balrog.aecom.yu.edu},
number = {1},
pages = {22--34},
pmid = {10098377},
title = {{An investigation of the auditory streaming effect using event-related brain potentials.}},
volume = {36},
year = {1999}
}
@article{Sussman1998,
abstract = {The purpose of this study was to test the previous report that generation of the mismatch negativity (MMN) component of event-related brain potentials (ERPs) is indifferent to the predictable occurrence of stimulus deviance. A pattern of standards (S) and deviants (D) were delivered in a predictable fashion (SSSSD) at two different speeds (1.3 s and 100 ms). An MMN was obtained to the D position tone at the slow but not the fast pace. These results demonstrate that, unlike the P3 component, the MMN is sensitive to the predictable occurrence of stimulus deviance when the predictability can be detected by the brain within the estimated limits of sensory memory.},
author = {Sussman, E and Ritter, W and Vaughan, H G},
institution = {Albert Einstein College of Medicine, Department of Neuroscience, Kennedy Center, Bronx, NY 10461, USA.},
issn = {0959-4965},
journal = {Neuroreport},
number = {18},
pages = {4167--4170},
pmid = {9926868},
title = {{Predictability of stimulus deviance and the mismatch negativity.}},
volume = {9},
year = {1998}
}
@article{Sussman1998a,
abstract = {The mismatch negativity (MMN), a component of event-related potentials (ERP), was used to investigate the effect of attention on auditory stream segregation. Subjects were presented with sequences of alternating high and low tones that occurred at a constant rate, which they ignored. When subjects ignored the stimuli, the three-tone standard and deviant sequences contained within the high- and low-pitched tones did not emerge and no MMNs were obtained. Subjects were then instructed to attend to the high-pitched tones of the stimulus sequences and detect the within-stream deviants. When subjects selectively attended the high-pitched tones, MMNs were obtained to the deviants within both the attended and unattended streams. These results indicate that attention can produce segregation such that the sequences of low- and high-pitched tones are available to the automatic deviance detection system that underlies the generation of the MMN. Selective attention can alter the organization of sensory input in the early stages of acoustic processing.},
author = {Sussman, Elyse and Ritter, Walter and Vaughan, Herbert G.},
issn = {00068993},
journal = {Brain Research},
keywords = {Auditory sensory memory,Auditory stream segregation,Event-related potential,Mismatch negativity,Selective attention},
number = {1},
pages = {130--138},
pmid = {9602095},
title = {{Attention affects the organization of auditory input associated with the mismatch negativity system}},
volume = {789},
year = {1998}
}
@article{Sussman2002,
abstract = {We recorded event-related potentials (ERPs) and magnetic fields (ERFs) of the human brain to determine whether top-down control could modulate the initial organization of sound representations in the auditory cortex. We presented identical sound stimulation and manipulated top-down processes by instructing participants to either ignore the sounds (Ignore condition), to detect pitch changes (Attend-pitch condition), or to detect violations of a repeating tone pattern (Attend-pattern condition). The ERP results obtained in the Attend-pattern condition dramatically differed from those obtained with the other two task instructions. The magnetoencephalogram (MEG) findings were fully compatible, showing that the neural populations involved in detecting pattern violations differed from those involved in detecting pitch changes. The results demonstrate a top-down effect on the sound representation maintained in auditory cortex. Â© 2002 Elsevier Science B.V. All rights reserved.},
author = {Sussman, Elyse and Winkler, Istv\'{a}n and Huotilainen, Minna and Ritter, Walter and N\"{a}\"{a}t\"{a}nen, Risto},
issn = {09266410},
journal = {Cognitive Brain Research},
keywords = {Auditory attention,Auditory organization,Event-related potential,Mismatch negativity,N2b},
number = {3},
pages = {393--405},
pmid = {11919003},
title = {{Top-down effects can modify the initially stimulus-driven auditory organization}},
volume = {13},
year = {2002}
}
@article{Trude2013,
abstract = {Although foreign accents can be highly dissimilar to native speech, existing research suggests that listeners readily adapt to foreign accents after minimal exposure. However, listeners often report difficulty understanding non-native accents, and the time-course and specificity of adaptation remain unclear. Across five experiments, we examined whether listeners could use a newly learned feature of a foreign accent to eliminate lexical competitors during on-line speech perception. Participants heard the speech of a native English speaker and a native speaker of Qu\'{e}bec French who, in English, pronounces /i/ as [. i] (e.g., weak as wick) before all consonants except voiced fricatives. We examined whether listeners could learn to eliminate a shifted /i/-competitor (e.g., weak) when interpreting the accented talker produce an unshifted word (e.g., wheeze). In four experiments, adaptation was strikingly limited, though improvement across the course of the experiment and with stimulus variations indicates learning was possible. In a fifth experiment, adaptation was not improved when a native English talker produced the critical vowel shift, demonstrating that the limitation is not simply due to the fact the accented talker was non-native. These findings suggest that although listeners can arrive at the correct interpretation of a foreign accent, this process can pose significant difficulty. Â© 2013 Elsevier Inc.},
author = {Trude, Alison M. and Tremblay, Annie and Brown-Schmidt, Sarah},
issn = {0749596X},
journal = {Journal of Memory and Language},
keywords = {Accommodation,Eye-tracking,Foreign accent,Perception,Speech},
number = {3},
pages = {349--367},
pmid = {24014935},
title = {{Limitations on adaptation to foreign accents}},
volume = {69},
year = {2013}
}
@article{Trude2012,
author = {Trude, AM and Brown-Schmidt, S},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trude, Brown-Schmidt - 2012 - Talker-specific perceptual adaptation during online speech perception.pdf:pdf},
journal = {Language and Cognitive Processes},
number = {April 2013},
pages = {37--41},
title = {{Talker-specific perceptual adaptation during online speech perception}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2011.597153},
volume = {27},
year = {2012}
}
@article{VanLinden2007,
abstract = {Listeners hearing an ambiguous phoneme flexibly adjust their phonetic categories in accordance with information telling what the phoneme should be (i.e., recalibration). Here the authors compared recalibration induced by lipread versus lexical information. Listeners were exposed to an ambiguous phoneme halfway between /t/ and /p/ dubbed onto a face articulating /t/ or /p/ or embedded in a Dutch word ending in /t/ (e.g., groot [big]) or /p/ (knoop [button]). In a posttest, participants then categorized auditory tokens as /t/ or /p/. Lipread and lexical aftereffects were comparable in size (Experiment 1), dissipated about equally fast (Experiment 2), were enhanced by exposure to a contrast phoneme (Experiment 3), and were not affected by a 3-min silence interval (Experiment 4). Exposing participants to 1 instead of both phoneme categories did not make the phenomenon more robust (Experiment 5). Despite the difference in nature (bottom-up vs. top-down information), lipread and lexical information thus appear to serve a similar role in phonetic adjustments.},
author = {van Linden, Sabine and Vroomen, Jean},
institution = {Department of Psychology, Tilburg University, The Netherlands.},
issn = {0096-1523},
journal = {Journal of experimental psychology. Human perception and performance},
number = {6},
pages = {1483--1494},
pmid = {18085958},
title = {{Recalibration of phonetic categories by lipread speech versus lexical information.}},
volume = {33},
year = {2007}
}
@phdthesis{VanNoorden1975,
abstract = {The perception of isolated tones has been studied extensively in psychophysics. Relations have been established between objective variables such as frequency spectrum, intensity and duration on the one hand and subjective variables such as pitch, timbre, loudness and subjection duration on the other (see e.g. Tobias, 1970). Much less attention has been paid to the perception of tone sequences. Study of the perception of sequences of tones teaches us nothing new if we consider it as a sequence of perceptions of tones - which is only the case if there is a long time between successive tones. In sequneces where the toens follow one another in quick succession, effects are observed which indicate that the tones are not processed invidiually by the perception system. On the one hand we find various types of mutual interaction between sucessive tones, such as forward and backward masking, loudness interactions and duration interactions. On the other hand, a kind of connection is foudn between the successive perceived tones. It is this coherence which will be the main subject of this investigation.},
author = {van Noorden, L and Schouten, J F},
booktitle = {Institute for Perception Research },
publisher = {University of Eindhoven},
title = {{Temporal Coherence in the Perception of Tone Sequences}},
volume = {Ph. D.},
year = {1975}
}
@article{VanWijngaarden2002,
abstract = {The intelligibility of speech pronounced by non-native talkers is generally lower than speech pronounced by native talkers, especially under adverse conditions, such as high levels of background noise. The effect of foreign accent on speech intelligibility was investigated quantitatively through a series of experiments involving voices of 15 talkers, differing in language background, age of second-language (L2) acquisition and experience with the target language (Dutch). Overall speech intelligibility of L2 talkers in noise is predicted with a reasonable accuracy from accent ratings by native listeners, as well as from the self-ratings for proficiency of L2 talkers. For non-native speech, unlike native speech, the intelligibility of short messages (sentences) cannot be fully predicted by phoneme-based intelligibility tests. Although incorrect recognition of specific phonemes certainly occurs as a result of foreign accent, the effect of reduced phoneme recognition on the intelligibility of sentences may range from severe to virtually absent, depending on (for instance) the speech-to-noise ratio. Objective acoustic-phonetic analyses of accented speech were also carried out, but satisfactory overall predictions of speech intelligibility could not be obtained with relatively simple acoustic-phonetic measures.},
author = {van Wijngaarden, Sander J and Steeneken, Herman J M and Houtgast, Tammo},
institution = {TNO Human Factors, P.O. Box 23, 3769 ZG Soesterberg, The Netherlands. vanWijngaarden@tm.tno.nl},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {6},
pages = {3004--3013},
pmid = {12509022},
title = {{Quantifying the intelligibility of speech in noise for non-native talkers.}},
volume = {112},
year = {2002}
}
@article{Vroomen2007,
abstract = {Exposure to incongruent auditory and visual speech produces both visual recalibration and selective adaptation of auditory speech identification. In an earlier study, exposure to an ambiguous auditory utterance (intermediate between /aba/ and /ada/) dubbed onto the video of a face articulating either /aba/ or /ada/, recalibrated the perceived identity of auditory targets in the direction of the visual component, while exposure to congruent non-ambiguous /aba/ or /ada/ pairs created selective adaptation, i.e. a shift of perceived identity in the opposite direction [Bertelson, P., Vroomen, J., \& de Gelder, B. (2003). Visual recalibration of auditory speech identification: a McGurk aftereffect. Psychological Science, 14, 592-597]. Here, we examined the build-up course of the after-effects produced by the same two types of bimodal adapters, over a 1-256 range of presentations. The (negative) after-effects of non-ambiguous congruent adapters increased monotonically across that range, while those of ambiguous incongruent adapters followed a curvilinear course, going up and then down with increasing exposure. This pattern is discussed in terms of an asynchronous interaction between recalibration and selective adaptation processes. Â© 2006 Elsevier Ltd. All rights reserved.},
author = {Vroomen, Jean and van Linden, Sabine and de Gelder, B\'{e}atrice and Bertelson, Paul},
journal = {Neuropsychologia},
keywords = {After-effect,Auditory-visual speech,McGurk effect,Perceptual learning,Recalibration,Selective adaptation,Speechreading},
number = {3},
pages = {572--577},
pmid = {16530233},
title = {{Visual recalibration and selective adaptation in auditory-visual speech perception: Contrasting build-up courses}},
volume = {45},
year = {2007}
}
@article{Watson2008,
author = {Watson, DG and Arnold, JE and Tanenhaus, MK},
doi = {10.1016/j.cognition.2007.06.009},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Watson, Arnold, Tanenhaus - 2008 - Tic Tac TOE Effects of predictability and importance on acoustic prominence in language production.pdf:pdf},
journal = {Cognition},
pages = {1548--1557},
title = {{Tic Tac TOE: Effects of predictability and importance on acoustic prominence in language production}},
url = {http://www.sciencedirect.com/science/article/pii/S0010027707001783},
volume = {106},
year = {2008}
}
@article{Witteman2013,
abstract = {We investigated how the strength of a foreign accent and varying types of experience with foreign-accented speech influence the recognition of accented words. In Experiment 1, native Dutch listeners with limited or extensive prior experience with German-accented Dutch completed a cross-modal priming experiment with strongly, medium, and weakly accented words. Participants with limited experience were primed by the medium and weakly accented words, but not by the strongly accented words. Participants with extensive experience were primed by all accent types. In Experiments 2 and 3, Dutch listeners with limited experience listened to a short story before doing the cross-modal priming task. In Experiment 2, the story was spoken by the priming task speaker and either contained strongly accented words or did not. Strongly accented exposure led to immediate priming by novel strongly accented words, while exposure to the speaker without strongly accented tokens led to priming only in the experiment's second half. In Experiment 3, listeners listened to the story with strongly accented words spoken by a different German-accented speaker. Listeners were primed by the strongly accented words, but again only in the experiment's second half. Together, these results show that adaptation to foreign-accented speech is rapid but depends on accent strength and on listener familiarity with those strongly accented words.},
author = {Witteman, Marijt J and Weber, Andrea and McQueen, James M},
issn = {1943-393X},
journal = {Attention, perception \& psychophysics},
keywords = {cross-modal priming,dutch,experience,foreign-accented speech,german-accented,it is estimated that,more than half of,perceptual learning,s population,short- and long-term,the world},
number = {3},
pages = {537--56},
pmid = {23456266},
title = {{Foreign accent strength and listener familiarity with an accent codetermine speed of perceptual adaptation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23456266},
volume = {75},
year = {2013}
}
@article{Wolfe2004,
abstract = {As you drive into the centre of town, cars and trucks approach from several directions, and pedestrians swarm into the intersection. The wind blows a newspaper into the gutter and a pigeon does something unexpected on your windshield. This would be a demanding and stressful situation, but you would probably make it to the other side of town without mishap. Why is this situation taxing, and how do you cope?},
author = {Wolfe, Jeremy M and Horowitz, Todd S},
institution = {Visual Attention Laboratory, Brigham and Women's Hospital and Harvard Medical School, 64 Sidney Street, Cambridge, Massachusetts 02139, USA. wolfe@search.bwh.harvard.edu},
journal = {Nature reviews. Neuroscience},
number = {6},
pages = {495--501},
pmid = {15152199},
title = {{What attributes guide the deployment of visual attention and how do they do it?}},
volume = {5},
year = {2004}
}
@article{Yeshurun1998,
abstract = {Covert attention, the selective processing of visual information at a given location in the absence of eye movements, improves performance in several tasks, such as visual search and detection of luminance and vernier targets. An important unsettled issue is whether this improvement is due to a reduction in noise (internal or external), a change in decisional criteria, or signal enhancement. Here we show that attention can affect performance by signal enhancement. For a texture segregation task in which performance is actually diminished when spatial resolution is too high, we observed that attention improved performance at peripheral locations where spatial resolution was too low, but impaired performance at central locations where spatial resolution was too high. The counterintuitive impairment of performance that we found at the central retinal locations appears to have only one possible explanation: attention enhances spatial resolution.},
author = {Yeshurun, Y and Carrasco, M},
institution = {Department of Psychology, New York University, New York 10003, USA.},
issn = {0028-0836},
journal = {Nature},
number = {6706},
pages = {72--75},
pmid = {9817201},
title = {{Attention improves or impairs visual performance by enhancing spatial resolution.}},
volume = {396},
year = {1998}
}
