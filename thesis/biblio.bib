@article{Bertelson2003,
abstract = {The kinds of aftereffects, indicative of cross-modal recalibration, that are observed after exposure to spatially incongruent inputs from different sensory modalities have not been demonstrated so far for identity incongruence. We show that exposure to incongruent audiovisual speech (producing the well-known McGurk effect) can recalibrate auditory speech identification. In Experiment 1, exposure to an ambiguous sound intermediate between /aba/ and /ada/ dubbed onto a video of a face articulating either /aba/ or /ada/ increased the proportion of /aba/ or /ada/ responses, respectively, during subsequent sound identification trials. Experiment 2 demonstrated the same recalibration effect or the opposite one, fewer /aba/ or /ada/ responses, revealing selective speech adaptation, depending on whether the ambiguous sound or a congruent nonambiguous one was used during exposure. In separate forced-choice identification trials, bimodal stimulus pairs producing these contrasting effects were identically categorized, which makes a role of postperceptual factors in the generation of the effects unlikely.},
author = {Bertelson, Paul and Vroomen, Jean and {De Gelder}, B\'{e}atrice},
journal = {Psychological Science},
number = {6},
pages = {592--597},
pmid = {14629691},
title = {{Visual Recalibration of Auditory Speech Identification: A McGurk Aftereffect}},
volume = {14},
year = {2003}
}
@article{Borsky2000,
abstract = {The purpose of this study was to investigate the temporal unfolding of local acoustic information and sentence context using both cross-modal interference (CMI) and word-monitoring tasks. The timing of sentence context effects have important theoretical implications for models of language processing (e.g., initial context independence vs. initial interaction). Yet, different tasks tend to yield different results. For both experiments, stimuli from an acoustically manipulated "goat-to-coat" continuum were embedded in sentences whose interpretation was biased toward either "goat" or "coat." In experiment 1 (CMI), the primary task was listening to sentences for comprehension; the interference task was a word/nonword decision to an unrelated visual probe that appeared at one of three positions within the sentence. Results showed immediate effects of the acoustic manipulation, but only delayed effects of sentence context. These results were interpreted to indicate that phonological processing is initially context-independent but is followed by rapid context integration. Experiment 2 used a word-monitoring task: Response times were significantly longer when sentence context was incongruent with the monitoring target, showing an immediate effect of context. The apparently contradictory results of the two experiments together support an account of language processing in which phoneme categorization is initially independent of sentence context unless an explicit judgment about the identity of the target is required.},
author = {Borsky, S and Shapiro, L P and Tuller, B},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Borsky, Shapiro, Tuller - 2000 - The temporal unfolding of local acoustic information and sentence context.pdf:pdf},
institution = {University of North Florida, Department of Psychology, Jacksonville 32224, USA. sborsky@unf.edu},
journal = {Journal of Psycholinguistic Research},
keywords = {adult,humans,phonetics,random allocation,reaction time,speech,speech acoustics,speech perception,speech perception physiology,speech physiology},
number = {2},
pages = {155--168},
publisher = {Springer},
title = {{The temporal unfolding of local acoustic information and sentence context}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10709181},
volume = {29},
year = {2000}
}
@article{Borsky1998,
abstract = {This study examined the effect of sentence context and local acoustic structure on phoneme categorization. Target stimuli from a 10-step GOAT-COAT continuum, differing only on a temporal cue for voice onset time (VOT), were embedded in carrier sentences that biased interpretation toward either "goat" or "coat." While subjects listened to the sentences they also responded as quickly as possible to a visual probe by indicating whether the probe matched the target stimulus they heard. Results showed that the interaction of VOT and sentence context significantly affected both identification and RT for stimuli near the perceptual boundary; the identification function showed a boundary shift in favor of the biased context and peak response times for each context reflected the shifted identification boundaries. In addition, response times were faster for identification of stimuli near the category boundary when responses were congruent, rather than incongruent with the sentence context. The response time differences for congruent versus incongruent responses in the boundary region are interpreted as depending on the results of initial phonological analysis; potentially ambiguous categorizations may be subject to additional evaluation in which a context-congruent response is both preferred and available earlier.},
author = {Borsky, S and Tuller, B and Shapiro, L P},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Borsky, Tuller, Shapiro - 1998 - How to milk a coat the effects of semantic and acoustic information on phoneme categorization.pdf:pdf},
institution = {Program in Complex Systems and Brain Sciences, Florida Atlantic University, Boca Raton 33431, USA.},
journal = {Journal of the Acoustical Society of America},
number = {5 Pt 1},
pages = {2670--2676},
title = {{"How to milk a coat:" the effects of semantic and acoustic information on phoneme categorization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9604360},
volume = {103},
year = {1998}
}
@article{Bradlow2008,
abstract = {This study investigated talker-dependent and talker-independent perceptual adaptation to foreign-accent English. Experiment 1 investigated talker-dependent adaptation by comparing native English listeners' recognition accuracy for Chinese-accented English across single and multiple talker presentation conditions. Results showed that the native listeners adapted to the foreign-accented speech over the course of the single talker presentation condition with some variation in the rate and extent of this adaptation depending on the baseline sentence intelligibility of the foreign-accented talker. Experiment 2 investigated talker-independent perceptual adaptation to Chinese-accented English by exposing native English listeners to Chinese-accented English and then testing their perception of English produced by a novel Chinese-accented talker. Results showed that, if exposed to multiple talkers of Chinese-accented English during training, native English listeners could achieve talker-independent adaptation to Chinese-accented English. Taken together, these findings provide evidence for highly flexible speech perception processes that can adapt to speech that deviates substantially from the pronunciation norms in the native talker community along multiple acoustic-phonetic dimensions. © 2007 Elsevier B.V. All rights reserved.},
author = {Bradlow, Ann R. and Bent, Tessa},
journal = {Cognition},
keywords = {Foreign-accented speech,Perceptual learning,Speech perception},
number = {2},
pages = {707--729},
title = {{Perceptual adaptation to non-native speech}},
volume = {106},
year = {2008}
}
@article{Brysbaert2009,
abstract = {Word frequency is the most important variable in research on word processing and memory. Yet, the main criterion for selecting word frequency norms has been the availability of the measure, rather than its quality. As a result, much research is still based on the old Kucera and Francis frequency norms. By using the lexical decision times of recently published megastudies, we show how bad this measure is and what must be done to improve it. In particular, we investigated the size of the corpus, the language register on which the corpus is based, and the definition of the frequency measure. We observed that corpus size is of practical importance for small sizes (depending on the frequency of the word), but not for sizes above 16-30 million words. As for the language register, we found that frequencies based on television and film subtitles are better than frequencies based on written sources, certainly for the monosyllabic and bisyllabic words used in psycholinguistic research. Finally, we found that lemma frequencies are not superior to word form frequencies in English and that a measure of contextual diversity is better than a measure based on raw frequency of occurrence. Part of the superiority of the latter is due to the words that are frequently used as names. Assembling a new frequency norm on the basis of these considerations turned out to predict word processing times much better than did the existing norms (including Kucera \& Francis and Celex). The new SUBTL frequency norms from the SUBTLEX(US) corpus are freely available for research purposes from http://brm.psychonomic-journals.org/content/supplemental, as well as from the University of Ghent and Lexique Web sites.},
author = {Brysbaert, Marc and New, Boris},
institution = {Ghent University, Ghent, Belgium. marc.brysbaert@ugent.be},
issn = {1554-3528},
journal = {Behavior research methods},
number = {4},
pages = {977--990},
pmid = {19897807},
title = {{Moving beyond Kucera and Francis: a critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English.}},
volume = {41},
year = {2009}
}
@misc{Clare2014,
author = {Clare, Emily},
title = {{Applying phonological knowledge to phonetic accommodation}},
year = {2014}
}
@article{Clarke-Davidson2008,
abstract = {Recent studies show that perceptual boundaries between phonetic categories are changeable with training (Norris, McQueen, \& Cutler, 2003). For example, Kraljic and Samuel (2005) exposed listeners in a lexical decision task to ambiguous /s-integral/ sounds in either s-word contexts (e.g., legacy) or integral-word contexts (e.g., parachute). In a subsequent /s/-/integral/ categorization test, listeners in the /s/ condition categorized more tokens as /s/ than did those in the /integral/ condition. The effect--termed perceptual learning in speech--is assumed to reflect a change in phonetic category representation. However, the result could be due to a decision bias resulting from the training task. In Experiment 1, we replicated the basic Kraljic and Samuel (2005) experiment and added an AXB discrimination test. In Experiment 2, we used a task that is less likely to induce a decision bias. Results of both experiments and signal detection analyses point to a true change in phonetic representation.},
author = {Clarke-Davidson, Constance M and Luce, Paul A and Sawusch, James R},
institution = {University at Buffalo, State University of New York, Buffalo, New York, USA. cmclarke@ualberta.ca},
issn = {0031-5117},
journal = {Perception \& psychophysics},
number = {4},
pages = {604--618},
pmid = {18556922},
title = {{Does perceptual learning in speech reflect changes in phonetic category representation or decision bias?}},
volume = {70},
year = {2008}
}
@article{Clopper2012,
abstract = {Words in semantically predictable sentences are more intelligible than words in less predictable or semantically anomalous sentences. However, the intelligibility benefit of semantically predictable words is reduced in noise and for non-native listeners, suggesting that semantic context contributes less to intelligibility under difficult listening conditions. The goal of the current study was to explore the effect of dialect variation on the semantic predictability benefit. Larger semantic predictability benefits were observed for more familiar dialects than less familiar dialects. In addition, more dialect differences in the size of the semantic predictability benefit were observed in conditions in which perceptual normalisation was more difficult. These results are consistent with a cue-weighting model in which dialect variation is comparable to energetic noise masking. Listeners attend less to semantic cues when perceptual normalisation for dialect variation is difficult and more to semantic cues when perceptual normalisation is easy. Words in semantically predictable sentences are more intelligible than words in less predictable or semantically anomalous sentences. However, the intelligibility benefit of semantically predictable words is reduced in noise and for non-native listeners, suggesting that semantic context contributes less to intelligibility under difficult listening conditions. The goal of the current study was to explore the effect of dialect variation on the semantic predictability benefit. Larger semantic predictability benefits were observed for more familiar dialects than less familiar dialects. In addition, more dialect differences in the size of the semantic predictability benefit were observed in conditions in which perceptual normalisation was more difficult. These results are consistent with a cue-weighting model in which dialect variation is comparable to energetic noise masking. Listeners attend less to semantic cues when perceptual normalisation for dialect variation is difficult and more to semantic cues when perceptual normalisation is easy.},
author = {Clopper, Cynthia G},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clopper - 2012 - Effects of dialect variation on the semantic predictability benefit.pdf:pdf},
journal = {Language \& Cognitive Processes},
number = {7-8},
pages = {1002--1020},
publisher = {Psychology Press},
title = {{Effects of dialect variation on the semantic predictability benefit}},
url = {http://dx.doi.org/10.1080/01690965.2011.558779},
volume = {27},
year = {2012}
}
@article{Clopper2008,
abstract = {This study explored the interaction between semantic predictability and regional dialect variation in an analysis of speech produced by college-aged female talkers from the Northern, Midland, and Southern dialects of American English. Previous research on the effects of semantic predictability has shown that vowels in high semantic predictability contexts are temporally and spectrally reduced compared to vowels in low semantic predictability contexts. In the current study, an analysis of vowel duration confirmed temporal reduction in the high predictability condition. An analysis of vowel formant structure and vowel space dispersion revealed overall spectral reduction for the Southern talkers. For the Northern talkers, more extreme Northern Cities shifting occurred in the high predictability condition than in the low predictability condition. No effects of semantic predictability were observed for the Midland talkers. These findings suggest an interaction between semantic and indexical factors in vowel reduction processes.},
author = {Clopper, Cynthia G and Pierrehumbert, Janet B},
doi = {10.1121/1.2953322},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clopper, Pierrehumbert - 2008 - Effects of semantic predictability and regional dialect on vowel space reduction.pdf:pdf},
institution = {Department of Linguistics, Northwestern University, Evanston, Illinois 60208, USA.},
issn = {1520-8524},
journal = {Journal of the Acoustical Society of America},
keywords = {Adolescent,Adult,Female,Humans,Residence Characteristics,Semantics,Speech Acoustics,Speech Intelligibility,Speech Perception,Speech Production Measurement,Time Factors,United States,Young Adult},
month = sep,
number = {3},
pages = {1682--1688},
pmid = {19045658},
publisher = {Acoustical Society of America},
title = {{Effects of semantic predictability and regional dialect on vowel space reduction}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2676620\&tool=pmcentrez\&rendertype=abstract},
volume = {124},
year = {2008}
}
@article{Connine1994,
abstract = {Four experiments investigated acoustic-phonetic similarity in the mapping process between the speech signal and lexical representations (vertical similarity). Auditory stimuli were used where ambiguous initial phonemes rendered a phoneme sequence lexically ambiguous (perceptual-lexical ambiguities). A cross-modal priming paradigm (Experiments 1, 2, and 3) showed facilitation for targets related to both interpretations of the ambiguities, indicating multiple activation. Experiment 4 investigated individual differences and the role of sentence context in vertical similarity mapping. The results support a model where spoken word recognition proceeds via goodness-of-fit mapping between speech and lexical representations that is not influenced by sentence context.},
author = {Connine, C M and Blasko, D G and Wang, J},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Connine, Blasko, Wang - 1994 - Vertical similarity in spoken word recognition multiple lexical activation, individual differences, and t.pdf:pdf},
institution = {State University of New York, Department of Psychology, Binghamton 13901.},
journal = {Perception And Psychophysics},
keywords = {connine bilingual},
number = {6},
pages = {624--636},
publisher = {Springer},
title = {{Vertical similarity in spoken word recognition: multiple lexical activation, individual differences, and the role of sentence context.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7816533},
volume = {56},
year = {1994}
}
@article{Connine1987a,
abstract = {Two experiments are reported that demonstrate contextual effects on identification of speech voicing continua. Experiment 1 demonstrated the influence of lexical knowledge on identification of ambiguous tokens from word-nonword and nonword-word continua. Reaction times for word and non-word responses showed a word advantage only for ambiguous stimulus tokens (at the category boundary); no word advantage was found for clear stimuli (at the continua endpoints). Experiment 2 demonstrated an effect of a postperceptual variable, monetary payoff, on nonword-nonword continua. Identification responses were influenced by monetary payoff, but reaction times for bias-consistent and bias-inconsistent responses did not differ at the category boundary. An advantage for bias-consistent responses was evident at the continua endpoints. The contrasting patterns of reaction-time data in the two experiments indicate different underlying mechanisms. We argue that the lexical status effect is attributable to a mechanism in which lexical knowledge directly influences perceptual processes.},
author = {Connine, C M and Clifton, C},
issn = {0096-1523},
journal = {Journal of experimental psychology. Human perception and performance},
number = {2},
pages = {291--299},
pmid = {2953858},
title = {{Interactive use of lexical information in speech perception.}},
volume = {13},
year = {1987}
}
@article{Connine1987,
author = {Connine, CM},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Connine - 1987 - Constraints on interactive processes in auditory word recognition The role of sentence context.pdf:pdf},
journal = {Journal of Memory and Language},
pages = {527--538},
title = {{Constraints on interactive processes in auditory word recognition: The role of sentence context}},
url = {http://www.sciencedirect.com/science/article/pii/0749596X87901380},
volume = {538},
year = {1987}
}
@article{Connine1991,
abstract = {Acoustic-phonetic ambiguity in word initial position was investigated in three experiments in which the ambiguous string was compatible with two lexical items. Perceptual/lexical ambiguities were embedded in sentences in which contextual information subsequent to the item served to bias the interpretation of the ambiguous string. Two experiments are reported that investigated temporal constraints on perceptual/lexical ambiguity resolution. The results suggest that commitment to a single lexical hypothesis occurs within a relatively small window of time. When information relevant to selection of the contextually appropriate word is not forthcoming within six syllables, commitment to a single lexical hypothesis is delayed but proceeds without benefit from sentence context. The results of Experiment 3 suggest that in addition to temporal constraints, a short independent clause promotes early commitment to a single lexical hypothesis. The findings are discussed in terms of assumptions of current auditory word recognition models.},
author = {Connine, Cynthia M and Blasko, Dawn G and Hall, Michael},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Connine, Blasko, Hall - 1991 - Effects of subsequent sentence context in auditory word recognition Temporal and linguistic constrainst.pdf:pdf},
journal = {Journal of Memory and Language},
number = {2},
pages = {234--250},
title = {{Effects of subsequent sentence context in auditory word recognition: Temporal and linguistic constrainst}},
url = {http://www.sciencedirect.com/science/article/B6WK4-4D7006D-5/2/52b72f93401eef7c379618e374ece032},
volume = {30},
year = {1991}
}
@article{Eisner2005,
abstract = {We conducted four experiments to investigate the specificity of perceptual adjustments made to unusual speech sounds. Dutch listeners heard a female talker produce an ambiguous fricative [?] (between [f] and [s]) in [f]- or [s]-biased lexical contexts. Listeners with [f]-biased exposure (e.g., [witlo?]; from witlof, "chicory"; witlos is meaningless) subsequently categorized more sounds on an [epsilonf]-[epsilons] continuum as [f] than did listeners with [s]-biased exposure. This occurred when the continuum was based on the exposure talker's speech (Experiment 1), and when the same test fricatives appeared after vowels spoken by novel female and male talkers (Experiments 1 and 2). When the continuum was made entirely from a novel talker's speech, there was no exposure effect (Experiment 3) unless fricatives from that talker had been spliced into the exposure talker's speech during exposure (Experiment 4). We conclude that perceptual learning about idiosyncratic speech is applied at a segmental level and is, under these exposure conditions, talker specific.},
author = {Eisner, Frank and McQueen, James M},
institution = {Max Planck Institute for Psycholinguistics, Nijmegen, The Netherlands. frank.eisner@mpi.nl},
journal = {Perception \& psychophysics},
number = {2},
pages = {224--238},
title = {{The specificity of perceptual learning in speech processing.}},
volume = {67},
year = {2005}
}
@article{Eisner2006,
author = {Eisner, Frank and McQueen, James M.},
doi = {10.1121/1.2178721},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/Eisner\_McQueen\_LongLearning.pdf:pdf},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {4},
pages = {1950},
title = {{Perceptual learning in speech: Stability over time}},
url = {http://scitation.aip.org/content/asa/journal/jasa/119/4/10.1121/1.2178721},
volume = {119},
year = {2006}
}
@article{Ganong1980,
abstract = {To investigate the interaction in speech perception of auditory information and lexical knowledge (in particular, knowledge of which phonetic sequences are words), acoustic continua varying in voice onset time were constructed so that for each acoustic continuum, one of the two possible phonetic categorizations made a word and the other did not. For example, one continuum ranged between the word dash and the nonword tash; another used the nonword dask and the word task. In two experiments, subjects showed a significant lexical effect-that is, a tendency to make phonetic categorizations that make words. This lexical effect was greater at the phoneme boundary (where auditory information is ambiguous) than at the ends of the condinua. Hence the lexical effect must arise at a stage of processing sensitive to both lexical knowledge and auditory information.},
author = {Ganong, W F},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ganong - 1980 - Phonetic categorization in auditory word perception.pdf:pdf},
journal = {Journal of Experimental Psychology: Human Perception and Performance},
keywords = {discrimination learning,humans,phonetics,speech perception},
number = {1},
pages = {110--125},
publisher = {American Psychological Assn},
title = {{Phonetic categorization in auditory word perception.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6444985},
volume = {6},
year = {1980}
}
@misc{Jesse2011,
abstract = {Listeners use lexical knowledge to adjust to speakers' idiosyncratic pronunciations. Dutch listeners learn to interpret an ambiguous sound between /s/ and /f/ as /f/ if they hear it word-finally in Dutch words normally ending in /f/, but as /s/ if they hear it in normally /s/-final words. Here, we examined two positional effects in lexically guided retuning. In Experiment 1, ambiguous sounds during exposure always appeared in word-initial position (replacing the first sounds of /f/- or /s/-initial words). No retuning was found. In Experiment 2, the same ambiguous sounds always appeared word-finally during exposure. Here, retuning was found. Lexically guided perceptual learning thus appears to emerge reliably only when lexical knowledge is available as the to-be-tuned segment is initially being processed. Under these conditions, however, lexically guided retuning was position independent: It generalized across syllabic positions. Lexical retuning can thus benefit future recognition of particular sounds wherever they appear in words.},
author = {Jesse, Alexandra and McQueen, James M.},
booktitle = {Psychonomic Bulletin \& Review},
issn = {1069-9384},
number = {5},
pages = {943--950},
pmid = {21735330},
title = {{Positional effects in the lexical retuning of speech perception}},
volume = {18},
year = {2011}
}
@article{Kalikow1977,
author = {Kalikow, DN and Stevens, KN and Elliott, LL},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalikow, Stevens, Elliott - 1977 - Development of a test of speech intelligibility in noise using sentence materials with controlled wor.pdf:pdf},
journal = {\ldots Journal of the Acoustical Society of \ldots},
number = {5},
title = {{Development of a test of speech intelligibility in noise using sentence materials with controlled word predictability}},
url = {http://link.aip.org/link/?JASMAN/61/1337/1},
volume = {61},
year = {1977}
}
@inproceedings{Kawahara2008,
abstract = {A simple new method for estimating temporally stable power spectra is introduced to provide a unified basis for computing an interference-free spectrum, the fundamental frequency (F0), as well as aperiodicity estimation. F0 adaptive spectral smoothing and cepstral liftering based on consistent sampling theory are employed for interference-free spectral estimation. A perturbation spectrum, calculated from temporally stable power and interference-free spectra, provides the basis for both F0 and aperiodicity estimation. The proposed approach eliminates ad-hoc parameter tuning and the heavy demand on computational power, from which STRAIGHT has suffered in the past.},
author = {Kawahara, H. and Morise, M. and Takahashi, T. and Nisimura, R. and Irino, T. and Banno, H.},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Consistent sampling,Periodic signal,Periodicity,Power spectrum,Speech processing},
pages = {3933--3936},
title = {{Tandem-straight: A temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, F0, and aperiodicity estimation}},
year = {2008}
}
@article{Kraljic2008a,
abstract = {Listeners are faced with enormous variation in pronunciation, yet they rarely have difficulty understanding speech. Although much research has been devoted to figuring out how listeners deal with variability, virtually none (outside of sociolinguistics) has focused on the source of the variation itself. The current experiments explore whether different kinds of variation lead to different cognitive and behavioral adjustments. Specifically, we compare adjustments to the same acoustic consequence when it is due to context-independent variation (resulting from articulatory properties unique to a speaker) versus context-conditioned variation (resulting from common articulatory properties of speakers who share a dialect). The contrasting results for these two cases show that the source of a particular acoustic-phonetic variation affects how that variation is handled by the perceptual system. We also show that changes in perceptual representations do not necessarily lead to changes in production.},
author = {Kraljic, Tanya and Brennan, Susan E and Samuel, Arthur G},
doi = {10.1016/j.cognition.2007.07.013},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/KraljicBrennanSamuel2008.pdf:pdf},
issn = {0010-0277},
journal = {Cognition},
keywords = {Adult,Culture,Female,Humans,Language,Learning,Male,Phonetics,Speech Perception,Speech Production Measurement},
month = apr,
number = {1},
pages = {54--81},
pmid = {17803986},
title = {{Accommodating variation: dialects, idiolects, and speech processing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2375975\&tool=pmcentrez\&rendertype=abstract},
volume = {107},
year = {2008}
}
@article{Kraljic2005,
abstract = {Recent work on perceptual learning shows that listeners' phonemic representations dynamically adjust to reflect the speech they hear (Norris, McQueen, \& Cutler, 2003). We investigate how the perceptual system makes such adjustments, and what (if anything) causes the representations to return to their pre-perceptual learning settings. Listeners are exposed to a speaker whose pronunciation of a particular sound (either /s/ or /integral/) is ambiguous (e.g., halfway between /s/ and /integral/). After exposure, participants are tested for perceptual learning on two continua that range from /s/ to /integral/, one in the Same voice they heard during exposure, and one in a Different voice. To assess how representations revert to their prior settings, half of Experiment 1's participants were tested immediately after exposure; the other half performed a 25-min silent intervening task. The perceptual learning effect was actually larger after such a delay, indicating that simply allowing time to pass does not cause learning to fade. The remaining experiments investigate different ways that the system might unlearn a person's pronunciations: listeners hear the Same or a Different speaker for 25 min with either: no relevant (i.e., 'good') /s/ or /integral/ input (Experiment 2), one of the relevant inputs (Experiment 3), or both relevant inputs (Experiment 4). The results support a view of phonemic representations as dynamic and flexible, and suggest that they interact with both higher- (e.g., lexical) and lower-level (e.g., acoustic) information in important ways.},
author = {Kraljic, Tanya and Samuel, Arthur G},
doi = {10.1016/j.cogpsych.2005.05.001},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/KraljicSamuel2005.pdf:pdf},
issn = {0010-0285},
journal = {Cognitive psychology},
keywords = {Adaptation, Psychological,Adult,Female,Humans,Learning,Male,Psycholinguistics,Retention (Psychology),Speech Acoustics,Speech Perception,Time Factors,Transfer (Psychology),Voice Quality},
month = sep,
number = {2},
pages = {141--78},
pmid = {16095588},
title = {{Perceptual learning for speech: Is there a return to normal?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16095588},
volume = {51},
year = {2005}
}
@article{Kraljic2006,
author = {Kraljic, Tanya and Samuel, Arthur G.},
doi = {10.3758/BF03193841},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/KraljicSamuel2006Generalization.pdf:pdf},
issn = {1069-9384},
journal = {Psychonomic Bulletin \& Review},
month = apr,
number = {2},
pages = {262--268},
title = {{Generalization in perceptual learning for speech}},
url = {http://link.springer.com/10.3758/BF03193841},
volume = {13},
year = {2006}
}
@article{Kraljic2007,
author = {Kraljic, Tanya and Samuel, Arthur G.},
doi = {10.1016/j.jml.2006.07.010},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/KraljicSamuel2006MultipleSpeakers.pdf:pdf},
issn = {0749596X},
journal = {Journal of Memory and Language},
keywords = {a speech-to-text system knows,adjustments,anyone who has used,multiple,multiple speakers,not equipped to handle,partner effects,perceptual learning,priming,speakers,speech perception,successful perfor-,that most systems are,to get even moderately},
month = jan,
number = {1},
pages = {1--15},
title = {{Perceptual adjustments to multiple speakers}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0749596X06000842},
volume = {56},
year = {2007}
}
@article{Kraljic2008,
abstract = {Perceptual theories must explain how perceivers extract meaningful information from a continuously variable physical signal. In the case of speech, the puzzle is that little reliable acoustic invariance seems to exist. We tested the hypothesis that speech-perception processes recover invariants not about the signal, but rather about the source that produced the signal. Findings from two manipulations suggest that the system learns those properties of speech that result from idiosyncratic characteristics of the speaker; the same properties are not learned when they can be attributed to incidental factors. We also found evidence for how the system determines what is characteristic: In the absence of other information about the speaker, the system relies on episodic order, representing those properties present during early experience as characteristic of the speaker. This "first-impressions" bias can be overridden, however, when variation is an incidental consequence of a temporary state (a pen in the speaker's mouth), rather than characteristic of the speaker.},
author = {Kraljic, Tanya and Samuel, Arthur G and Brennan, Susan E},
doi = {10.1111/j.1467-9280.2008.02090.x},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/KraljicSamuelBrennan2008.pdf:pdf},
issn = {0956-7976},
journal = {Psychological science},
keywords = {Adaptation, Psychological,Adolescent,Adult,Attention,Attitude,Humans,Phonetics,Reaction Time,Social Perception,Speech Perception},
month = apr,
number = {4},
pages = {332--8},
pmid = {18399885},
title = {{First impressions and last resorts: how listeners adjust to speaker variability.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18399885},
volume = {19},
year = {2008}
}
@article{Ladefoged1957,
abstract = {Most speech sounds may be said to convey three kinds of information: linguistic information which enables the listener to identify the words that are being used; socio-linguistic information, which enables him to appreciate something about the background of the speaker; and personal information which helps to identify the speaker. An e'qleriment has been carried out which shows that the linguistic information con- veyed by a vowel sound does not depend on the absolute values of its formant frequencies, but on the relationship between the formant frequencies for that vowel and the formant frequencies of other vowels pronounced by that speaker. Six versions of tile sentence Please say what this word is were synthesized on a Parametric Artificial Talking device. Four test words of the form b-(vowel)-t were also synthesized. It is shown that the identification of the test word depends on the formant structure of the introductory sentence. Some psychological implications of this e:tperiment are discussed, and hypotheses are put forward concerning the ways in which all three kinds of information are conveyed by vowels.},
author = {Ladefoged, Peter and Broadbent, D. E.},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {98--104},
title = {{Information conveyed by vowels}},
url = {http://scitation.aip.org/content/asa/journal/jasa/29/1/10.1121/1.1908694},
volume = {29},
year = {1957}
}
@article{Lieberman1963,
abstract = {Three speakers read aloud meaningful grammatical English sentences at a fast rate. Some of these sentences contained common maxims and stereotyped phrases. Other sentences which were less familiar contained, in similar phonetic environments, certain test words that also occurred in the stereotyped sentences. The test words were “excised”, i.e., gated out of all the sentences, and listening tests were performed by 43 listeners. Quite apart from these listening tests two operational measures of redundancy were derived. One measure was based on a Markovian model of perception and readers were asked to guess what word n would be after they had read all the words through word n–1 in a sentence. The second measure was based on the hypothesis that people reserve their final decision on the recognition of each word until they perceive an entire sentence. Readers read an entire sentence in the midst of which a word was represented by a dash. They were then asked to guess what the missing word was. Two groups of thirty readers each performed these written tests on the sentences from which the test words had been excised. Indexes of redundancy were computed for each test word from the percent of correct guesses that occurred for the two written tests.The results of the listening tests show that the intelligibility of the excised words is inversely proportional to the redundancy index obtained from the total sentence context. Two acoustic correlates of linguistic stress (duration and amplitude) were measured for each excised word. Stress is also apparently inversely proportional to the sentence context redundancy measure. The results of this experiment support the hypothesis that both the acoustic realization and auditory perception of a given word in a meaningful sentence may be a function of the speaker's and listener's knowledge of the semantic and grammatical information contained in the entire sentence. The experimental results also indicate that, in some circumstances, a listener may be able to identify a word only after he adapts to the speaker's voice.},
author = {Lieberman, Philip},
journal = {Language and Speech},
number = {3},
pages = {172 --187},
title = {{Some Effects of Semantic and Grammatical Context on the Production and Perception of Speech}},
url = {http://las.sagepub.com/content/6/3/172.abstract},
volume = {6},
year = {1963}
}
@article{Mattys2009,
abstract = {Effects of perceptual and cognitive loads on spoken-word recognition have so far largely escaped investigation. This study lays the foundations of a psycholinguistic approach to speech recognition in adverse conditions that draws upon the distinction between energetic masking, i.e., listening environments leading to signal degradation, and informational masking, i.e., listening environments leading to depletion of higher-order, domain-general processing resources, independent of signal degradation. We show that severe energetic masking, such as that produced by background speech or noise, curtails reliance on lexical-semantic knowledge and increases relative reliance on salient acoustic detail. In contrast, informational masking, induced by a resource-depleting competing task (divided attention or a memory load), results in the opposite pattern. Based on this clear dissociation, we propose a model of speech recognition that addresses not only the mapping between sensory input and lexical representations, as traditionally advocated, but also the way in which this mapping interfaces with general cognition and non-linguistic processes. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Mattys, Sven L. and Brooks, Joanna and Cooke, Martin},
journal = {Cognitive Psychology},
keywords = {Energetic masking,Informational masking,Processing load,Psycholinguistics,Speech segmentation,Spoken-word recognition},
number = {3},
pages = {203--243},
pmid = {19423089},
title = {{Recognizing speech under a processing load: Dissociating energetic from informational factors}},
volume = {59},
year = {2009}
}
@article{Mattys2011,
abstract = {The effect of cognitive load (CL) on speech recognition has received little attention despite the prevalence of CL in everyday life, e.g., dual-tasking. To assess the effect of CL on the interaction between lexically-mediated and acoustically-mediated processes, we measured the magnitude of the "Ganong effect" (i.e., lexical bias on phoneme identification) under CL and no CL. CL consisted of a concurrent visual search task. Experiment 1 showed an increased Ganong effect under CL. A time-course analysis of this pattern (Experiments 2 and 3) revealed that the Ganong effect decreased over time under optimal conditions, but it did not under CL. Thus, CL appears to be delaying (and perhaps preventing) listeners' ability to rely on fine phonetic detail to perform the sub-lexical task. This finding, along with an absence of measurable effects at the post-lexical level (Experiment 4) or at the lexical level (Experiment 5) and a clear negative effect of CL on perceptual discrimination (Experiment 6), suggests that the increased reliance on lexically-mediated processes under CL is the cascaded effect of impoverished encoding of the sensory input. Ways of implementing a link between CL and sensory analysis into existing models of speech recognition are proposed. ?? 2011 Elsevier Inc.},
author = {Mattys, Sven L. and Wiget, Lukas},
journal = {Journal of Memory and Language},
keywords = {Cognitive load,Divided attention,Ganong,Speech perception,Spoken-word recognition},
number = {2},
pages = {145--160},
title = {{Effects of cognitive load on speech recognition}},
volume = {65},
year = {2011}
}
@article{Mitterer2013,
abstract = {Recent evidence shows that listeners use abstract prelexical units in speech perception. Using the phenomenon of lexical retuning in speech processing, we ask whether those units are necessarily phonemic. Dutch listeners were exposed to a Dutch speaker producing ambiguous phones between the Dutch syllable-final allophones approximant [r] and dark [l]. These ambiguous phones replaced either final /r/ or final /l/ in words in a lexical-decision task. This differential exposure affected perception of ambiguous stimuli on the same allophone continuum in a subsequent phonetic-categorization test: Listeners exposed to ambiguous phones in /r/-final words were more likely to perceive test stimuli as /r/ than listeners with exposure in /l/-final words. This effect was not found for test stimuli on continua using other allophones of /r/ and /l/. These results confirm that listeners use phonological abstraction in speech perception. They also show that context-sensitive allophones can play a role in this process, and hence that context-insensitive phonemes are not necessary. We suggest there may be no one unit of perception. ?? 2013 Elsevier B.V.},
author = {Mitterer, Holger and Scharenborg, Odette and McQueen, James M.},
journal = {Cognition},
keywords = {Allophones,Perceptual learning,Phonemes,Speech perception},
number = {2},
pages = {356--361},
pmid = {23973464},
title = {{Phonological abstraction without phonemes in speech perception}},
volume = {129},
year = {2013}
}
@article{Norris2003,
abstract = {This study demonstrates that listeners use lexical knowledge in perceptual learning of speech sounds. Dutch listeners first made lexical decisions on Dutch words and nonwords. The final fricative of 20 critical words had been replaced by an ambiguous sound, between [f] and [s]. One group of listeners heard ambiguous [f]-final words (e.g., [witlo?], from witlof, chicory) and unambiguous [s]-final words (e.g., naaldbos, pine forest). Another group heard the reverse (e.g., ambiguous [na:ldbo?], unambiguous witlof ). Listeners who had heard [?] in [f]-final words were subsequently more likely to categorize ambiguous sounds on an [f]-[s] continuum as [f] than those who heard [?] in [s]-final words. Control conditions ruled out alternative explanations based on selective adaptation and contrast. Lexical information can thus be used to train categorization of speech. This use of lexical information differs from the on-line lexical feedback embodied in interactive models of speech perception. In contrast to on-line feedback, lexical feedback for learning is of benefit to spoken word recognition (e.g., in adapting to a newly encountered dialect). ?? 2003 Elsevier Science (USA). All rights reserved.},
author = {Norris, Dennis and McQueen, James M. and Cutler, Anne},
booktitle = {Cognitive Psychology},
number = {2},
pages = {204--238},
pmid = {12948518},
title = {{Perceptual learning in speech}},
volume = {47},
year = {2003}
}
@article{Pitt1993,
abstract = {Recent studies that used Ganong's (1980) identification task have produced discrepant results. The present study sought to resolve these discrepancies by examining the influence of methodological factors on phoneme identification and differences in data analysis techniques. Three factors were examined across 2 experiments: position of target phoneme, phonetic contrast, and 2 task conditions in which stimulus quality (S/N ratio) or cognitive load varied. A meta-analysis was then performed on the results from all identification studies, including the present one, in an effort to obtain additional insight on factors that influence the task. The experiments and meta-analysis identified the importance of several methodological factors in affecting identification, most notably position of the target phoneme.},
author = {Pitt, M A and Samuel, A G},
institution = {Department of Psychology, Ohio State University, Columbus 43210-1222.},
issn = {0096-1523},
journal = {Journal of experimental psychology. Human perception and performance},
number = {4},
pages = {699--725},
pmid = {8409855},
title = {{An empirical and meta-analytic evaluation of the phoneme identification task.}},
volume = {19},
year = {1993}
}
@article{Pitt2012,
author = {Pitt, MA and Szostak, CM},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pitt, Szostak - 2012 - A lexically biased attentional set compensates for variable speech quality caused by pronunciation variation.pdf:pdf},
journal = {Language and Cognitive Processes},
number = {April 2013},
pages = {37--41},
title = {{A lexically biased attentional set compensates for variable speech quality caused by pronunciation variation}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2011.619370},
year = {2012}
}
@article{Pitt2006,
abstract = {Many models of spoken word recognition posit the existence of lexical and sublexical representations, with excitatory and inhibitory mechanisms used to affect the activation levels of such representations. Bottom-up evidence provides excitatory input, and inhibition from phonetically similar representations leads to lexical competition. In such a system, long words should produce stronger lexical activation than short words, for 2 reasons: Long words provide more bottom-up evidence than short words, and short words are subject to greater inhibition due to the existence of more similar words. Four experiments provide evidence for this view. In addition, reaction-time-based partitioning of the data shows that long words generate greater activation that is available both earlier and for a longer time than is the case for short words. As a result, lexical influences on phoneme identification are extremely robust for long words but are quite fragile and condition-dependent for short words. Models of word recognition must consider words of all lengths to capture the true dynamics of lexical activation.},
author = {Pitt, Mark A and Samuel, Arthur G},
institution = {Department of Psychology, Ohio State University, Columbus, OH 43210-1222, USA. pitt.2@osu.edu},
issn = {0096-1523},
journal = {Journal of experimental psychology. Human perception and performance},
number = {5},
pages = {1120--1135},
pmid = {17002526},
title = {{Word length and lexical activation: longer is better.}},
volume = {32},
year = {2006}
}
@article{Reinisch2013a,
abstract = {Listeners use lexical knowledge to retune phoneme categories. When hearing an ambiguous sound between /s/ and /f/ in lexically unambiguous contexts such as gira[s/f], listeners learn to interpret the sound as /f/ because gira[f] is a real word and gira[s] is not. Later, they apply this learning even in lexically ambiguous contexts (perceiving knife rather than nice). Although such retuning could help listeners adapt to foreign-accented speech, research has focused on single phonetic contrasts artificially manipulated to create ambiguous sounds; however, accented speech varies along many dimensions. It is therefore unclear whether analogies to adaptation to accented speech are warranted. In the present studies, the to-be-adapted ambiguous sound was embedded in a global foreign accent. In addition, conditions of cross-speaker generalization were tested with focus on the extent to which perceptual similarity between 2 speakers' fricatives is a condition for generalization to occur. Results showed that listeners retune phoneme categories manipulated within the context of a global foreign accent, and that they generalize this short-term learning to the perception of phonemes from previously unheard speakers. However, generalization was observed only when exposure and test speakers' fricatives were sampled across a similar perceptual space. (PsycINFO Database Record (c) 2013 APA, all rights reserved).},
author = {Reinisch, Eva and Holt, Lori L},
issn = {1939-1277},
journal = {Journal of experimental psychology. Human perception and performance},
keywords = {foreign accent,lexically-guided phonetic category,perceptual learning,retuning,speaker generalization,speech perception},
number = {2},
pages = {539--555},
pmid = {24059846},
title = {{Lexically Guided Phonetic Retuning of Foreign-Accented Speech and Its Generalization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24059846},
volume = {40},
year = {2013}
}
@article{Reinisch2013,
abstract = {Native listeners adapt to noncanonically produced speech by retuning phoneme boundaries by means of lexical knowledge. We asked whether a second language lexicon can also guide category retuning and whether perceptual learning transfers from a second language (L2) to the native language (L1). During a Dutch lexical-decision task, German and Dutch listeners were exposed to unusual pronunciation variants in which word-final /f/ or /s/ was replaced by an ambiguous sound. At test, listeners categorized Dutch minimal word pairs ending in sounds along an /f/-/s/ continuum. Dutch L1 and German L2 listeners showed boundary shifts of a similar magnitude. Moreover, following exposure to Dutch-accented English, Dutch listeners also showed comparable effects of category retuning when they heard the same speaker speak her native language (Dutch) during the test. The former result suggests that lexical representations in a second language are specific enough to support lexically guided retuning, and the latter implies that production patterns in a second language are deemed a stable speaker characteristic likely to transfer to the native language; thus retuning of phoneme categories applies across languages.},
author = {Reinisch, Eva and Weber, Andrea and Mitterer, Holger},
doi = {10.1037/a0027979},
file = {:C$\backslash$:/Users/michael/Downloads/documents-export-2014-10-23/ReinischWeberMitterer\_JEPHPP2012.pdf:pdf},
issn = {1939-1277},
journal = {Journal of experimental psychology. Human perception and performance},
keywords = {Acoustic Stimulation,Adolescent,Adult,Humans,Learning,Multilingualism,Phonetics,Psycholinguistics,Speech,Speech Perception,Young Adult},
month = feb,
number = {1},
pages = {75--86},
pmid = {22545600},
title = {{Listeners retune phoneme categories across languages.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22545600},
volume = {39},
year = {2013}
}
@article{Reinisch2014,
abstract = {Listeners use lexical or visual context information to recalibrate auditory speech perception. After hearing an ambiguous auditory stimulus between /aba/ and /ada/ coupled with a clear visual stimulus (e.g., lip closure in /aba/), an ambiguous auditory-only stimulus is perceived in line with the previously seen visual stimulus. What remains unclear, however, is what exactly listeners are recalibrating: phonemes, phone sequences, or acoustic cues. To address this question we tested generalization of visually-guided auditory recalibration to (1) the same phoneme contrast cued differently (i.e., /aba/-/ada/ vs. /ibi/-/idi/ where the main cues are formant transitions in the vowels vs. burst and frication of the obstruent), (2) a different phoneme contrast cued identically (/aba/-/ada/ vs. /ama/-/ana/ both cued by formant transitions in the vowels), and (3) the same phoneme contrast with the same cues in a different acoustic context (/aba/-/ada/ vs. /ubu/-/udu/). Whereas recalibration was robust for all recalibration control trials, no generalization was found in any of the experiments. This suggests that perceptual recalibration may be more specific than previously thought as it appears to be restricted to the phoneme category experienced during exposure as well as to the specific manipulated acoustic cues. We suggest that recalibration affects context-dependent sub-lexical units.},
author = {Reinisch, Eva and Wozny, David R. and Mitterer, Holger and Holt, Lori L.},
doi = {10.1016/j.wocn.2014.04.002},
issn = {00954470},
journal = {Journal of Phonetics},
pages = {91--105},
pmid = {24932053},
title = {{Phonetic category recalibration: What are the categories?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S009544701400045X},
volume = {45},
year = {2014}
}
@article{Scarborough2010,
author = {Scarborough, R},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scarborough - 2010 - Lexical and contextual predictability Confluent effects on the production of vowels.pdf:pdf},
journal = {Laboratory phonology},
pages = {575--604},
title = {{Lexical and contextual predictability: Confluent effects on the production of vowels}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Lexical+and+contextual+predictability:+confluent+effects+on+the+production+of+vowels\#0},
year = {2010}
}
@article{Sumner2011,
author = {Sumner, Meghan},
doi = {10.1016/j.cognition.2010.10.018},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sumner - 2011 - The role of variation in the perception of accented speech.pdf:pdf},
issn = {0010-0277},
journal = {Cognition},
number = {1},
pages = {131--136},
publisher = {Elsevier B.V.},
title = {{The role of variation in the perception of accented speech}},
url = {http://dx.doi.org/10.1016/j.cognition.2010.10.018},
volume = {119},
year = {2011}
}
@article{Trude2012,
author = {Trude, AM and Brown-Schmidt, S},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trude, Brown-Schmidt - 2012 - Talker-specific perceptual adaptation during online speech perception.pdf:pdf},
journal = {Language and Cognitive Processes},
number = {April 2013},
pages = {37--41},
title = {{Talker-specific perceptual adaptation during online speech perception}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01690965.2011.597153},
volume = {27},
year = {2012}
}
@article{VanLinden2007,
abstract = {Listeners hearing an ambiguous phoneme flexibly adjust their phonetic categories in accordance with information telling what the phoneme should be (i.e., recalibration). Here the authors compared recalibration induced by lipread versus lexical information. Listeners were exposed to an ambiguous phoneme halfway between /t/ and /p/ dubbed onto a face articulating /t/ or /p/ or embedded in a Dutch word ending in /t/ (e.g., groot [big]) or /p/ (knoop [button]). In a posttest, participants then categorized auditory tokens as /t/ or /p/. Lipread and lexical aftereffects were comparable in size (Experiment 1), dissipated about equally fast (Experiment 2), were enhanced by exposure to a contrast phoneme (Experiment 3), and were not affected by a 3-min silence interval (Experiment 4). Exposing participants to 1 instead of both phoneme categories did not make the phenomenon more robust (Experiment 5). Despite the difference in nature (bottom-up vs. top-down information), lipread and lexical information thus appear to serve a similar role in phonetic adjustments.},
author = {van Linden, Sabine and Vroomen, Jean},
institution = {Department of Psychology, Tilburg University, The Netherlands.},
issn = {0096-1523},
journal = {Journal of experimental psychology. Human perception and performance},
number = {6},
pages = {1483--1494},
pmid = {18085958},
title = {{Recalibration of phonetic categories by lipread speech versus lexical information.}},
volume = {33},
year = {2007}
}
@article{Vroomen2007,
abstract = {Exposure to incongruent auditory and visual speech produces both visual recalibration and selective adaptation of auditory speech identification. In an earlier study, exposure to an ambiguous auditory utterance (intermediate between /aba/ and /ada/) dubbed onto the video of a face articulating either /aba/ or /ada/, recalibrated the perceived identity of auditory targets in the direction of the visual component, while exposure to congruent non-ambiguous /aba/ or /ada/ pairs created selective adaptation, i.e. a shift of perceived identity in the opposite direction [Bertelson, P., Vroomen, J., \& de Gelder, B. (2003). Visual recalibration of auditory speech identification: a McGurk aftereffect. Psychological Science, 14, 592-597]. Here, we examined the build-up course of the after-effects produced by the same two types of bimodal adapters, over a 1-256 range of presentations. The (negative) after-effects of non-ambiguous congruent adapters increased monotonically across that range, while those of ambiguous incongruent adapters followed a curvilinear course, going up and then down with increasing exposure. This pattern is discussed in terms of an asynchronous interaction between recalibration and selective adaptation processes. © 2006 Elsevier Ltd. All rights reserved.},
author = {Vroomen, Jean and van Linden, Sabine and de Gelder, B\'{e}atrice and Bertelson, Paul},
journal = {Neuropsychologia},
keywords = {After-effect,Auditory-visual speech,McGurk effect,Perceptual learning,Recalibration,Selective adaptation,Speechreading},
number = {3},
pages = {572--577},
pmid = {16530233},
title = {{Visual recalibration and selective adaptation in auditory-visual speech perception: Contrasting build-up courses}},
volume = {45},
year = {2007}
}
@article{Watson2008,
author = {Watson, DG and Arnold, JE and Tanenhaus, MK},
doi = {10.1016/j.cognition.2007.06.009},
file = {:C$\backslash$:/Users/michael/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Watson, Arnold, Tanenhaus - 2008 - Tic Tac TOE Effects of predictability and importance on acoustic prominence in language production.pdf:pdf},
journal = {Cognition},
pages = {1548--1557},
title = {{Tic Tac TOE: Effects of predictability and importance on acoustic prominence in language production}},
url = {http://www.sciencedirect.com/science/article/pii/S0010027707001783},
volume = {106},
year = {2008}
}
