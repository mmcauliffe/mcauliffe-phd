%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Background}

Listeners of a language are faced with a large degree of phonetic variability when interacting with their fellow language users.  
Speakers can have different sizes, different genders, and different backgrounds that make speech sound categories, at first blush, overlapping in distribution and hard to separate in acoustic domains.
In addition to properties of the speaker varying, properties of the listener's mindset can vary as well.  
Whether the listener has prior knowledge about a speaker characteristic can affect their perception of that speaker's speech \citep{Pitt2012}.
Despite variability on the part of both the listener and the speaker, listeners can maintain a large degree of perceptual constancy, interpreting disparate and variable productions as belonging to one category.
Exposure to a speaker affects a listener's perceptual system for that speaker to increase their ability to understand that speaker.
Broadly, perceptual learning or perceptual adaptation refers to the updating of distributions corresponding to a sound category for a particular speaker.

In this dissertation, perceptual learning will be tested in conditions that manipulate the top-down biases of the listeners and the bottom-up properties of the exposure speaker.
The hypothesis I test in this dissertation is that the more evidence a listener has toward a specific word, be it acoustic, lexical or semantic/sentential, the stronger the link between an ambiguous sound embedded in that word and a sound category will be.
The more strong evidence that a listener accrues for a given sound category of a speaker, the more perceptual learning will take place.
This chapter reviews the recent literature on perceptual learning in speech perception, which has primarily focused on the duration of perceptual learning effects (Section~\ref{sec:retention}), and the ability of listeners to generalize perceptual learning to new speakers  and to new categories and contexts (Section~\ref{sec:generalization}). 
This literature has used consistent top-down and bottom-up processing conditions to elicit perceptual learning effects. 
 The goal of this dissertation is to examine the robustness and degree of perceptual learning across different processing conditions.

\section{Perceptual learning}
\label{sec:perceptuallearning}

%\citet{Gibson1953}: "Improvement is arbitrarily defined, then, as closer, more precise, more immediate approximation of \emph{O}'s judgment to the appropriate physical standard or measure" (p403).  Also: "Practice, for the present purposes, will be defined as any controlled activity of \emph{O} which involves repeated perception of the test stimuli or ones similar to them.  This definition assumes attention on the part of \emph{O}, but it deliberately omits any requirement of reinforcement, correction or reward."

\citet{Norris2003} began the recent set of investigation into perceptual learning in speech.
\citet{Norris2003} exposed one group of Dutch listeners to a fricative halfway between /s/ and /f/ at the ends of words like \emph{olif} "olive" and \emph{radijs} "radish", and another group to the ambiguous fricative at the ends of nonwords, like \emph{blif} and \emph{blis}.
Following exposure, both groups of listeners were tested on their categorization on a fricative continuum from 100\% /s/ to 100\% /f/. 
Listeners exposed to the ambiguous fricative at the end of words shifted their categorization behaviour, while those exposed to them at the end of nonwords did not.  The exposure using words was further differentiated by the bias introduced by the words.  
Half the tokens ending in the ambiguous fricative formed a word if the fricative was interpreted as /s/ but not if it was interpreted as /f/, and the others were the reverse.  
Listeners exposed only to the /s/-biased tokens categorized more of the /f/-/s/ continuum as /s/, and listeners exposed to /f/-biased tokens categorized more of the continuum as /f/.  
The ambiguous fricative was associated with either /s/ or /f/ dependent on the bias of the word, which led to an expanded category for that fricative at the expense of the other category.

Perceptual learning is a well established phenomenon in the psychology and psychophysics literature. 
Training can improve a participants ability to discriminate in many disparate modalities, such as visual acuity, somatosensory spatial resolution, weight estimation, and discrimination of hue and acoustic pitch \citep[for review]{Gibson1953}. 
 In this literature, perceptual learning is the improvement of a perceiver to judge the physical characteristics of objects in the world through training that assumes attention on the task, but doesn't require reinforcement, correction or reward.
This definition of perceptual learning corresponds more to what is termed ``selective adaptation'' in the speech perception literature rather than what is termed ``perceptual learning'', ``perceptual adaptation'' or ``perceptual recalibration.''  
In speech perception, selective adaptation is the phenomenon where listeners that are exposed repeatedly to a narrow distribution for a sound category, narrow their own perceptual category, resulting in a change in variance of the category, not of the mean of the category (along some acoustic-phonetic dimension) \citep{Eimas1973,Samuel1986,Vroomen2007}.
 Perceptual learning or recalibration in the speech perception literature is a more broad updating of perceptual categories, either in mean or variance \citep{Norris2003, Vroomen2007}.  
!!Something something differences between psychophysics literature and speech perception literature!!

%\citet{SamuelKraljic2009}

%\citet{Sumner2011}


%\section{Timecourse}

%Mitterer, H., & Reinisch, E. (in press). No delays in application of perceptual learning in speech recognition: Evidence from eye tracking.Journal of Memory and Language.

%\citet{Vroomen2004} \cite{Kleinschmidt2011}

%\citet{Trude2012}

In addition to lexically-guided perceptual learning, unambiguous visual cues to sound identity can cause perceptual learning as well (referred to as perceptual recalibration in that literature).
In \citet{Bertelson2003}, an auditory continuum from /aba/ to /ada/ was synthesized and paired with a video of a speaker producing /aba/ and a video of /ada/.  
Participants first completed a pretest that identified the maximally ambiguous step of the /aba/-/ada/ auditory continuum. 
 In eight blocks, participants were randomly exposed to the ambiguous auditory token paired with video for /aba/ or with the video for /ada/.  Following each block, they completed a short categorization test.  
These categorization tests showed clear perceptual recalibration effects, such that the participant was more likely to respond with /aba/ if they had been exposed to video of /aba/ paired with the ambiguous token in the preceding block, and likewise for /ada/.

\citet{vanLinden2007} compared the perceptual recalibration effects from the visual lipread paradigm \citep{Bertelson2003} to the standard lexically-guided perceptual learning paradigm \citep{Norris2003}.  
Lipread recalibration and perceptual learning effects had comparable size, lasted equally as long, were enhanced when presented with a contrasting sound, and were both unaffected by periods of silence between exposure and categorization.  
The effects did not last through prolonged testing in this study, unlike in other studies \citep{Kraljic2005,Eisner2006}.


\subsection{Retention}
\label{sec:retention}

\citet{Kraljic2005} looked at the strength of the perceptual learning effect across time in an experimental session with intervening tasks between exposure and testing. 
Native English listeners were exposed to ambiguous fricatives in between /s/ and /\textesh/ in words that biased interpretation toward either /s/ or /\textesh/ in a lexical decision task either spoken by a female voice or a male voice.
Participants that completed an /s/-/\textesh/ categorization task immediately following exposure and also those that performed an unrelated visual task between exposure and categorization showed a large perceptual learning effect.  
However, participants that performed a task involving auditory input from the same speaker as they were exposed to showed attenuated perceptual learning effects when the speech contained correct versions of the ambiguous fricatives, i.e. typical /s/ tokens when they were exposed to the ambiguous sibilant in all /s/ words in the exposure task.  
The perceptual learning effect in this condition was almost eliminated. 
Participants that had an intervening task involving auditory input that did not have any /s/ or /\textesh/ tokens showed perceptual learning effects comparable to those with no intervening task.  
When the intervening task involved auditory input from a different speaker than exposure, perceptual learning effects were as robust as when there was no intervening task.
Percpetual learning was a robust and persisted across non-auditory tasks as well as linguistic ones so long as no new exposure to the particular speaker characteristic was present.

\citet{Eisner2006} looked at the stability of perceptual learning. 
Dutch participants first completed a categorization task for a continuum of /s/ to /f/ and then listened to a story that contained ambiguous fricative between /s/ and /f/, with one version having the ambiguous fricative in /s/ contexts and the other having it in /f/ contexts.
Immediately following, they completed another /s/ to /f/ categorization task.  
Finally, 12 hours later, they completed the categorization task once again.
Half the participants started at 9 am and completed the final categorization task at 9 pm the same day, and the other half started at 9 pm and completed the final categorization at 9 am the following day.
Perceptual learning effects were found in both categorization tasks following exposure, with no significant differences between the two times. 
 The perceptual learning effects found were robust across time and across intervening language exposure, as the participants tested at 9 am and 9 pm likely had many interactions with other people in between.

Perceptual learning effects are robust across time.  
Given no other interactions with a given speaker, a listener's behaviour for that speaker's speech will remain constant over the course of a day. 
However, the perceptual system remains plastic.  
If a listener is exposed to a speaker trait in the first interaction, but the trait disappears in later interactions, the perceptual system will attenuate to the newer evidence.

\subsection{Generalization}
\label{sec:generalization}

\subsubsection{Generalizing to new speakers}
\label{sec:speakergeneralization}

\citet{Kraljic2007} looked at different sound contrasts and the ability of participants to generalize across speakers.  
The first contrast was voicing between /t/ and /d/.  
When exposed to ambiguous stops between /t/ and /d/ corresponding to /d/ for one speaker, and ambiguous stops correpsonding to /t/ for another, perceptual learning effects corresponded to the most recent exposure, regardless of whether the test voice was the same as that recent exposure voice.  
The second contrast they used was sibilants of /s/ and /\textesh/. 
For this contrast, they found the speaker-specificity effect found in the literature, where recency of the exposure did not affect the perceptual learning for a particular voice.
They argue that the differences between these two contrasts lies in the relative indexical information carried by the sibilants and the lack of much indexical information carried in the stops.
Voicing contrasts in onset position are delimited clearly along the voice-onset time dimension, with variability between native English speakers producing little ambiguity between /t/ and /d/.
Sibilant productions can vary dramatically, however.
For instance, male speakers of English produce /s/ and /\textesh/ with spectral means lower than female speakers of English, with overlapping distributions between male /s/ productions and female /\textesh/ productions, and continua of synthetic sibilants are categorized differently depending on the gender of the speaker who produced the rest of the word \citep{Strand1996}.  

\citet{Eisner2005} looked at what manipulations could cause perceptual learning to generalize to additional talkers.  
Using a Dutch lexical decision task with ambiguous fricatives in between /s/ and /f/, they tested categorization on a contniuum between /s/ and /f/ from two talkers.  
When exposed to one talker's ambiguous fricative and tested on another speaker's continuum, no perceptual learning effect was found.  
There were two conditions where the fricatives in the exposure and categorization came from the same speaker, but the speakers of the carrier speech differed.  
In one, the ambiguous fricative from the categorization speaker's produtions was spliced into the exposure tokens, and in the other, the exposure talker's /s/ to /f/ continuum were spliced with vowels from the categorization talker.
In these two conditions, they did find a perceptual learning effect across speakers, despite reports by the participants of hearing different speakers, suggesting a sound-specificity effect in addition (or potentially causing) speaker-specificity effects. 

\citet{Reinisch2013} looked at perceptual learning of speaker characteristics across languages.  
When exposed to a native Dutch speaker speaking English with ambiguous fricatives between /s/ and /f/, participants show a perceptual learning effect when categorizing Dutch minimal pairs differing only in whether one sound is an /f/ or /s/ spoken by the same speaker.  
When exposed to the same native Dutch speaker speaking Dutch words and nonwords with the same ambiguous fricative, native Dutch listeners and L2 Dutch listeners (native German speakers) show comparable perceptual learning effects.

\citet{Reinisch2013a} exposed native English listeners to a female Dutch-accented voice speaking words and nonwords, where words ending in /s/ or /f/ instead ended either in a natural token or an ambiguous fricative between /s/ and /f/.  
Listeners showed a perceptual learning effect for the exposure talker as well as another female Dutch-accented voice, showing cross-speaker generalization.  
However, when categorizing a continuum from /s/ to /f/ of a male Dutch-accented voice, listeners did not generalize the perceptual learning effect, but there was a slight perceptual learning effect for the first block of categorization tokens for the male voice that disappeared in later blocks.  
Pretests of the continua revealed that the male voice continuum was heavily biased towards /s/, with steps 1-4 of the continuum consistently heard as /s/. 
 For the exposure female voice, Steps 1-4 were already ambiguous between /s/ and /f/, with the proportion /s/ response less than 60\% for all steps.  
The new female voice was responded to similar to the exposure female voice, but with less ambiguity for the initial step.  
When the continua for the exposure female voice and the male voice were more closely matched by removing the first four steps of the male voice and removing 4 steps along the continuum of the female voice, generalization of the perceptual learning effects matched those for the new female voice.  
These findings suggest that the perceptual system leverages known distributions that may not be speaker specific.  
When new tokens fall outside that known distribution, as in the male voice categorization, perceptual learning for that voice occurred using the relatively abundant unambiguous examples for that speaker to adjust the perceptual system for that talker.

\citet{Kraljic2005}, in addition to the findings reported in in Section~\ref{sec:retention}, also found an asymmetry in how listeners generalized perceptual learning.  
Across all of their experiments, categorization of the same voice as exposure produced perceptual learning.  
When the continuum from /s/ to /\textesh/ from the male voice was categorized following exposure to the female voice, perceptual learning effects were consistently present.
When the voice of the exposure and categorization was reversed, the consistency of perceptual learning effects disappeared. 
They performed an acoustic analysis of the spectral means for the categorization items and the exposure items for each talker.  
The spectral means of the voices for the continuum steps were well separated, but the exposure items were much closer together.  
The spectral mean of the ambiguous sibilant for the female voice fell in between the ranges of the female continuum steps and the male continuum steps, but the spectral mean exposure ambiguous sibilant for the male voice was squarely in the range of the male continuum steps.  
The asymmetry of speaker generalization can then be attributed to the female exposure ambiguous items being acoustically (and therefore perceptually) similar enough to generalize the perceptual learning for the female voice to the male voice.

From these studies, the clearest thread through all of them is the degree of acoustic similarity or perceptual similarity between the exposure stimuli that prompts perceptual learning and whatever continuum the listeners are tested on.  
Stops and fricative do not actually appear to behave categorically different, per se, but their differences in behaviour can be attributed to the fact that stops are acoustically more similar to each other in general than fricatives are.
The generalization from exposure items only to similar testing items is found in the psychophysics literature as well, where perceptual learning is argued to reside or affect the early sensory pathways, where stimuli are represented with the greatest detail \citep{Gilbert2001}.


\subsubsection{Generalizing to other categories and contexts}
\label{sec:othergeneralization}

\citet{Kraljic2006} looked at two sound contrasts (/d/-/t/ and /b/-/p/) that shared a feature, namely voicing.  
Participants were exposed to an ambiguous coronal stop in between /d/ and /t/ in English words and then tested on two continua with two different voices.  
The categorization continuum that was presented first, from /b/ to /p/, showed the stronger perceptual learning effects than the second continuum from /d/ to /t/, even though participants were exposed to ambiguous tokens between /d/ and /t/. 
 Both old voices and new voices for the categorization task produced perceptual learning effects.

\citet{Mitterer2013} exposed participants to either words ending with /r/ or /l/ or to words beginning with /r/ or /l/ in Dutch, and tested each group's perceptual learning behaviour on continua from /r/ to /l/ in both initial and final positions.  
If listeners were exposed to an ambiguous sound between /r/ and /l/ at the ends of words, they showed a perceptual learning effect for the continuum that ended with an /r/ or /l/, but not for the continuum that began with an /r/ or an /l/.  
The inverse behaviour was observed for the group that was exposed to an ambiguous sound between /r/ and /l/ at the beginnings of words.  
They argue that the category being affected is not a context-insensitive phoneme, but rather a context sensitive allophone.  
\citet{Jesse2011} found that listeners did indeed generalize across positions, but their stimuli were the fricatives /s/ and /f/, and the same physical fricatives were the same across all positions.

%\citet{Clare2014}

\citet{Reinisch2014} utilized a visually-guided recalibration paradigm from \citet{Bertelson2003} to test whether acoustic cues to place of articulation could be recalibrated with unambiguous visual feedback.  
In Experiment 1, they trained listeners by exposing one group to the maximally ambiguous token between /aba/ and /ada/ with either a video of /ada/ or /aba/ and another group to the maximally ambiguous token between /ibi/ and /idi/ with either a video of /ibi/ or /idi/.  
In vowel context of /a/, the consonant portion of the token was silenced so that only formant cues to place of articulation would be available.  
In the vowel context of /i/, the consonant part was not silenced, but since formant transitions in the /i/ context are the same for labials and dentals, the only cues to place of articulation would be in the consonant itself.  
Afterwards, when they categorized auditory only continua of /aba/ to /ada/ and /ibi/ to /idi/ with the same silencing of the consonant in the /a/ context.  They found a recalibration effect for the continuum participants were exposed to, but no generalization effect to the novel continuum.  
Experiment 2 followed the same setup as Experiment 1, with the same /aba/ to /ada/ as in Experiment 1, but replaced the /ibi/ to /idi/ continuum with a /ama/ to /ana/ continuum.  
As in Experiment 1, clear recalibration effects were found for the continuum participants were exposed to, but not for the novel continuum.  
Experiment 3 followed the same procedure but used a /ubu/ to /udu/ continuum instead of an /ibi/ to /idi/ continuum.  
The /u/ context provides formant transition cues to place of articulation, so the consonant was silenced for this continuum as well.  
As before, there were perceptual recalibration effects for the continuum participants were exposed to, but not to the novel continuum.  
Across all of these, there is a context specifity effect. 

\citet{Kraljic2008a} exposed participants to ambiguous sibilants between /s/ and /\textesh/ in two different contexts.  
In one, the ambiguous sibilants were intervocalic, and in the other, they occurred as part of a /str/ cluster.  
Participants exposed to the ambiguous sound intervocalically showed a perceptual learning effect, while those exposed to the sibilants in /str/ environments did not.  
The sibilant in /str/ often surfaces closer to [\textesh] in many varieties of English, due to coarticulatory effects from the other consonants in the cluster, but the coarticulatory effects for merging /s/ and /\textesh/ are much weaker in intervocalic position.  
They argue that the interpretation of the ambiguous sound is done in context of the surrounding sounds, and only when the pronunciation variant is unexplainable from context is the variant learned and attributed to the speaker, see also \citet{Kraljic2008}.  
In addition to a continuum of /asi/ to /a\textesh i/, they also tested a continuum from /astri/ to /a\textesh tri/, and found comparable perceptual learning effects across both continua for those exposed to the intervocalic ambiguous sibilants, but no perceptual learning effects on either continua for the other condition, showing a context insentivity absent in other studies.

\section{Attention}

In this dissertation, attention is used as a way to reweight the contribution of bottom-up acoustic information and top-down linguistic information.
Attention has been found to have a role on perceptual learning in the psychophysics literature.  
For instance, \citet{Ahissar1993} found that in general, learning global detection (orientation of arrays of lines) does not transfer to local detection (singleton detection that differs in angle), and vice versa.  However, there was a small degree of local detection learned from global detection, with the singleton popping out from its field as highly salient.

In the auditory domain, similar asymmetries have been found between global and local processing.
In \citet{Sussman2002}, participants were presented with three types of tones in different orders, with one second stimulus onset asynchrony (SOA).  
The order of the tones were either random, with tone 1 more frequent than tone 2, which in turn was more than tone 3, or the order of the tones had a general pattern of five tones (1 1 1 1 2).  
They looked at event-related potentials (ERPs) following tone 2, specifically focusing on the mismatch negativity (MMN) ERP and the N2b ERP, which is evoked when a stimulus is attentively detected as being different from regular stimuli.
In addition to the two patterns, there were three attentional conditions.  
In one, participants were instructed to ignore the auditory stimuli and read a book.  
In the second, participants were instructed to listen to the tone pitch and press a key when they heard the lowest pitch one (tone 3). 
 In the final condition, participants were told of the five tone pattern and instructed to respond when one of the tones was replaced by the lowest tone.
When participants were ignoring the auditory stimuli, there was a MMN response following tone 2.  
When attending to the pitch of the tones, regardless of the tone pattern, there was both a MMN response and an N2b response following tone 2.  
When participants attended to the pattern, there was neither a MMN response nor an N2b response after tone 2.
These results suggest that auditory representations can be manipulated by attention, resulting in processing differences.

However, attention can only modulate the representation to the degree that the acoustics allow.
In other similar studies with different stimulus onset asynchronies, patterns were detected more readily.
At a fast presentation rate (100-ms SOA), no MMN was present, even when participants were told to ignore the auditory stimuli \citep{Sussman1998}.
The fast presentation rate is more conducive to chunking the incoming stream into a pattern, whereas the slow presentation rate allows for the participants to process each tone in isolation or as part of a bigger pattern.
Similar results were found for varying SOA in an auditory streaming task \citep{Sussman1998,Sussman1998a}.

\citet{Scharenborg2014} presents a perceptual learning study of older Dutch listeners in the model of \citet{Norris2003}.  
In addition to the exposure and test phases, these older listeners completed three tests for hearing loss, selective attention and attention-switching control.  
They found no evidence that perceptual learning was influenced by listeners' hearing loss or selective attention abilities, but they did find a significant relationship between a listener's attention-switching control and their perceptual learning.  
Listeners with worse attention-switching control showed greater perecptual learning effects, which the authors ascribed to an increased reliance on lexical information.  
Older listeners had previously been shown to have smaller perceptual learning effects as compared to younger listeners, but the differences were most prominent directly following exposure \citep{Scharenborg2013}.  
Younger listeners initially had a larger perceptual learning effect in the first block of testing, but the effect lessened over the subsequent blocks.  
Older listeners showed smaller initial perceptual learning effects, but no such decay.  
\citet{Scharenborg2013} also found that performance in the lexical decision task significantly affected the perceptual learning in the testing phase.

\section{Lexical bias}

Lexical bias is the primary way through which perceptual learning is induced in the experimental literature.  Given that ambiguous productions must be associated with a word to induce lexically-guided perceptual learning \citep{Norris2003}, I predict that differing degrees of lexical bias will lead to differing degrees of perceptual learning.  The stronger the lexical bias, the stronger the link will be between the ambiguous sound and the sound category (via the word).


Lexical bias, also known as the Ganong Effect, refers to the tendency for listeners to interpret a speaker's production as a meaningful word rather than a nonsense word.  
For instance, given a continuum from a nonword like \emph{dask} to word like \emph{task} that differs only in one sound, listeners in general are more likely to interpret any step along the continuum as the word endpoint rather than the nonword endpoint \citep{Ganong1980}. 
This bias is exploited in perceptual learning studies to allow for noncanonical, ambiguous productions of a sound to be linked to pre-existing sound categories.

Studies looking at lexical bias use a phoneme categorization task, where participants identify a sound at the beginning or end of a word from two possible options that vary in one dimension.  
For instance, given a continuum from /t/ to /d/, participants are asked to identify the sound at the beginning or end as either /t/ or /d/. 
Lexical bias effects are calculated based on two continua, where words are formed at opposite ends. 
For the /t/ to /d/ continua, one would form a word at the /t/ end, such as \emph{task} and one would form a word at the other end, such as \emph{dash}.  
Lexical bias effects are then calculated from the different categorization behaviour for these two continua.

%Word length
Lexical bias varies in strength according to several factors.  
First, the length of the word in syllables has a large effect on lexical bias, with longer words showing stronger lexical bias than shorter words \citep{Pitt2006}.  
Continua formed using trisyllabic words, such as \emph{establish} and \emph{malpractice}, were found to show consistently large lexical bias effects than monosyllabic words, such as \emph{kiss} and \emph{fish}.  
\citet{Pitt2006} also found that lexical bias from trisyllabic words was robust across experimental conditions, but lexical bias from monosyllabic words was more fragile and condition dependent.  They argue that these affects arise from both the greater bottom-up information present in longer words and the greater lexical competition for shorter words.

%Position in the word
\citet{Pitt2012} used a lexical decision task with a continuum of fricatives from /s/ to /\textesh/ embedded in words differing in the position of a sibilant.  
They found that ambiguous fricatives earlier in the word, such as \emph{serenade} or \emph{chandelier}, lead to greater nonword responses than the same ambiguous fricatives embedded later in a word, such as \emph{establish} or \emph{embarass}.  
In the experiments and meta-analysis of phoneme identification results presented in \citet{Pitt1993}, they found that, for monosyllabic word frames, token-final targets produce more robust lexical bias effects than token-initial targets.

%Attention
\citet{Pitt2012} additionally investigated the role of attention in modulating lexical bias.  
When listeners were told that the speaker's /s/ and /\textesh/ were ambiguous and to listen carefully to ensure correct responses, they were less tolerant of noncanonical productions across all positions in the word.  
That is, participants attending to the speaker's sibilants were less likely to accept the modified production as a word than participants given no particular instructions about the sibilants.
Given that the task listeners performed was a lexical decision task, the default attentional set for the task, termed ``diffuse'' by \citet{Pitt2012}, would have attention distributed across both acoustic-phonetic and lexical domains.  Listeners given the instructions about the speaker's /s/ productions had a ``focused'' attentional set, with more weighting on the acoustic-phonetic domain than the ``diffuse'' attentional set.

%Cognitive load
Lexical bias is affected by processing conditions.  
In their first experiment, \citet{Mattys2011} found that lexical bias has a larger effect when listeners have more cognitive load than when they have no cognitive load from concurrent tasks.  
In their second experiment, the same cognitive load and non-cognitive load conditions were present, but listeners had to respond to the phoneme identification task within 1500 ms.  
In this experiment, the lexical bias across cognitive load conditions was identical and similar to the cognitive load condition of experiment 1.  
In experiment 3, listeners could only respond \emph{after} 1500 ms, and the effect of cognitive load re-emerged with similar magnitude to experiment 1.  
In the remainder of their experiments, they examined whether cognitive load increased word activation or decreased sensory information, the two primary possible explanations for the pattern of results found in experiment 1.  
They found that semantic priming was not significantly different across cognitive loads, but fine-grained discrimination suffered under cognitive load conditions.  
Thus, the effect of cognitive load observed is a function of impoverished fine detail leading to a greater reliance on lexical information.  
However, the timecourse for the incorporation of these different sources of information is interesting, with lexical information playing a larger role earlier on in processing and sensory signal information being incoporated later on.  
Earlier responses are more likely to be lexically biased than later responses.
The relative weightings of lexical information and acoustic information are malleable in different processing conditions.

\section{Semantic predictability}

Semantic predictability has been shown to have effects on speech production in the same direction as lexical predictability, and the two appear to not interact \citep{Scarborough2010}.  No experiments in perceptual learning have used linguistic biases other than lexical ones, but we predict compounding effects for perceptual learning, given the compounding effects in production.  Semantic predictability and lexical bias should combine to form a stronger association between a deviant production and the sound category, leading to greater perceptual learning.

Sentences are highly predictable when they contain words prior to the final word that points almost definitively to the identity of the final word.  
For instance, the sentence fragment \emph{The cow gave birth to the...} from \citet{Kalikow1977} is almost guaranteed to be completed with the word \emph{calf}.  
On the other hand, a fragment like \emph{She is glad Jane called about the...} is far from having a guaranteed completion, other than having the category of noun. 
Semantic predictability has been found to have effects on both production and perception, which will be discussed in the two following sections

In general, semantic predictability results in acoustically reduced word tokens.  
In \citet{Scarborough2010}, words produced in highly predictable frames tend to be shorter in duration and have less dispersed vowel realizations.  
This semantic predictability effect did not interact with neighbourhood density, a lexical factor that proxies for the amount of lexical competition a word has.  
Words with many neighbours, and therefore less lexical predictability, had longer durations and more dispersed vowel realizations.  
For both the lexical and the semantic predictability, high predictability led to less distinct word realizations, and low predictability led to more distinct word realizations, independently of each other.

In a study looking at semantic predictability across dialects, \citet{Clopper2008} found that not all dialects realize the effects of semantic predictability the same.  
For the Southern dialect of American English, the results were much the same as in \citet{Scarborough2010}, showing temporal and spectral reduction in high predictabilty environments.  
However, speakers in the Midland dialect showed no such effect, and speakers of the Northern dialect showed more extreme Northen Cities shifting in the the high predictability environment.

Despite the temporal and spectral reduction found in high predictability contexts, high predictability sentences are generally more intelligible.  
Sentences that form a semantically coherent whole have higher word identification rates across varying signal-to-noise ratios \citep{Kalikow1977}.  
However, when words at the ends of predictive sentences are excised from their context, they tend to be less intelligible than words excised from non-predictive contexts \citep{Lieberman1963}.

\section{Current contribution}

Perceptual learning effects require two important aspects to be involved in the exposure phase.  
There must be some ambiguous acoustic aspect to be learned, and there must be an unambiguous link between the ambiguous acoustics and a sound category. 
This link can be provided through bottom-up information, such as in the visual domain \citep{Bertelson2003}, or it can be top-down information, such as the lexical status of the tokens \citep{Norris2003}. 
Most of the literature within the domain of lexically-guided perceptual learning has been on the presence or absence of perceptual learning in the categorization phases, with manipulations to the categorization phase to test for generalization.  
The question that I am interested in is in the linking of ambiguous tokens to sound categories.  
Can manipulations in the exposure phase that reduce the reliability of the linking cause differences in perceptual learning?

In this dissertation,  I will induce manipulations of lexical bias and attention in Experiments 1 and 2, and manipulations of sentence predictability and attention in Experiment 3. 
Lexical bias has been shown to affect phoneme categorization tasks \citep{Ganong1980}, and can be manipulated by position of the ambiguous sound in the word and attention \citep{Pitt2012}.  
Sentence predictability, has likewise been found to affect phoneme categorization tasks similar to lexical bias \citep{Borsky1998}, and can be manipulated by the preceding words in the sentence \citep{Kalikow1977}.  
The interaction of sentence predictability with attention has not been explicitly studied, but the interaction should be similar to lexical bias, given findings that sentence predictability exerts similar effects on phone categorization as lexical bias \citep{Borsky1998}.
