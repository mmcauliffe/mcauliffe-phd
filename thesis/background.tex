%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Introduction}
In addition to properties of the speaker varying, a listener's attention or goals in an interaction can vary, such as paying attention to a non-native speaker or being distracted by planning upcoming utterances or by another task entirely.  

Listeners are faced with a large degree of phonetic variability when interacting with their fellow language users.  
Speakers differ in size, gender, and sociolect which makes speech sound categories overlap in acoustic dimensions.
Despite this variation, listeners can interpret disparate and variable productions as belonging to a single word type or sound category, a phenomenon referred to as perceptual constancy in categorization studies \citep{Shankweiler1977, Kuhl1979} and as recognition equivalence in word recognition tasks \citep{Sumner2013}.
One of the processes for achieving this constancy is perceptual learning, whereby perceivers update a perceptual category based on contextual factors.

In the speech perception literature perceptual learning refers to the updating of distributions corresponding to a sound category, such as /s/ or /\textesh/, following exposure to speech with a modification to the sound category's distribution \citep{Norris2003}.
Exposure to a speaker affects a listener's perceptual system for that speaker,  even after only a few tokens \citep{Vroomen2007, Kraljic2008} or within the span of a single utterance \citep{Ladefoged1957}.
Perceptual learning is not limited to single speakers, and exposure to multiple speakers sharing a non-native accent increases the intelligibility of new speakers with that accent \citep{Bradlow2008}.

Generalization to novel contexts has been a locus of investigation within the speech-focused perceptual learning literature. Across most studies, exposure to modified sound categories generalizes well to other words and nonwords when the modified exposure tokens are embedded in real words \citep{Norris2003, Reinisch2013}.  
This paradigm is referred to as lexically-guided perceptual learning, as listeners are exposed to the modified sounds in the context of real words in a lexical decision task.
On the other hand, generalization appears to be more limited when perceptual learning is induced through visually-guided paradigms in which listeners are exposed to a modified category through matching it to an unambiguous video signal.  
The perceptual learning exhibited from these visually-guided experiments is only found to influence the specific nonword that perceivers are exposed to, and not other similar nonwords \citep{Reinisch2014}.  
This kind of exposure-specificity effect has been widely reported in perceptual learning studies in the psychophysics literature \citep{Gibson1953}.

Why, then, does lexically-guided perceptual learning produce such generalization?
Here I posit that these results can be understood by considering the attentional set exploited in the exposure phase.
An attentional set is a strategy that is employed by perceivers to prioritize certain aspects of stimuli, such as color or shape in the visual domain.
In the speech perception literature, two broad attentional sets have been posited under varying names \citep{Cutler1987, Pitt2012}.  
The first is a \emph{comprehension-oriented} or \emph{diffuse} attentional set -- this is the attentional set assumed to operate during normal language use.  
When oriented towards comprehension, listeners are focused on comprehending the intended message of the speech, and a comprehension set is promoted by tasks that focus on word identity and word recognition.
The comprehension-oriented attentional set is elicited in lexically-guided perceptual learning paradigms through their use of lexical decision tasks and the embedding of modified sound categories in word tokens. A second kind of attentional set is a \emph{perception-oriented} or \emph{focused} attentional set, where a listener is focused more on the low-level signal properties of the speech rather than the message.
The perception-oriented attentional set is promoted by tasks such as phoneme/syllable monitoring or mispronunciation detection.
The tasks used in visually-guided perceptual learning and perceptual learning within the psychophysics literature can be thought of as eliciting this attentional set, due to their focus on stimuli that are devoid of linguistic meaning.

The hypothesis of this dissertation is that perceivers who adopt a more comprehension-oriented attentional set will show more generalization than those who adopt a more perception-oriented attentional set.
To test this hypothesis, I use a lexically-guided perceptual learning paradigm to expose listeners to an /s/ category modified to sound more like /\textesh/.  
Groups of participants differ in whether comprehension-oriented or perception-oriented attentional sets are favored when processing the modified /s/ category based on experimental manipulations.
The favoring of attentional sets are implemented in four ways across the three experiments presented in this dissertation.  
Two manipulations are linguistic in nature, and one is instruction-based, and the fourth is stimuli-based.
The rest of this section is devoted to an overview of these manipulations and their motivations.

The first linguistic manipulation that affects attentional sets is the position of the modified /s/ in the exposure words.  
Accurate perception is most critical when expectations are low, as perception of highly expected elements serves a more confirmatory role.
Some groups of participants are exposed to the modified sound only at the beginnings of words (e.g. \emph{silver}, \emph{settlement}) and other groups are exposed to the category only in the middle of words (e.g. \emph{carousel}, \emph{fossil}).
Word-initial positions lack the expectations afforded to the word-medial positions, and lexical information exhibits less of an effect on word-initial positions as compared to later positions \citep{Pitt2006}.
As such, word-initial exposure is predicted to promote a more perception-oriented attentional set, and word-medial exposure is predicted to promote a more comprehension-oriented attentional set.  
Experiments 1 and 2 use this manipulation in Chapter~\ref{chap:lexdec}, and further background is given in Section~\ref{sec:lexicalbias} of this chapter.

The second linguistic manipulation employed is the context that a word appears in.  
In Experiments 1 and 2, participants are exposed to the modified sound category in words in isolation, as in previous work \citep{Norris2003}.  
However, in Experiment 3, the words containing the sound category have been embedded in sentences that are either predictive or unpredictive of the target word.
Using sentence frames is predicted to promote comprehension-oriented attentional sets more than words in isolation.
Increasing the predictability of a word increases the expectations for the sounds in those words as well, mirroring the word-position manipulation above.
Further background on the use of the sentence frames is given in Section~\ref{sec:semanticpredictability}.

Participants in all three experiments receive the same general instructions for the exposure task, but groups of participants in each experiment receive additional instructions about the nature of the /s/ category, following previous studies \citep{Pitt2012}.
Without any additional instructions, the task is predicted to promote a comprehension-oriented attentional set
The instructions about /s/ are expected to promote a perception-oriented attentional set.
Section~\ref{sec:attention} contains background on attention and instructions.

The stimuli-based manipulation is the typicality of the modified /s/ category -- Experiments 1 and 2 differ in this respect. 
In line with previous work \citep{Norris2003}, participants in Experiment 1 are exposed to a modified category halfway between /s/ and /\textesh/. 
Participants in Experiment 2 are exposed to an even more atypical /s/ -- the modified fricative is more /\textesh/-like than /s/-like.  
A more atypical category is predicted to promote a perception-oriented attentional set than a more typical category because its atypicality is predicted to be more perceptually salient.

Perceptual salience is relevant in this dissertation only in so far as it promotes a perception-oriented attentional set.
Salience is a widely-used and poorly-defined term across literatures.
For the purposes of this dissertation I adopt the following definition:  an element is salient if it is unpredictable from context or easily distinguishable from other possible elements.  
The sound category that is being learned in this dissertation already has salient signal properties (e.g., in the form of high frequency, relatively high amplitude, aperiodic noise).
Therefore, increasing salience of the modified /s/ category is a function of embedding it a linguistic position with little conditioning context.  
Increasing the acoustic distance of the modified category to the typical distribution of /s/, and in turn increasing its distinguishability, will also increase the salience of the category and promote a more perception-oriented attentional set.

The results of these experiments will be contextualized in the existing perceptual learning literature and in models of perceptual learning.
Section~\ref{sec:perceptuallearning} reviews the perceptual learning literature in greater depth, as well as conceptual models that have been proposed for it \citep{Clark2013, Kleinschmidt2011}.
Broadly speaking, perceptual learning is well accounted for by a Bayesian model, where differences between expectations and observed input cause an error signal which updates future expectations, leading to perceptual learning.
The results of the experiments in this dissertation support a more nuanced attention mechanism, however, than that proposed in \citet{Clark2013}.
The mechanism for attention proposed by Clark is gain-based such that increased attention increases the weight of error signals
Thus, increased attention should lead to increased perceptual learning.
Generalization of perceptual learning, however, is dependent on the task and the attentional set being employed.
An expanded attention mechanism is therefore proposed to unify the results of perceptual learning studies across the psychophysics and speech perception literatures, encompassing the results of the current experiments.
The proposed mechanism inhibits error propagation beyond the level where attention is directed.  
In a perception-oriented attentional set, perceptual learning is hypothesized to occur at the initial perception where encoding is more fine-grained, inhibiting generalization.  
Comprehension-oriented attentional sets are predicted to allow for greater error propagation and abstraction, leading to greater levels of generalization.

The structure of the thesis is as follows.
This chapter reviews the recent literature on perceptual learning in speech perception (Section~\ref{sec:perceptuallearning}), as well as literature on linguistic expectations (Section~\ref{sec:linguistic}), attention (Section~\ref{sec:attention}), and category typicality (Section~\ref{sec:signal}) as they relate to the three experiments of this dissertation.
Chapter~\ref{chap:lexdec} will detail two experiments using a lexically-guided perceptual learning paradigm, each with different conditions for levels of lexical bias and attention.  
The two experiments differ in the acoustic properties of the exposure tokens, with the first experiment using a slightly atypical /s/ category that is halfway between /s/ and /\textesh/
The second experiment using a more atypical /s/ category that is more /\textesh/-like than /s/-like.  
Chapter~\ref{chap:sent} details an experiment using a novel perceptual learning paradigm that manipulates semantic predictability to increase the linguistic expectations during exposure.
Finally, Chapter~\ref{chap:conclusion} summarizes the results and places them in a theoretical framework.
The perceptual learning literature has generally used consistent processing conditions to elicit perceptual learning effects, and a goal of this dissertation is to examine the robustness and degree of perceptual learning across conditions that manipulate the two primary attentional sets.

\section{Perceptual learning}
\label{sec:perceptuallearning}

Perceptual learning is a well established phenomenon in the psychology and psychophysics literature. 
Training can improve a perceiver's ability to discriminate in many disparate modalities (e.g., visual acuity, somatosensory spatial resolution, weight estimation, and discrimination of hue and acoustic pitch \citep[for review]{Gibson1953}). 
In the psychophysics literature, perceptual learning is an improvement in a perceiver's ability to judge the physical characteristics of objects in the world through training that assumes attention on the task, but does not require reinforcement, correction, or reward.
This definition of perceptual learning corresponds more to what is termed ``selective adaptation'' in the speech perception literature rather than what is termed ``perceptual learning'', ``perceptual adaptation'' or ``perceptual recalibration.''  
In speech perception, selective adaptation is the phenomenon where listeners that are exposed repeatedly to a narrow distribution of a sound category narrow their own perceptual category.
This results in a change in variance of the category, but not of the mean of the category (along some acoustic-phonetic dimension) \citep{Eimas1973,Samuel1986,Vroomen2007}.
Perceptual learning or recalibration in the speech perception literature is a more broad updating of perceptual categories either in mean or variance \citep{Norris2003, Vroomen2007}.

\citet{Norris2003} began the recent set of investigations into lexically-guided perceptual learning in speech.
Norris and colleagues exposed one group of Dutch listeners to a fricative halfway between /s/ and /f/ at the ends of words like \emph{olif} ``olive'' and \emph{radijs} ``radish'', while exposing another group to the ambiguous fricative at the ends of nonwords, like \emph{blif} and \emph{blis}.
Following exposure, both groups of listeners were tested on their categorization of a fricative continuum from 100\% /s/ to 100\% /f/. 
Listeners exposed to the ambiguous fricative at the end of words shifted their categorization behaviour, while those exposed to the same sounds at the end of nonwords did not.  The exposure using words was further differentiated by the bias introduced by the words.  That is, half the tokens ending in the ambiguous fricative formed a word if the fricative was interpreted as /s/ but not if it was interpreted as /f/, and the others were the reverse.  
Listeners exposed only to the /s/-biased tokens categorized more of the /f/-/s/ continuum as /s/, and listeners exposed to /f/-biased tokens categorized more of the continuum as /f/.  
The ambiguous fricative was associated with either /s/ or /f/ according to the bias induced by the word, which led to an expanded category for that fricative at the expense of the other category.
These results crucially show that perceptual categories in speech are malleable, and that the linguistic system of the listener facilitates generalization to that category in new forms and contexts.

In addition to lexically-guided perceptual learning, unambiguous visual cues to sound identity can cause perceptual learning as well; this is referred to as perceptual recalibration.
In \citet{Bertelson2003}, an auditory continuum from /aba/ to /ada/ was synthesized and paired with a video of a speaker producing /aba/ or /ada/.  
Participants first completed a pretest that identified the maximally ambiguous step of the /aba/-/ada/ auditory continuum. 
In eight blocks, participants were randomly exposed to the ambiguous auditory token paired with video for /aba/ or /ada/.  Following each block, they completed a short categorization test.  
Participants showed perceptual learning effects, such that they were more likely to respond with /aba/ if they had been exposed to the video of /aba/ paired with the ambiguous token in the preceding block, and vice versa for /ada/.

\citet{vanLinden2007} compared the perceptual recalibration effects from the visually-guided perceptual learning paradigm \citep{Bertelson2003} to the standard lexically-guided perceptual learning paradigm \citep{Norris2003}.  
Speech-read recalibration and perceptual learning effects had comparable sizes, lasted equally as long, were enhanced when presented with a contrasting sound, and were both unaffected by periods of silence between exposure and categorization.  
The perceptual learning effects for both lexically- and visually-guided paradigms did not last through prolonged testing.
However, in \citet{Kraljic2005} and \citet{Eisner2006}, lexically-guided perceptual learning effects persisted through intermediate tasks, as long as the task did not involve any contradictory evidence of the trait learned.

Visually-guided perceptual learning in speech perception has been modeled using a Bayesian belief updating framework \citep{Kleinschmidt2011}.  
A more general Bayesian framework for perception and action in the brain is the predictive coding model \citep{Clark2013}.
In Bayesian belief updating, the model categorizes the incoming stimuli based on multimodal cues, and then updates the distribution to reflect that categorization.  
This updated conditional distribution is then used for future categorizations in an iterative process.  
\citet{Kleinschmidt2011} effectively model the results of the behavioural study in \citet{Vroomen2007} in a Bayesian framework, with models fit to each participant capturing the perceptual recalibration and selective adaptation shown over the course of the experiment.  
A similar, but more broad, framework is that of predictive coding \citep{Clark2013}. 
This framework uses a hierarchical generative model that aims to minimize prediction error between bottom-up sensory inputs and top-down expectations.  
Mismatches between the top-down expectations and the bottom-up signals generate error signals that are used to modify future expectations.  
Perceptual learning then is the result of modifying expectations to match learned input.

Perceptual learning in the psychophysics literature has shown a large degree of exposure-specificity, where observers only show learning effects on the same or very similar stimuli as those they were trained on. 
As such, perceptual learning has been argued to reside or affect the early sensory pathways, where stimuli are represented with the greatest detail \citep{Gilbert2001}.  
Perceptual learning in speech perception has also shown a large degree of exposure-specificity, where participants do not generalize cues across speech sounds \citep{Reinisch2014} or across speakers unless the sounds are sufficiently similar across exposure and testing \citep{Eisner2005, Kraljic2005, Kraljic2007, Reinisch2013a}.  
Crucially, lexically-guided perceptual learning in speech has shown a greater degree of generalization than would be expected from a purely psychophysical standpoint.  
The testing stimuli are in many ways quite different from the exposure stimuli, with participants trained on multisyllabic words ending in an ambiguous sound and tested on monosyllabic words \citep{Reinisch2013} and nonwords \citep{Norris2003, Kraljic2005}, though exposure-specificity has been found when exposure and testing use different positional allophones \citep{Mitterer2013}.

Why is lexically-guided perceptual learning more context-general?
The experiments performed in this dissertation provide evidence that this context-generality is the result of a listener's attentional set, which can be influenced by linguistic, attentional and stimulus properties.
A comprehension-oriented attentional set, where a listener's goal is to understand the meaning of speech, promotes generalization and leads to greater perceptual learning.  
A purely perception-oriented attentional set, where a listener's goal is to perceive specific qualities of a signal, does not promote generalization.
The experiments in this dissertation use the lexically-guided perceptual learning paradigm, which uses tasks oriented towards comprehension, so generalization is to be expected in general, but the more perception-oriented the attentional set, the less perceptual learning should be observed.
In terms of a Bayesian framework with error propagation, a more perception-oriented attentional set may keep error propagation more local, resulting in the exposure-specificity seen more in the psychophysics literature.
A more comprehension-oriented attentional set would propagate errors farther upward in the hierarchy of expectations.
In both cases, errors would propagate to where attention is focused, but larger updating associated with farther error propagation would lead to larger observed perceptual learning effects.
These attentional sets will be explored in more detail in Section~\ref{sec:attention} following an examination of the linguistic factors that will be manipulated in the experiments in Chapters~\ref{chap:lexdec} and \ref{chap:sent}.

\section{Linguistic expectations and perceptual learning}
\label{sec:linguistic}

Two linguistic manipulations on attentional sets are lexical bias and semantic predictability.
Chapter~\ref{chap:lexdec} presents two experiments using a standard lexically-guided perceptual learning paradigm, which uses lexical bias as the means to link an ambiguous sound to an unambiguous category.
In Chapter~\ref{chap:sent}, a novel, sententially-guided perceptual learning paradigm is used to further promote use of comprehension-oriented attentional sets.

\subsection{Lexical bias}
\label{sec:lexicalbias}

Lexical bias is the primary way through which perceptual learning is induced in the experimental speech perception literature.
Lexical bias, also known as the Ganong Effect, refers to the tendency for listeners to interpret a speaker's (noncanonical) production as a particular meaningful word rather than a nonsense word.  
For instance, given a continuum from a nonword like \emph{dask} to word like \emph{task} that differs only in the initial sound, listeners are more likely to interpret any step along the continuum as the word endpoint rather than the nonword endpoint compared to a continuum where neither endpoint is a word \citep{Ganong1980}. 
This bias is exploited in perceptual learning studies to allow for noncanonical, ambiguous productions of a sound to be readily linked to pre-existing sound categories.
Given that ambiguous productions must be associated with a word to induce lexically-guided perceptual learning \citep{Norris2003}, differing degrees of lexical bias could lead to differing degrees of perceptual learning, a prediction which is tested in Chapter~\ref{chap:lexdec}.  
The stronger the lexical bias, the stronger the link will be between the ambiguous sound and the general sound category.

Lexical bias effects have also been found for reaction time in phoneme detection tasks.
Sounds are detected faster in words than in nonwords.  However, such lexical effects are dependent on the attentional set being employed.  
If the stimuli are sufficiently repetitive (e.g. all having the same CV shape) the lexical bias effects disappear \citep{Cutler1987}.  
The monotony or variation of filler items is sufficient to bias listeners towards one attentional set or the other.
Increasing cognitive load has been found to increase lexical bias effects as well.
Performing a hard task concurrent to a phoneme categorization task results in a greater weighting of lexical information, due to impoverished auditory encoding \citep{Mattys2011}.
Both of these tasks are perception-oriented as well, and the prevalence of effects from comprehension-oriented attentional sets speaks to its prominent role in ordinary language use.

%Word length
Aspects of the stimuli items themselves have a large impact on the size of lexical bias effects.
Longer words show stronger lexical bias than shorter words \citep{Pitt2006}.  
Continua formed using trisyllabic words, such as \emph{establish} and \emph{malpractice}, were found to show consistently larger lexical bias effects than monosyllabic words, such as \emph{kiss} and \emph{fish}.  
\citet{Pitt2006} also found that lexical bias from trisyllabic words was robust across experimental conditions (e.g., compressing the durations by up to 30\%), but lexical bias from monosyllabic words was more fragile and condition dependent.
The lexical bias effects shown by monosyllabic words only approached those of trisyllabic words when the participants were told to keep response times within a certain margin and given feedback when the response time fell outside the desired range.
The reaction time monitoring could have added a greater cognitive load for participants, which has been shown to increase lexical bias effects \citep{Mattys2011}.
They argue that longer words exert stronger lexical bias due to both more conditioning information present in longer words and the greater lexical competition for shorter words.

%Position in the word
Different positions within words of a given length have stronger or weaker lexical bias effects.
\citet{Pitt2012} used a lexical decision task with a continuum of fricatives from /s/ to /\textesh/ embedded in words that differed in the position of the sibilant.  
They found that ambiguous fricatives later in the word, such as \emph{establish} or \emph{embarrass}, show greater lexical bias effects than the same ambiguous fricatives embedded earlier in the word, such as \emph{serenade} or \emph{chandelier}.
In the experiments and meta-analysis of phoneme identification results presented in \citet{Pitt1993}, they found that for monosyllabic word frames token-final targets produce more robust lexical bias effects than token-initial targets.
Considering word length and position in the word, lexical bias appears to be strengthened over the course of the word.
As a listener hears more evidence for a particular word, their expectations for hearing the rest of that word increase.

%Samuel1981
One final research paradigm that has investigated lexical biases is phoneme restoration tasks \citep{Samuel1981}.  
In this paradigm, listeners hear words with noise added to or replacing sounds and are asked to identify whether noise completely replaced part of the speech or noise was simply added to the speech.  
Lower sensitivity to noise addition versus noise replacement and increased bias for responding that noise has been added is indicative of phoneme restoration -- that is, listeners are perceiving sounds not physically present in the signal.  
\citet{Samuel1981} identified several factors that increase the likelihood of the phoneme restoration effect.
In the lexical domain, words are more likely than nonwords to have phoneme restorations.  
More frequent words are also more likely to exhibit phoneme restoration effects, and longer words also show greater phoneme restoration effects.  
Position of the sound in the word also influences listeners' decisions, with non-initial positions showing greater effects. 
The other influences on phoneme restoration discussed in \citet{Samuel1981}, namely the signal properties and sentential context are discussed in subsequent sections.

Sounds at the beginnings of words show more fragile lexical bias effects than those later in the word.  
Sounds at the beginning of words require more accurate perception than sounds later in words, because they are used to narrow the set of potential words and no particular expectations are available before the word begins.  
As such, perception-oriented attentional sets are favored in the processing of word-initial sounds.  
Lexically-guided perceptual learning typically maximizes the lexical bias of the modified category by placing it at the end of the word \citep{Norris2003}, but perceptual learning has been found when the ambiguous stimuli is embedded earlier in the word (e.g., the onset of the final syllable \citep{Kraljic2005, Kraljic2008, Kraljic2008a} or the onset of the first syllable \citep{Clare2014}).  
However, in light of the findings in \citet{Pitt2012}, we expect less lexical bias when the ambiguous sounds occur earlier in the word.
Word-initial modified sound categories are predicted to have lower word endorsement rates and smaller perceptual learning effect sizes.  
Experiments 1 and 2  in Chapter~\ref{chap:lexdec} implement this manipulation.

\subsection{Semantic predictability}
\label{sec:semanticpredictability}

The second type of linguistic expectation manipulation used in this dissertation is semantic predictability \citep{Kalikow1977}.
Sentences are semantically predictable when they contain words prior to the final word that point almost definitively to the identity of that final word.  
For instance, the sentence fragment \emph{The cow gave birth to the...} from \citet{Kalikow1977} is almost guaranteed to be completed with the word \emph{calf}.  
On the other hand, a fragment like \emph{She is glad Jane called about the...} is far from having a guaranteed completion beyond being a noun.

Words that are predictable from context are temporally and spectrally reduced compared to words that less predictable \cite{Scarborough2010, Clopper2008}. Despite this acoustic reduction, high predictability sentences are generally more intelligible.
Sentences that form a semantically coherent whole have higher word identification rates across varying signal-to-noise ratios \citep{Kalikow1977} in both children and adults \citep{Fallon2002}, and across native monolingual and early bilingual listeners, but not late bilingual listeners \citep{Mayo1997}.
Highly predictable sentences are more intelligible to native listeners in noise, even when signal enhancements are not made, though non-native listeners require both signal enhancements and high predictability together to see any benefit \cite{Bradlow2007}.
However, when words at the ends of predictive sentences are excised from their context, they tend to be less intelligible than words excised from non-predictive contexts \citep{Lieberman1963}.

Semantic predictability has been found to have similar effects to lexical bias on phoneme categorization \citep{Connine1987, Borsky1998}.  
In those studies, a continuum from one word to another, such as \emph{coat} to \emph{goat}, was embedded in a sentence frame that semantically cohered with one of the endpoints.  
Participants showed a shift in the category boundary based on the sentence frame.
If the sentence frame cued the voiced stop, more of the continuum was subsequently categorized as the voiced stop and vice versa for the voiceless stop.

In the phoneme restoration paradigm, higher semantic predictability has been found to bias listeners toward perceptually restoring a sound \citep{Samuel1981}.
This increased bias towards interpreting the stimuli as an intact word was also coupled with an increase in sensitivity between the two types of stimuli (i.e. noise added to speech, speech replaced with noise), which \citet{Samuel1981} suggests is the result of a lower cognitive load in predictable contexts. Later work has suggested that in cases of lower cognitive load, greater phonetic encoding is available \citep[see also][]{Mattys2011}.

To summarize, the literature on semantic predictability has shown largely similar effects as lexical bias in terms of how sounds are categorized and restored.  
From this, I hypothesize that increasing the expectations for a word through semantic predictability will promote a comprehension-oriented attentional set, as perception of the modified sound category will not be strictly necessary for comprehension.
Listeners that are exposed to an /s/ category that is more /\textesh/-like only in words that are highly predictable from context are therefore predicted to show larger perceptual learning effects than listeners exposed to the same category only in words that are unpredictable from context.
However, there may be an upper limit for listener expectations when both semantic predictability and lexical bias are high, as committing too much to a particular expectation could lead to garden path phenomena \citep{Levy2008} or other misunderstandings.
The effect of semantic predictability on perceptual learning is explicitly tested in Chapter~\ref{chap:sent}.

\section{Attention and perceptual learning}
\label{sec:attention}

Attention is a large topic of research in its own right, and this section only reviews literature that is directly relevant to perceptual learning and this thesis.
Attention has been found to have a role on perceptual learning in the psychophysics literature, indeed \citet{Gibson1953} identifies it as the sole prerequisite to perceptual learning.
As an example, \citet{Ahissar1993} found that, in general, attending to \emph{global} features for detection (i.e., discriminating different orientations of arrays of lines) does not make participants better at using \emph{local} features for detection (i.e., detection of a singleton that differs in angle in the same arrays of lines), and vice versa.  Perceptual learning in this visual domain is limited to the task and stimuli on which a given participant was trained.  

Attentional sets can refer to the strategies that the perceiver uses to perform a task.
The attentional sets widely used in the visual perception literature do not align completely with the notion of perception-oriented and comprehension-oriented attentional sets used here.
For instance, in a visual search task, colour, orientation, motion, and size are the predominant strategies \citep{Wolfe2004}.
However, some parallels are present.
The two broad categories in the visual perception literatures are \emph{focused} and \emph{diffuse} attentional sets.
Focused sets direct attention to components of the sensory input, perceiving the trees instead of the forest. 
Diffuse sets direct attention to global properties of the sensory input, perceiving the forest instead of the trees.  
In the visual search literature, an example of a focused attentional set is the ``feature search mode", which gives priority to a single feature, such as the colour of the target. An example of a diffuse attentional set is the ``singleton detection mode", which gives priority to any salient features \citep{Bacon1994}.
Perception-oriented and comprehension-oriented attentional sets have also been referred to as focused and diffuse attentional sets in recent speech perception work.
In \citet{Pitt2012}, a diffuse attentional set is where primary attention is on detecting words from nonwords in a lexical decision task, and a focused attentional set is where instructions direct participants' attention to a potentially misleading sound.
Attentional set selection is not necessarily optimal on the part of the perceiver, and it has been shown to be biased based on experience, with the amount of training performed influencing the length of time that perceivers will continue to use non-optimal sets after the task has changed \citep{Leber2006}.

Listeners can employ different attentional sets depending on the nature of the task, as well as other processing considerations.
For instance, listeners can attend to particular syllables or sounds in syllable- or phoneme-monitoring tasks \citep[and others]{Norris1988}, and even particular linguistically relevant temporal positions \citep{Pitt1990}.
However, even in these low-level, signal based tasks, lexical properties of the signal can exhibit some influence if the stimuli are not monotonous enough to disengage comprehension \citep{Cutler1987}.  
Additionally, when performing a phoneme categorization task under higher cognitive load, such as performing a more difficult concurrent task, listeners show increased lexical bias effects \citep{Mattys2011}.
Variation in speech in general seems to lead towards a more diffuse, comprehension-oriented attentional set, where the goal is firmly more comprehension-based than low-level perception-based.
In comprehension-oriented tasks, such as a lexical decision tasks, explicit instructions can promote a more perception-oriented attentional set.
When listeners are told that the speaker's /s/ is ambiguous and to listen carefully to ensure correct responses, they are less tolerant of noncanonical productions across all positions in the word \citep{Pitt2012}.  
That is, listeners whose attention is directed to the speaker's sibilants are less likely to accept the modified production as a word than listeners given no particular instructions about the sibilants.
While the primary task has a large influence on the type of attentional set adopted, other instructions and aspects about the stimuli can shift the listener's attentional set toward another one.

Attentional sets have not been manipulated in previous work on perceptual learning in speech perception, but some work has been done on how individual differences in attention control can impact perceptual learning.
\citet{Scharenborg2014} presents a perceptual learning study of older Dutch listeners in the model of \citet{Norris2003}.  
In addition to the exposure and test phases, these older listeners completed tests for hearing loss, selective attention, and attention-switching control.  
They found no evidence that perceptual learning was influenced by listeners' hearing loss or selective attention abilities, but they did find a significant relationship between a listener's attention-switching control and their perceptual learning.  
Listeners with worse attention-switching control showed greater perceptual learning effects, which the authors ascribed to an increased reliance on lexical information.  
Older listeners were shown to have smaller perceptual learning effects compared to younger listeners, but the differences were most prominent directly following exposure \citep{Scharenborg2013}.  
Younger listeners initially had a larger perceptual learning effect in the first block of testing, but the effect lessened over the subsequent blocks.  
Older listeners showed smaller initial perceptual learning effects, but no such decay.  
\citet{Scharenborg2013} also found that participants who endorsed more of the target items as words in the exposure phase showed significantly larger perceptual learning effects in the testing phase.

There is evidence that attentional sets in the visual domain become entrenched over time \cite{Leber2006}, but the fact that attention-switching control in older adults was a significant predictor of the size of perceptual learning effects \citep{Scharenborg2014} suggests that listeners are indeed switching between sets through an experiment.
The lexical decision task is oriented toward comprehension, so the primary attentional set is likely to be a diffuse one relying more on lexical information than acoustic.
Participants with worse attention-switching control would have adopted this attentional set for more of the exposure than those with better attention-switching control, and it is precisely those with worse attention-switching control that showed the larger perceptual learning effects.
The ability to switch attention to a more perception-oriented set could have allowed those listeners to prevent over-generalization, leading to a smaller perceptual learning effect for participants with better attention-switching control.

As stated above, Bayesian models account well for perceptual learning experiments.
Attentional sets are crucial to the hypothesis tested in this dissertation, but they do not play a role in the conceptual and computational models of perceptual learning.
The predictive coding framework \citep{Clark2013} provides a gain-based attentional mechanism. 
In this model, attention causes greater weight to be attached to error signals from mismatched expectations and sensory input, increasing their weight and their effect on future expectations.  
However, as noted by \citet{Block2013}, this view of attention does not capture the full range of experimental results.  
For instance, in a texture segregation task, spatial attention to the periphery improves performance where spatial resolution is poor, but attention to central locations, where spatial resolution is high, actually harms performance \citep{Yeshurun1998}.  
This detrimental effect is an instance of missing the forest for the trees, as spatial resolution increased too much in the central locations to perceive the larger texture.  
The attentional mechanism proposed in this dissertation limits error propagation beyond where attention is focused.
Attending to perception rather than comprehension should only update expectations about perception of that individual instance.
The lower sensory levels is where stimuli are represented with the greatest degree of detail \citep{Gilbert2001}.
Perceptual learning at these lower levels should be more exposure specific and less generalized than any learning that propagates to higher representational levels.
In contrast, according to the mechanism proposed in \citet{Clark2013}, any increases in attention, perception-oriented or otherwise, is predicted to lead to greater perceptual learning.

\section{Category typicality and perceptual learning}
\label{sec:signal}

A primary finding across the perceptual learning literature is that learning effects are only found on testing items that are similar in some sense to the exposure items.  In the most extreme instance, perceptual learning is only found on the exact same items as exposure \citep{Reinisch2014}, but most commonly, perceptual learning is limited to items produced by the same speaker as exposure \citep{Norris2003,Reinisch2013}.  However, a less studied question is what properties of the exposure items cause different degrees of perceptual learning.  

Variability is a fundamental property of the speech signal, so sound categories must have some variance associated with them and certain contexts can have increased degrees of variability.
For example, \citet{Kraljic2008a} exposed participants to ambiguous sibilants between /s/ and /\textesh/ in two different contexts.  
In one, the ambiguous sibilants were intervocalic, and in the other, they occurred as part of a /str/ cluster in English words.  
Participants exposed to the ambiguous sound intervocalically showed a perceptual learning effect, while those exposed to the sibilants in /str/ environments did not.  
The sibilant in /str/ often surfaces closer to [\textesh] in many varieties of English, due to coarticulatory effects from the other consonants in the cluster.  
They argue that the interpretation of the ambiguous sound is done in context of the surrounding sounds, and only when the pronunciation variant is unexplainable from context is the variant learned and attributed to the speaker \citep[see also][]{Kraljic2008}.
In other words, a more /\textesh/-like /s/ category is typical in the context of the /str/ clusters, but is atypical in intervocalic position.
Interestingly, given the lack of learning present in in the /str/ context, some degree of salience seems to be required to trigger perceptual learning.

\citet{Sumner2011} investigated category typicality through a manipulation of presentation order. 
Listeners were exposed to French-accented English with modifications to the /b/-/p/ category boundary.
Participants were exposed to stimuli ranging from English-like to French-like voice onset time for /b/ and /p/.
In one presentation order, the order of stimuli was random, but in the others the voice onset time changed in a consistent manner, for instance, starting as more French-like and becoming more English-like.
The presentation order that showd the greatest perceptual learning effects was the one that began more English-like and ended more French-like.
The condition that mirrored the more normal course of non-native speaker pronunciation changes, starting as more French-like and ending as more English-like, did not produce significantly different behaviour than control participants who only completed the categorization task.
The random presentation order had perceptual learning effects in between the two ordered condtions.
These results suggest that listeners constantly update their category following each successive input, rather than only relying on initial impressions \citep[contra][]{Kraljic2008}.
This finding is mirrored in \citet{Vroomen2007}, where participants initially expand their category in response to a single, repeated modified input, but then entrench that category as subsequent input is the same.
The data in \citet{Vroomen2007} is modeled using a Bayesian framework with constant updating of beliefs.
However, in \citet{Sumner2011}, the constantly shifting condition also had more perceptual learning than a random order of the same stimuli, suggesting that small differences in expectations and observed input induce greater updating than large differences.
The bias towards small differences is better captured by the exemplar model proposed by \citet{Pierrehumbert2001}, where only input similar to the learned distribution is used for updating that distribution, and input too ambiguous given learned distributions is discarded.

Similarity of input to known distributions has effects in many psycholinguistic paradigms.
For instance in phoneme restoration, \citet{Samuel1981} found that the likelihood of restoring a sound increases when said sound is acoustically similar to the noise replacing it.  
When the replacement noise is white noise, fricatives and stops are more likely to be restored than vowels and liquids.
Simply, acoustic signals that better match expectations are less likely to be noticed as atypical.

In this dissertation, the degree of typicality of the modified category is manipulated across Experiments 1 and 2.  
In one case, the /s/ category for the speaker is maximally ambiguous between /s/ and /\textesh/, but in the other, the category is more like /\textesh/ than /s/.
The maximally ambiguous category is hypothesized to be less salient than the more /\textesh/-like /s/ category.
This lessened salience will result in greater use of comprehension-oriented attentional sets.
I hypothesize that the more /\textesh/-like category will shift listeners' attentional sets to be more perception-oriented due to their greater atypicality, which will lead to less generalized perceptual learning.

\section{Current contribution}

Perceptual learning in speech perception generalizes to new forms and contexts far more than would be expected from a purely psychophysical perspective \citep{Norris2003,Gilbert2001}.
The two paradigms promote different attentional sets, with speech perception paradigms providing a focus on comprehension and psychophysics tasks giving focus to perception.  
Indeed, visually-guided perceptual learning in speech perception, with its emphasis on perception, shows largely similar exposure-specificity effects as the psychophysics findings \citep{Reinisch2014}.
This dissertation expands the existing literature by modifying the exposure tasks to promote comprehension- or perception-oriented attentional sets.
Perceptual learning effects are hypothesized to be smaller in the conditions that promote perception-oriented attentional sets, as perception exposure tasks have shown greater exposure-specificity effects than comprehension exposure tasks.

