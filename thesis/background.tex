%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Introduction}

Listeners of a language are faced with a large degree of phonetic variability when interacting with their fellow language users.  
Speakers can have different sizes, different genders, and different backgrounds that make speech sound categories, at first blush, overlapping in distribution and hard to separate in acoustic dimensions.
In addition to properties of the speaker varying, a listener's attention or goals in an interaction can vary, such as paying attention to a non-native speaker or being distracted by planning upcoming utterances or by another task entirely.  
Despite variability on the part of both the listener and the speaker, listeners can interpret disparate and variable productions as belonging to a single word type or sound category, a phonemenon referred to as perceptual constancy in categorization studies \citep{Shankweiler1977, Kuhl1979} and as recognition equivalence in word recognition tasks \citep{Sumner2013}.
One of the processes for achieving this constancy is perceptual learning, whereby perceivers update context-dependent categories.
In the speech perception literature, perceptual learning or perceptual adaptation refers to the updating of distributions corresponding to a sound category, such as /s/ or /\textesh/, for a particular speaker after exposure to speech with a modification to the sound category's distribution \citep{Norris2003}.
Exposure to a speaker affects a listener's perceptual system for that speaker,  even after only a few tokens \citep{Vroomen2007, Kraljic2008} or within the span of a single utterance \citep{Ladefoged1957}.
Perceptual learning is not limited to single speakers, and exposure to multiple speakers sharing a non-native accent increases the intelligibility of new speakers with that accent \citep{Bradlow2008}.

%
%Perceptual learning has been found only when it is either lexically-guided \citep{Norris2003} or visually-guided \citep{Bertelson2003}.
%If the speaker characteristic to be learned is embedded in audio-only non-words, no perceptual learning occurs \citep{Norris2003}, suggesting that the lexical connections are crucial for updating distributions in perceptual learning.
%If a listener is to learn that a speaker produces their /s/ sounds with a more /\textesh/-like quality, some words may facilitate this learning more than others.  
%The position in the word that a critical sound occupies has an effect on a myriad of tasks, such as phoneme restoration, phoneme categorization \citep{Pitt2006}, and even lexical decision \citep{Pitt2012}.
%Increasing the bias towards a word 

However, perceptual learning, particularly in speech perception, relies on the linguistic system, which interacts with listener attention and signal properties.
This dissertation investigates the interaction between linguistic, attentional and acoustic factors on perceptual learning of the /s/ category of a speaker is modified to be more /\textesh/-like.
The key hypothesis being tested is that the more evidence a listener has toward a specific word, be it acoustic, lexical or semantic/sentential, the stronger the link between an ambiguous sound embedded in that word and a sound category will be, which will lead to greater observed perceptual learning effects.
Two linguistic factors will be manipulated that have been found to affect the categorization and discrimination of speech sounds embedded in words, namely lexical bias and semantic predictability.
According the hypothesis above, increasing linguistic expectations for the words that contain the target sounds will result in greater perceptual learning.
Attention will be manipulated through task demands focusing on word recognition and additional instructions to some listeners to pay attention to the specific sound category to be learned.
I predict that focusing attention on the sound category will result in less perceptual learning, as the same attention manipulation has led to lower word endorsement rates in previous studies \citep{Pitt2012}.
Finally, the acoustic tokens that listeners are exposed to are manipulated to expose listeners to /s/ categories that are closer or farther from a typical /s/ category.
I predict that exposure only to tokens farther from the typical category will result in less perceptual learning than exposure to tokens closer to the typical category.
Attention is predicted to interact with the other factors, because the attention manipulation draws the listener's attention to the /s/ sounds, which may occur naturally in some conditions, but not others.
Lower lexical bias positions, such as the beginnings of words, are salient positions in linguistic theory. 
Productions farther from the typical production are more likely to be noticed, such as in phoneme restoration tasks where restorations are more likely when the replacing sound is similar to the restored sound \citep{Samuel1981}.
In both of these cases, increasing external attention on the /s/ category will likely not produce an effect beyond what attention the signal itself draws.

The structure of the thesis is as follows.
This chapter will review the recent literature on perceptual learning in speech perception (Section~\ref{sec:perceptuallearning}), as well as literature on the linguistic (Section~\ref{sec:linguistic}), attentional (Section~\ref{sec:attention}), and signal (Section~\ref{sec:signal}) factors manipulated in the three experiments of this dissertation.
Chapter~\ref{chap:lexdec} will detail two experiments using a lexically-guided perceptual learning paradigm, each with different conditions for levels of lexical bias and attention.  
The two experiments differ in the acoustic properties of the exposure tokens, with the first experiment using an /s/ category that is halfway between /s/ and /\textesh/, and the second experiment using an /s/ category that is more /\textesh/-like than /s/-like.  
Chapter~\ref{chap:sent} details an experiment using a novel perceptual learning paradigm that manipulates an additional linguistic factor, namely semantic predictability, to increase the linguistic expectations during exposure.
The perceptual learning literature has generally used consistent processing conditions to elicit perceptual learning effects, and a goal of this dissertation is to examine the robustness and degree of perceptual learning across different processing conditions.

\section{Perceptual learning}
\label{sec:perceptuallearning}

Perceptual learning is a well established phenomenon in the psychology and psychophysics literature. 
Training can improve a participants ability to discriminate in many disparate modalities, such as visual acuity, somatosensory spatial resolution, weight estimation, and discrimination of hue and acoustic pitch \citep[for review]{Gibson1953}. 
In this literature, perceptual learning is the improvement of a perceiver to judge the physical characteristics of objects in the world through training that assumes attention on the task, but doesn't require reinforcement, correction or reward.
This definition of perceptual learning corresponds more to what is termed ``selective adaptation'' in the speech perception literature rather than what is termed ``perceptual learning'', ``perceptual adaptation'' or ``perceptual recalibration.''  
In speech perception, selective adaptation is the phenomenon where listeners that are exposed repeatedly to a narrow distribution for a sound category, narrow their own perceptual category, resulting in a change in variance of the category, not of the mean of the category (along some acoustic-phonetic dimension) \citep{Eimas1973,Samuel1986,Vroomen2007}.
Perceptual learning or recalibration in the speech perception literature is a more broad updating of perceptual categories, either in mean or variance \citep{Norris2003, Vroomen2007}.

\citet{Norris2003} began the recent set of investigations into lexically-guided perceptual learning in speech.
\citet{Norris2003} exposed one group of Dutch listeners to a fricative halfway between /s/ and /f/ at the ends of words like \emph{olif} "olive" and \emph{radijs} "radish", while exposing another group to the ambiguous fricative at the ends of nonwords, like \emph{blif} and \emph{blis}.
Following exposure, both groups of listeners were tested on their categorization on a fricative continuum from 100\% /s/ to 100\% /f/. 
Listeners exposed to the ambiguous fricative at the end of words shifted their categorization behaviour, while those exposed to them at the end of nonwords did not.  The exposure using words was further differentiated by the bias introduced by the words.  
Half the tokens ending in the ambiguous fricative formed a word if the fricative was interpreted as /s/ but not if it was interpreted as /f/, and the others were the reverse.  
Listeners exposed only to the /s/-biased tokens categorized more of the /f/-/s/ continuum as /s/, and listeners exposed to /f/-biased tokens categorized more of the continuum as /f/.  
The ambiguous fricative was associated with either /s/ or /f/ dependent on the bias of the word, which led to an expanded category for that fricative at the expense of the other category.
These results crucially show that perceptual categories in speech are malleable to new exposure, and that the linguistic system of the listener facilitates generalization to that category in new forms and contexts.

In addition to lexically-guided perceptual learning, unambiguous visual cues to sound identity can cause perceptual learning as well; this is referred to as perceptual recalibration in that literature.
In \citet{Bertelson2003}, an auditory continuum from /aba/ to /ada/ was synthesized and paired with a video of a speaker producing /aba/ and a video of /ada/.  
Participants first completed a pretest that identified the maximally ambiguous step of the /aba/-/ada/ auditory continuum. 
 In eight blocks, participants were randomly exposed to the ambiguous auditory token paired with video for /aba/ or with the video for /ada/.  Following each block, they completed a short categorization test.  
Participants showed perceptual learning effects, such that they were more likely to respond with /aba/ if they had been exposed to video of /aba/ paired with the ambiguous token in the preceding block, and likewise for /ada/.

\citet{vanLinden2007} compared the perceptual recalibration effects from the visual lipread paradigm \citep{Bertelson2003} to the standard lexically-guided perceptual learning paradigm \citep{Norris2003}.  
Lipread recalibration and perceptual learning effects had comparable size, lasted equally as long, were enhanced when presented with a contrasting sound, and were both unaffected by periods of silence between exposure and categorization.  
The effects did not last through prolonged testing in this study, unlike in other studies \citep{Kraljic2005,Eisner2006}.
In those studies, lexically-guided perceptual learning effects persisted through intermediate tasks, as long as the task did not involve any contradictory evidence of the trait learned \citep{Kraljic2005}, and also persisted across 12 hours \citep{Eisner2006}.

Perceptual learning in speech perception can be captured well in terms of Bayesian belief updating \citep{Kleinschmidt2011} or as part of a predictive coding model of the brain \citep{Clark2013}.  
In Bayesian belief updating, the model categorizes the incoming stimuli based on multimodel cues, and then updates the distribution to reflect that categorization.  
This updated conditional distribution is then used for future categorizations in an iterative process.  
\citet{Kleinschmidt2011} model the results of the behavioural study in \citet{Vroomen2007} in a Bayesian framework, with models fit to each participant capturing the perceptual recalibration and selective adaptation shown by the participants over the course of the experiment.  
A similar, but more broad, framework is that of the predictive brain \citep{Clark2013}. 
This framework uses a hierarchical generative model that aims to minimize prediction error between bottom-up sensory inputs and top-down expectations.  
Mismatches between the top-down expectations and the bottom-up signals generate error signals that are used to modify future expectations.  
Perceptual learning then is the result of modifying expectations to match learned input.

Perceptual learning in the psychophysics literature has shown a large degree of exposure-specificity, where observers only show learning effects on the same or very similar stimuli as those they were trained on. As such, perceptual learning has been argued to reside or affect the early sensory pathways, where stimuli are represented with the greatest detail \citep{Gilbert2001}.  Perceptual learning in speech perception has also shown a large degree of exposure-specificity, where participants do not generalize cues across speech sounds \citep{Reinisch2014} or across speakers unless the sounds are similar across exposure and testing \citep{Eisner2005, Kraljic2005, Kraljic2007, Reinisch2013a}.  
On the other hand, lexically-guided perceptual learning in speech has shown a greater degree of generalization than would be expected from a purely psychophysical standpoint.  
The testing stimuli are generally quite different from the exposure stimuli, with participants exposed to multisyllabic words ending in an ambiguous sound and tested on monosyllabic words \citep{Reinisch2013} and nonwords \citep{Norris2003, Kraljic2005}, though exposure-specificity is found when exposure and testing use different positional allophones \citep{Mitterer2013}.

Why is lexically-guided perceptual learning more context-general?
The experiments performed in this dissertation will provide evidence that this context-generality can be manipulated through attentional sets of listeners.
A more comprehension-oriented attentional set, where a listener's goal is understand the meaning of speech, promotes generalization and leads to greater perceptual learning.  
A more perception-oriented attentional set, where a listener's goal is to perceive specific qualities of a signal, does not promote generalization.
These attentional sets will be explored in more detail in Section~\ref{sec:attention} following an examination of the linguistic factors that will be manipulated in the experiments in Chapters~\ref{chap:lexdec} and \ref{chap:sent}.
These linguistic factors serve to aid the listener in adopting comprehension-oriented attentional sets.

\section{Linguistic factors and perceptual learning}
\label{sec:linguistic}

The two linguistic factors manipulated in this dissertation are lexical bias and semantic predictability.  
Lexically-guided perceptual learning paradigms use lexical bias as the means to link an ambiguous sound to an unambiguous category.
In Chapter~\ref{chap:sent}, a novel, sententially-guided perceptual learning paradigm is used to manipulate the listener's linguistic expectations in a different manner than manipulating lexical bias.

\subsection{Lexical bias}
\label{sec:lexicalbias}

Lexical bias is the primary way through which perceptual learning is induced in the experimental speech perception literature.
Lexical bias, also known as the Ganong Effect, refers to the tendency for listeners to interpret a speaker's (noncanonical) production as a particular, meaningful word rather than a nonsense word.  
For instance, given a continuum from a nonword like \emph{dask} to word like \emph{task} that differs only in the initial sound, listeners in general are more likely to interpret any step along the continuum as the word endpoint rather than the nonword endpoint \citep{Ganong1980}. 
This bias is exploited in perceptual learning studies to allow for noncanonical, ambiguous productions of a sound to be linked to pre-existing sound categories.
Given that ambiguous productions must be associated with a word to induce lexically-guided perceptual learning \citep{Norris2003}, differing degrees of lexical bias could lead to differing degrees of perceptual learning, a prediction which will be tested in Chapter~\ref{chap:lexdec}.  
The stronger the lexical bias, the stronger the link will be between the ambiguous sound and the sound category, as mediated by the word.

Studies looking at lexical bias generally use a phoneme categorization task, where participants identify a sound at the beginning or end of a word from two possible options that vary in one dimension.  
For instance, given a continuum from /t/ to /d/, participants are asked to identify the sound at the beginning or end as either /t/ or /d/. 
Lexical bias effects are calculated based on two continua, where words are formed at opposite ends. 
For the /t/ to /d/ continua, one would form a word at the /t/ end, such as \emph{task} and one would form a word at the other end, such as \emph{dash}.  
Lexical bias effects are then calculated from the different categorization behaviour for these two continua.

%Word length
Lexical bias has been found to vary in strength according to several factors.  
First, the length of the word in syllables has a large effect on lexical bias, with longer words showing stronger lexical bias than shorter words \citep{Pitt2006}.  
Continua formed using trisyllabic words, such as \emph{establish} and \emph{malpractice}, were found to show consistently large lexical bias effects than monosyllabic words, such as \emph{kiss} and \emph{fish}.  
\citet{Pitt2006} also found that lexical bias from trisyllabic words was robust across experimental conditions, but lexical bias from monosyllabic words was more fragile and condition dependent.  
They argue that these affects arise from both the greater bottom-up information present in longer words and the greater lexical competition for shorter words.

%Position in the word
\citet{Pitt2012} used a lexical decision task with a continuum of fricatives from /s/ to /\textesh/ embedded in words differing in the position of a sibilant.  
They found that ambiguous fricatives earlier in the word, such as \emph{serenade} or \emph{chandelier}, lead to greater nonword responses than the same ambiguous fricatives embedded later in a word, such as \emph{establish} or \emph{embarass}.  
In the experiments and meta-analysis of phoneme identification results presented in \citet{Pitt1993}, they found that, for monosyllabic word frames, token-final targets produce more robust lexical bias effects than token-initial targets.
Between word length and position in the word, lexical bias appears to be strengthened over the course of the word.
As a listener hears more evidence for a particular word, their expectations for hearing the rest of that word increase.

%Samuel1981
Lexical bias has been found to affect phoneme restoration tasks as well \citep{Samuel1981}.  
In this paradigm, listeners heard words with noise added to or replacing sounds and were asked to identify which they had heard for each trial.  
Lower sensitivity to noise addition versus replacement and increased bias for responding that noise was added is indicative of phoneme restoration, where listeners are perceiving sounds not actually present in the signal.  
Several factors are identified in the study as increasing the likelihood of the phoneme restoration effect.
In the lexical domain, words are more likely than nonwords to have phoneme restorations, and this discrepency strengthens when listeners are primed with a form without noise before the trial.  
More frequent words were also more likely to exhibit phoneme restoration effects, and longer words also showed greater phoneme restoration effects.  
Position of the sound in the word also influenced the decision, with non-initial positions showing greater phoneme restoration effects. 
The other influences on phoneme restoration discussed in this paper, namely the signal properties and sentential context will be discussed in subsequent sections.

In the stimuli used in \citet{Norris2003} and most other lexically-guided perceptual learning experiments, lexical bias tends to be maximized by using multisyllabic words with the ambiguous sound at the end for exposure stimuli.  
Perceptual learning has also been found when the ambiguous stimuli is embedded earlier in the word, such as the onset of the final syllable \citep{Kraljic2005, Kraljic2008, Kraljic2008a} or the even the onset of the first syllable \citep{Clare2014}.  
In light of the findings in \citet{Pitt2012}, we would expect less lexical biases the earlier in the word those ambiguous sounds were heard, and therefore, I hypothesize, lower endorsement rates and smaller perceptual learning effect sizes.  
This prediction will be explicitly tested in Experiment 1 in Chapter~\ref{chap:lexdec}.

\subsection{Semantic predictability}
\label{sec:semanticpredictability}

The second type of linguistic expectation manipulation used in this dissertation is known as semantic predictability \citep{Kalikow1977}.
Sentences are semantically predictable when they contain words prior to the final word that points almost definitively to the identity of that final word.  
For instance, the sentence fragment \emph{The cow gave birth to the...} from \citet{Kalikow1977} is almost guaranteed to be completed with the word \emph{calf}.  
On the other hand, a fragment like \emph{She is glad Jane called about the...} is far from having a guaranteed completion, other than having the category of noun.

Despite the temporal and spectral reduction found in high predictability contexts \cite{Scarborough2010, Clopper2008}, high predictability sentences are generally more intelligible.
Sentences that form a semantically coherent whole have higher word identification rates across varying signal-to-noise ratios \citep{Kalikow1977}, which has been found across children and adults \citep{Fallon2002}, and across native monolingual and early bilingual listeners, but not late bilingual listeners \citep{Mayo1997}.
However, when words at the ends of predictive sentences are excised from their context, they tend to be less intelligible than words excised from non-predictive contexts \citep{Lieberman1963}.
Highly predictable sentences are more intelligible to native listeners in noise, even when signal enhancements are not made, though non-native listeners require both signal enhancements and high predictability together to see any benefit \cite{Bradlow2007}.

Two studies looking at similarities between lexical bias and semantic predictability found similar effects on phoneme categorization, though differing effects in reaction time \citep{Connine1987,Borsky1998}.
\citet{Connine1987} found evidence that semantic predictability operated similar to the ``postperceptual'' processes in \citet{Connine1987a}.
The perceptual process was lexical bias towards one endpoint of a continuum, and the postperceptual process was a monetary benefit for responding with one of the end points of a continuum \citep{Connine1987a}.
The difference between ``perceptual'' and ``postperceptual'' processes in those studies was primarily one of reaction time; both kinds of bias produced comparable shifts in categorization.
\citet{Borsky1998} attempted to replicate \citet{Connine1987} while removing potential confounds from the methodological design.  
In contrast to \citet{Connine1987}, they used only one voicing continuum (from \emph{goat} to \emph{coat}), embedded the acoustic target in the middle of the sentence rather than at the end, and only presented one instance of each sentence to each participant rather than all continuum steps for each sentence to each participant.  
The reaction time profile in their study more closely aligned to the profile of lexical bias in \citet{Connine1987a}, but again the shift in how the continuum was categorized aligned with the sentential context.

In phoneme restoration, higher semantic predictability has been found to bias listeners toward interpreting the stimuli as intact with noise rather than replaced with noise \citep{Samuel1981}.
This increased bias towards interpreting the stimuli as an intact word was also coupled with an increase in sensitivity between the two types of stimuli, which \citet{Samuel1981} suggests is the result of a lower cognitive load in predictable contexts, and therefore greater phonetic encoding is available \citep[see also][]{Mattys2011}.

The previous literature on semantic predictability has shown largely similar effects as lexical bias in how sounds are categorized and restored.  
From this, I hypothesize that increasing the expectations for a word through semantic predictability will increase the link between sound categories and words in a similar fashion as increasing lexical bias.
Listeners that are exposed to an /s/ category that is more /\textesh/-like only in words that are highly predictable from context should show larger perceptual learning effects than listeners exposed to the same category only in words that are unpredictable from context.
However, there may be an upper limit for listener expectations when both semantic predictability and lexical bias are high, as committing too much to a particular expectation could lead to garden path phenomena \citep{Levy2008}.
The effect of semantic predictability on perceptual learning will be explicitly tested in Chapter~\ref{chap:sent}.

\section{Attentional factors and perceptual learning}
\label{sec:attention}

Attention is a large topic of research in its own right, and this section only reviews literature that is directly relevant to perceptual learning.
Attention has been found to have a role on perceptual learning in the psychophysics literature.  
For instance, \citet{Ahissar1993} found that in general, attending to global features for detection (i.e., discriminating different orientations of arrays of lines) does not make participants better at using local features for detection (i.e., detection of a singleton that differs in angle in the same arrays of lines), and vice versa.  However, there was a small degree of local detection learned from global detection, with the singleton popping out from its field as highly salient.

Attentional sets are a widely used term in the attention literature.  
In the visual domain, attentional sets can refer to the strategies that the perceiver uses to perform a task.  
For instance, in a visual search task, colour, orientation, motion and size are the predominant strategies \citep{Wolfe2004}.  
Two broad categories of attentional sets are generally used.  
Focused sets direct attention to components of the sensory input, and diffuse sets direct attention to global properties of the sensory input.  
In the visual search literature, a focused attentional set is the feature search mode, which gives priority to a single feature, such as the colour of the target, and a diffuse attentional set is singleton detection mode, which gives priority to any salient features \citep{Bacon1994}. 
In the auditory streaming literature, two attentional sets have been identified as ``selective listening'', where the perceiver attempts to hear the components of two streams, and ``comprehensive listening'', where the perceiver tries to hear all components as a single stream  \citep{vanNoorden1975}.
Finally, in a lexical decision tasks, the diffuse attentional set is where primary attention is on detecting words from nonwords, and the focused attentional set is where instructions direct participants attention to a potentially misleading sound \citep{Pitt2012}.
Attentional set selection is not necessarily optimal on the part of the perceiver, and it has been shown to be biased based on experience, with the amount of training performed influencing the length of time that perceivers will continue to use non-optimal sets after the task has changed \citep{Leber2006}.  

Listeners can selectively attend to many aspects of a signal, broadly falling into the categories of perception and comprehension.
As specified in \citet{Cutler1987}, perception refers to low-level, signal based attentional sets, while comprehension refers to lexical and other linguistically-based attentional sets.
For instance, listeners can attend to particular syllables or sounds in syllable or phoneme monitoring tasks \citep[and others]{Norris1988}, and even particular linguistically relevant temporal positions \citep{Pitt1990}.
However, even in these low-level, signal based tasks, lexical properties of the signal can exhibit some influence, if the stimuli are not monotonous enough to disengage comprehension \citep{Cutler1987}.  
Variation in speech in general seems to lead towards a more diffuse, comprehension-oriented attentional set, which is likely the default attentional set employed in everyday use of language, where the goal is firmly more comprehension than perception.

Attention has not been manipulated in previous work on perceptual learning in speech perception, but some work has been done on how individual differences in attention control can impact perceptual learning.
\citet{Scharenborg2014} presents a perceptual learning study of older Dutch listeners in the model of \citet{Norris2003}.  
In addition to the exposure and test phases, these older listeners completed tests for hearing loss, selective attention and attention-switching control.  
They found no evidence that perceptual learning was influenced by listeners' hearing loss or selective attention abilities, but they did find a significant relationship between a listener's attention-switching control and their perceptual learning.  
Listeners with worse attention-switching control showed greater perecptual learning effects, which the authors ascribed to an increased reliance on lexical information.  
Older listeners had previously been shown to have smaller perceptual learning effects as compared to younger listeners, but the differences were most prominent directly following exposure \citep{Scharenborg2013}.  
Younger listeners initially had a larger perceptual learning effect in the first block of testing, but the effect lessened over the subsequent blocks.  
Older listeners showed smaller initial perceptual learning effects, but no such decay.  
\citet{Scharenborg2013} also found that performance in the lexical decision task significantly affected the perceptual learning in the testing phase.

%Attention
Lexical bias is affected by attentional processing conditions.
\citet{Pitt2012} additionally investigated the role of attention in modulating lexical bias.  
When listeners were told that the speaker's /s/ and /\textesh/ were ambiguous and to listen carefully to ensure correct responses, they were less tolerant of noncanonical productions across all positions in the word.  
That is, participants attending to the speaker's sibilants were less likely to accept the modified production as a word than participants given no particular instructions about the sibilants.
Given that the task listeners performed was a lexical decision task, the default attentional set for the task, termed ``diffuse'' by \citet{Pitt2012}, would have attention distributed across both acoustic-phonetic and lexical domains.  Listeners given the instructions about the speaker's /s/ productions had a ``focused'' attentional set, with more weighting on the acoustic-phonetic domain than the ``diffuse'' attentional set.
Under higher cognitive load, such as performing a more difficult concurrent task, listeners show an increased lexical bias, as a result of weaker encoding of the auditory details \citep{Mattys2011}.  These results suggest that detailed encoding requires attentional resources.  In \citet{Mattys2011}, the primary task was a phoneme identification task, where a ``focused'' attentional set would likely be the default for participants, but a more ``diffuse'' attentional set, or one that weights lexical information more heavily, seems to be employed in the higher cognitive load conditions.

Attentional sets are thought to be constant across a task, but the fact that attention switching in older adults was a significant predictor of the size of perceptual learning effects \citep{Scharenborg2014} suggests that listeners are indeed switching sets through an experiment.  
The task is oriented toward comprehension, so the primary attentional set is likely to be a diffuse one relying more on lexical information than acoustic.
Participants with worse attention-switching control would have adopted this attentional set for more of the exposure than those with better attention-switching control, and it is precisely those with worse attention-switching control that showed the larger perceptual learning effects.
The ability to switch attention to a more perception-oriented set could have allowed those listeners prevent over-generalization, leading to a smaller perceptual learning effect for participants with better attention-switching control. 

In this dissertation, focusing a listener's attention on the signal through explicit instructions is hypothesized to lead to smaller perceptual learning effects.
In a focused perception-oriented attentional set, comprehension will be de-emphasized and the link between the sound category and the ambiguous token will be less mediated by lexical categories, leading to a more exposure-specific learning that will not generalize to novel tokens as readily.
The work on phoneme restoration suggests that higher predictability sentences are lower cognitive load than unpredictable sentences \citep{Samuel1981}, which may lead to greater encoding for the sounds to be learned, leading to larger perceptual learning effects.
However, if fewer attentional resources are required for comprehending the word, there could be more attention on perception in the predictable sentences, leading to a reduced perceptual effect.

\section{Signal factors and perceptual learning}
\label{sec:signal}

A primary finding across the perceptual learning literature is that learning effects are only found on testing items that are similar to the exposure items.  However, a less studied question is what properties of the exposure items cause different degrees of perceptual learning.  

Variability is a fundamental property of the speech signal, so sound categories must have some variance associated with them, and certain contexts can have increased degrees of variability.
\citet{Kraljic2008a} exposed participants to ambiguous sibilants between /s/ and /\textesh/ in two different contexts.  
In one, the ambiguous sibilants were intervocalic, and in the other, they occurred as part of a /str/ cluster.  
Participants exposed to the ambiguous sound intervocalically showed a perceptual learning effect, while those exposed to the sibilants in /str/ environments did not.  
The sibilant in /str/ often surfaces closer to [\textesh] in many varieties of English, due to coarticulatory effects from the other consonants in the cluster, but the coarticulatory effects for merging /s/ and /\textesh/ are much weaker in intervocalic position.  
They argue that the interpretation of the ambiguous sound is done in context of the surrounding sounds, and only when the pronunciation variant is unexplainable from context is the variant learned and attributed to the speaker\citet[see also][]{Kraljic2008}.  
In addition to a continuum of /asi/ to /a\textesh i/, they also tested a continuum from /astri/ to /a\textesh tri/, and found comparable perceptual learning effects across both continua for those exposed to the intervocalic ambiguous sibilants, but no perceptual learning effects on either continua for the other condition, showing a context insentivity absent in other studies.

%
\citet{Sumner2011} investigated whether differences in presentation order in a perceptual learning experiment led to different learning effects of the /b/-/p/ category boundary for a native French speaker of English.  
The presentation order that showed the greatest perecptual learning effects was the one where tokens started out close to what a listener would expect for the categories (English-like voice onset time for /b/ and /p/) and shifted over the course of the experiment to what the speaker's actual categories were (French-like voice onset time for /b/ and /p/), despite the fact that this presentation order is not anything like what a listener would normally encounter when interacting with a non-native speaker of English.  
The condition that mirrored the more normal course of non-native speaker pronunciation changes, starting as more French-like and ending as more English-like, did not produce significantly different behaviour than control participants.  
One explanation for these results are that a small difference between the listener's expectations and the input provides more robust learning, and the future input uses the updated expectations for future input, allowing for greater shifts over time through smaller shifts per trial, which aligns with some conceptualizations for exemplar model dynamics.
For instance, in the exemplar model proposed by \citet{Pierrehumbert2001}, only input similar to the learned distribution is used for updating that distribution, and input too ambiguous given learned distributions is discarded.

Additionally, in the phoneme restoration literature, sounds that are similar to the replacement sound are more likely to be restored \citep{Samuel1981}.  
When the replacement noise is white noise, fricatives and stops are more likely to be restored than vowels and liquids.
Acoustic signals that better match expectations are thus less likely to be noticed as atypical.

In this dissertation, the degree of typicality of the modified category is manipulated.  
In one case, the /s/ category for the speaker is maximally ambiguous between /s/ and /\textesh/, but in the other, the category is more like /\textesh/ than like /s/.
The maximally ambiguous category is less likely to be noticed as an atypical production than the more /\textesh/-like /s/ category.
I hypothesize that the maximally ambiguous category will provide stronger links between ambiguous acoustic tokens and the /s/ category, due to their higher degree of similarity to the expected, typical /s/ category.
As such, perceptual learning effects are predicted to be greater when the modified /s/ category is more like the expected /s/ category than the expected /\textesh/ category.

\section{Current contribution}

Perceptual learning in speech perception generalizes to new forms and contexts far more than would be expected from a purely psychophysical perspective \citep{Norris2003,Gilbert2001}.
This dissertation examines how different conditions for exposure affect generalization to subsequent categorization, leading to larger or smaller observed perceptual learning effects.
The primary hypothesis being tested is that the greater the link between the sound category and the modified acoustic tokens, mediated by lexical items, the greater the generalization to new tokens will be and the larger the perceptual learning effect will be.
Additionally, comprehension-oriented attentional sets are hypothesized to facilitate the link between the sound category and the modified tokens, while perception-oriented attentional sets are hypothesized to inhibit the link.
The specific predictions being tested are that increased expectation, either through lexical bias or semantic predictability, will increase this link and lead to greater perceptual learning effects.  
Directing attention to the specific sound category will shift the attentional set of participants to a more perception-oriented one, decreasing overall perceptual learning. 

In this dissertation,  I induce manipulations of lexical bias and attention in Experiments 1 and 2 (Chapter~\ref{chap:lexdec}), and manipulations of sentence predictability and attention in Experiment 3 (Chapter~\ref{chap:sent}). 
Lexical bias has been shown to affect phoneme categorization tasks \citep{Ganong1980}, and can be manipulated by position of the ambiguous sound in the word and attention \citep{Pitt2012}.  
Sentence predictability has likewise been found to affect phoneme categorization and phoneme restoration tasks similar to lexical bias \citep{Borsky1998, Samuel1981}, and can be manipulated by the preceding words in the sentence \citep{Kalikow1977}.  
In all experiments, attention will be either comprehension-oriented, with a focus on lexical recognition or identity, or perception-oriented, with a focus on the /s/ category that is atypical.
