%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Introduction}

Listeners of a language are faced with a large degree of phonetic variability when interacting with their fellow language users.  
Speakers can have different sizes, different genders, and different backgrounds that make speech sound categories, at first blush, overlapping in distribution and hard to separate in acoustic dimensions.
In addition to properties of the speaker varying, a listener's attention or goals in an interaction can vary, such as paying attention to a non-native speaker or being distracted by planning upcoming utterances or by another task entirely.  
Despite variability on the part of both the listener and the speaker, listeners can interpret disparate and variable productions as belonging to a single word type or sound category, a phenomenon referred to as perceptual constancy in categorization studies \citep{Shankweiler1977, Kuhl1979} and as recognition equivalence in word recognition tasks \citep{Sumner2013}.
One of the processes for achieving this constancy is perceptual learning, whereby perceivers update their representation of a category based on contextual factors.
In the speech perception literature, perceptual learning refers to the updating of distributions corresponding to a sound category, such as /s/ or /\textesh/, following exposure to speech with a modification to the sound category's distribution \citep{Norris2003}.
Exposure to a speaker affects a listener's perceptual system for that speaker,  even after only a few tokens \citep{Vroomen2007, Kraljic2008} or within the span of a single utterance \citep{Ladefoged1957}.
Perceptual learning is not limited to single speakers, and exposure to multiple speakers sharing a non-native accent increases the intelligibility of new speakers with that accent \citep{Bradlow2008}.

The most investigated aspect of perceptual learning in speech perception is the degree of generalization to new contexts.
In general, exposure to modified sound categories, such as /s/ or /\textesh/, generalizes well to other words and nonwords when the modified exposure tokens are embedded in real words \citep{Norris2003, Reinisch2013}.  
This paradigm is referred to as lexically-guided perceptual learning, where participants perform a lexical decision task.
Another paradigm is visually-guided perceptual learning, which exposes listeners to a modified category through matching it to an unambiguous video signal.  
The perceptual learning exhibited from these experiments is only found to influence the specific nonword that perceivers are exposed to, and not other similar nonwords \citep{Reinisch2014}.  
Such exposure-specificity effects have been widely reported in perceptual learning studies in the psychophysics literature \citep{Gibson1953}.

Why, then, does lexically-guided perceptual learning produce such generalization?
One way to divide the literature is by the attentional set elicited in the exposure phase.
An attentional set is a strategy that is employed by perceivers to prioritize certain aspects of stimuli, such as color or shape in the visual domain, and ignore other aspects.
In the speech perception literature, two broad attentional sets have been posited under varying names \citep{Cutler1987, Pitt2012}.  
The first is a comprehension-oriented, or diffuse, attentional set, which is the most similar to normal language use.  
In this attentional set, listeners are focused on comprehending the intended message of the speech, and is promoted by tasks that focus on word identity and word recognition.
The comprehension-oriented attentional set would be elicited in the lexically-guided perceptual learning paradigms through their use of lexical decision tasks and the embedding of modified sound categories in word tokens.
This attentional set is used for lexically-guided perceptual learning, where modified sound categories are embedded in words to be recognized by the perceiver.
The second attentional set is a perception-oriented, or focused, attentional set, where a listener is focused more on the low-level, signal properties of the speech rather than the message.
The perception-oriented attentional set is promoted by tasks such as phoneme or syllable monitoring or mispronunciation detection.
The tasks used in visually-guided perceptual learning and perceptual learning in psychophysics would elicit this attentional set, due to their focus on perceiving stimuli devoid of linguistic meaning.
The hypothesis of this dissertation is that perceivers who adopt a comprehension-oriented attentional set will show more generalization than those who adopt a perception-oriented attentional set.

To test this hypothesis, a lexically-guided perceptual learning paradigm will expose listeners to an /s/ category modifed to sound more like /\textesh/.  
Groups of participants will differ in whether comprehension-oriented or perception-oriented attentional sets are favored when processing the modified /s/ category.
The favoring of attentional sets will be implemented in four ways across the three experiments presented in this dissertation.  
The first two manipulations will be linguistic in nature, and the other two will be instruction-based and stimuli-based.

All the manipulations other than the instruction manipulation can be viewed in terms of salience.
While salience is a rather amorphous term across literatures, I adopt a definition that an element is salient if it is unpredictable from context or easily distinguishable from other possible elements.  
The sound category that is being learned in this dissertation already has salient signal properties.  
The sibilants /s/ and /\textesh/ are easily distinguishable from other speech sounds in English, so the salience that is manipulated is a more linguistic salience, where increased salience is the result of position in the word or the sentential context.
In all cases, increased salience should draw attention to the sound itself, promoting perception-oriented attentional sets.

The first linguistic manipulation that affects attentional sets is the position of the modified /s/ in the exposure words.  
Some groups of participants will be exposed to the modified sound only at the beginnings of words, such as \emph{silver} or \emph{settlement}, and other groups will be exposed to the category only in the middle of words, such as {carousel} or \emph{fossil}.
Word-initial exposure will promote a more perception-oriented attentional set, as that position is important in linguistic theory \citep{Beckman1998} and lexical information exhibits less of an effect on that position \citep{Pitt2006}.
In contrast, word-medial exposure will promote a more comprehension-oriented attentional set, as the identity of the word will be largely known by the time the modified sound is processed.  
Lexical information has been shown to exert greater influence the later in the word a given sound is \citep{Pitt2006}.
Perception is more critical earlier in the word than later, as the set of possible words shrinks as incoming information progressively narrows the possible words.
Experiments 1 and 2 use this manipulation in Chapter~\ref{chap:lexdec}, and further background is given in Section~\ref{sec:lexicalbias} of this chapter.

The second linguistic manipulation is the context that a word appears in.  
In Experiments 1 and 2, participants will be exposed to the modified sound category in words in isolation, as in previous work \citep{Norris2003}.  
However, in Experiment 3, the words containing the sound category will be embedded in sentences that are either predictive or unpredictive of the target word.
Using sentence frames should promote comprehension-oriented attentional sets more than words in isolation, and further background is given in Section~\ref{sec:semanticpredictability}.

All three experiments contain manipulations to the explicit instructions they receive.
Participants will receive the same general instructions about the exposure task, but some groups of participants will receive additional instructions about the nature of the /s/ category, following previous studies \citep{Pitt2012}.
Without any additional instructions, the task will promote a comprehension-oriented attentional set, and the instructions about /s/ will promote a perception-oriented attentional set.
Section~\ref{sec:attention} contains background on attention and instructions.

The stimuli-based manipulation is the typicality of the modified /s/ category.
Some groups of participants will be exposed to a modified category halfway in between /s/ and /\textesh/, much like in previous work \citep{Norris2003}, and some groups will be exposed to a category that is more like /\textesh/ than /s/. 
A more atypical category will promote a perception-oriented attentional set than a more typical category.
These two types of categories can be thought of in terms of salience, where a more atypical category is perceptually more salient than 
Experiment 1 uses the more typical category in its stimuli and Experiment 2 uses the more atypical category.

The results of these experiments will be contextualized in the existing perceptual learning literature and in models of perceptual learning.
Section~\ref{sec:perceptuallearning} reviews the perceptual learning literature in greater depth, as well as conceptual models that have been proposed for it \citep{Clark2013, Kleinschmidt2011}.
Broadly speaking, perceptual learning is well accounted for by a Bayesian model, where differences between expectations and observed input cause an error signal which updates future expectations, leading to perceptual learning.
The results of the experiments in this dissertation support a more nuanced attention mechanism than proposed in \citet{Clark2013}.
The mechanism for attention proposed there is a gain-based one, where increased attention increases the weight of error signals, and predicts that increased attention should lead to increased perceptual learning.
As reviewed in Section~\ref{sec:perceptuallearning}, generalization of perceptual learning is dependent on the task and the attention set being employed.
An expanded attention mechanism is therefore proposed to unify the results of perceptual learning studies across psychophysics and speech perception and the results of the current experiments.
The proposed mechanism inhibits error propagation beyond where attention is directed.  In a perception-oriented attentional set, perceptual learning occurs at the initial perception where encoding is fine-grained, preventing generalization.  
Comprehension-oriented attentional sets allow greater error propagation and abstraction, leading to greater generalization.

The structure of the thesis is as follows.
This chapter will review the recent literature on perceptual learning in speech perception (Section~\ref{sec:perceptuallearning}), as well as literature linguistic expectations (Section~\ref{sec:linguistic}), attention (Section~\ref{sec:attention}), and category typicality (Section~\ref{sec:signal}) as they relate to perceptual learning and to the three experiments of this dissertation.
Chapter~\ref{chap:lexdec} will detail two experiments using a lexically-guided perceptual learning paradigm, each with different conditions for levels of lexical bias and attention.  
The two experiments differ in the acoustic properties of the exposure tokens, with the first experiment using a slightly atypical /s/ category that is halfway between /s/ and /\textesh/, and the second experiment using a more atypical /s/ category that is more /\textesh/-like than /s/-like.  
Chapter~\ref{chap:sent} details an experiment using a novel perceptual learning paradigm that manipulates an additional linguistic factor, namely semantic predictability, to increase the linguistic expectations during exposure.
Finally, Chapter~\ref{chap:conclusion} will summarize the results and place them in a theoretical framework.
The perceptual learning literature has generally used consistent processing conditions to elicit perceptual learning effects, and a goal of this dissertation is to examine the robustness and degree of perceptual learning across different processing conditions.

\section{Perceptual learning}
\label{sec:perceptuallearning}

Perceptual learning is a well established phenomenon in the psychology and psychophysics literature. 
Training can improve a perceiver's ability to discriminate in many disparate modalities, such as visual acuity, somatosensory spatial resolution, weight estimation, and discrimination of hue and acoustic pitch \citep[for review]{Gibson1953}. 
In this literature, perceptual learning is the improvement of a perceiver to judge the physical characteristics of objects in the world through training that assumes attention on the task, but doesn't require reinforcement, correction or reward.
This definition of perceptual learning corresponds more to what is termed ``selective adaptation'' in the speech perception literature rather than what is termed ``perceptual learning'', ``perceptual adaptation'' or ``perceptual recalibration.''  
In speech perception, selective adaptation is the phenomenon where listeners that are exposed repeatedly to a narrow distribution for a sound category, narrow their own perceptual category, resulting in a change in variance of the category, not of the mean of the category (along some acoustic-phonetic dimension) \citep{Eimas1973,Samuel1986,Vroomen2007}.
Perceptual learning or recalibration in the speech perception literature is a more broad updating of perceptual categories, either in mean or variance \citep{Norris2003, Vroomen2007}.

\citet{Norris2003} began the recent set of investigations into lexically-guided perceptual learning in speech.
\citet{Norris2003} exposed one group of Dutch listeners to a fricative halfway between /s/ and /f/ at the ends of words like \emph{olif} "olive" and \emph{radijs} "radish", while exposing another group to the ambiguous fricative at the ends of nonwords, like \emph{blif} and \emph{blis}.
Following exposure, both groups of listeners were tested on their categorization on a fricative continuum from 100\% /s/ to 100\% /f/. 
Listeners exposed to the ambiguous fricative at the end of words shifted their categorization behaviour, while those exposed to them at the end of nonwords did not.  The exposure using words was further differentiated by the bias introduced by the words.  
Half the tokens ending in the ambiguous fricative formed a word if the fricative was interpreted as /s/ but not if it was interpreted as /f/, and the others were the reverse.  
Listeners exposed only to the /s/-biased tokens categorized more of the /f/-/s/ continuum as /s/, and listeners exposed to /f/-biased tokens categorized more of the continuum as /f/.  
The ambiguous fricative was associated with either /s/ or /f/ dependent on the bias of the word, which led to an expanded category for that fricative at the expense of the other category.
These results crucially show that perceptual categories in speech are malleable to new exposure, and that the linguistic system of the listener facilitates generalization to that category in new forms and contexts.

In addition to lexically-guided perceptual learning, unambiguous visual cues to sound identity can cause perceptual learning as well; this is referred to as perceptual recalibration.
In \citet{Bertelson2003}, an auditory continuum from /aba/ to /ada/ was synthesized and paired with a video of a speaker producing /aba/ and a video of /ada/.  
Participants first completed a pretest that identified the maximally ambiguous step of the /aba/-/ada/ auditory continuum. 
In eight blocks, participants were randomly exposed to the ambiguous auditory token paired with video for /aba/ or with the video for /ada/.  Following each block, they completed a short categorization test.  
Participants showed perceptual learning effects, such that they were more likely to respond with /aba/ if they had been exposed to video of /aba/ paired with the ambiguous token in the preceding block, and likewise for /ada/.

\citet{vanLinden2007} compared the perceptual recalibration effects from the visually-guided perceptual learning paradigm \citep{Bertelson2003} to the standard lexically-guided perceptual learning paradigm \citep{Norris2003}.  
Lipread recalibration and perceptual learning effects had comparable size, lasted equally as long, were enhanced when presented with a contrasting sound, and were both unaffected by periods of silence between exposure and categorization.  
The effects for both lexically- and visually-guided paradigms did not last through prolonged testing in this study, unlike in other studies \citep{Kraljic2005,Eisner2006}.
In those studies, lexically-guided perceptual learning effects persisted through intermediate tasks, as long as the task did not involve any contradictory evidence of the trait learned \citep{Kraljic2005}, and also persisted across 12 hours \citep{Eisner2006}.

Perceptual learning in speech perception can be captured well in terms of Bayesian belief updating \citep{Kleinschmidt2011} or as part of a predictive coding model of the brain \citep{Clark2013}.  
In Bayesian belief updating, the model categorizes the incoming stimuli based on multimodel cues, and then updates the distribution to reflect that categorization.  
This updated conditional distribution is then used for future categorizations in an iterative process.  
\citet{Kleinschmidt2011} model the results of the behavioural study in \citet{Vroomen2007} in a Bayesian framework, with models fit to each participant capturing the perceptual recalibration and selective adaptation shown by the participants over the course of the experiment.  
A similar, but more broad, framework is that of predictive coding \citep{Clark2013}. 
This framework uses a hierarchical generative model that aims to minimize prediction error between bottom-up sensory inputs and top-down expectations.  
Mismatches between the top-down expectations and the bottom-up signals generate error signals that are used to modify future expectations.  
Perceptual learning then is the result of modifying expectations to match learned input.

Perceptual learning in the psychophysics literature has shown a large degree of exposure-specificity, where observers only show learning effects on the same or very similar stimuli as those they were trained on. 
As such, perceptual learning has been argued to reside or affect the early sensory pathways, where stimuli are represented with the greatest detail \citep{Gilbert2001}.  
Perceptual learning in speech perception has also shown a large degree of exposure-specificity, where participants do not generalize cues across speech sounds \citep{Reinisch2014} or across speakers unless the sounds are similar across exposure and testing \citep{Eisner2005, Kraljic2005, Kraljic2007, Reinisch2013a}.  
On the other hand, lexically-guided perceptual learning in speech has shown a greater degree of generalization than would be expected from a purely psychophysical standpoint.  
The testing stimuli are generally quite different from the exposure stimuli, with participants exposed to multisyllabic words ending in an ambiguous sound and tested on monosyllabic words \citep{Reinisch2013} and nonwords \citep{Norris2003, Kraljic2005}, though exposure-specificity is found when exposure and testing use different positional allophones \citep{Mitterer2013}.

Why is lexically-guided perceptual learning more context-general?
The experiments performed in this dissertation will provide evidence that this context-generality is the result of a listener's attentional set, which can be influenced by linguistic, attentional and signal properties.
A more comprehension-oriented attentional set, where a listener's goal is to understand the meaning of speech, promotes generalization and leads to greater perceptual learning.  
A more perception-oriented attentional set, where a listener's goal is to perceive specific qualities of a signal, does not promote generalization.
In terms of a Bayesian framework with error propagation, a more perception-oriented attentional set may keep error propagation more local, resulting in the exposure-specificity seen more in the psychophysics literature.
A more comprehension-oriented attentional set would propagate errors farther upward in the hierarchy of expectations.
In both cases, errors would propagate to where attention is focused, but larger updating associated with farther error propagation would lead to larger observed perceptual learning effects.
These attentional sets will be explored in more detail in Section~\ref{sec:attention} following an examination of the linguistic factors that will be manipulated in the experiments in Chapters~\ref{chap:lexdec} and \ref{chap:sent}, namely lexical bias and semantic predictability; both of which are hypothesized to aid the listener in adopting comprehension-oriented attentional sets.

\section{Linguistic expectations and perceptual learning}
\label{sec:linguistic}

Two linguistic manipulations on attentional sets are lexical bias, as a function of the position of the modified category in the word, and semantic predictability.
Chapter~\ref{chap:lexdec} presents two experiments using a standard lexically-guided perceptual learning paradigm, which uses lexical bias as the means to link an ambiguous sound to an unambiguous category.
In Chapter~\ref{chap:sent}, a novel, sententially-guided perceptual learning paradigm is used to promote further use of comprehension-oriented attentional sets.

\subsection{Lexical bias}
\label{sec:lexicalbias}

Lexical bias is the primary way through which perceptual learning is induced in the experimental speech perception literature.
Lexical bias, also known as the Ganong Effect, refers to the tendency for listeners to interpret a speaker's (noncanonical) production as a particular, meaningful word rather than a nonsense word.  
For instance, given a continuum from a nonword like \emph{dask} to word like \emph{task} that differs only in the initial sound, listeners in general are more likely to interpret any step along the continuum as the word endpoint rather than the nonword endpoint as compared to a continuum with no word endpoints \citep{Ganong1980}. 
This bias is exploited in perceptual learning studies to allow for noncanonical, ambiguous productions of a sound to be linked to pre-existing sound categories.
Given that ambiguous productions must be associated with a word to induce lexically-guided perceptual learning \citep{Norris2003}, differing degrees of lexical bias could lead to differing degrees of perceptual learning, a prediction which will be tested in Chapter~\ref{chap:lexdec}.  
The stronger the lexical bias, the stronger the link will be between the ambiguous sound and the sound category, as mediated by the word.

%Word length
Lexical bias has been found to vary in strength according to several factors.  
First, the length of the word in syllables has a large effect on lexical bias, with longer words showing stronger lexical bias than shorter words \citep{Pitt2006}.  
Continua formed using trisyllabic words, such as \emph{establish} and \emph{malpractice}, were found to show consistently larger lexical bias effects than monosyllabic words, such as \emph{kiss} and \emph{fish}.  
\citet{Pitt2006} also found that lexical bias from trisyllabic words was robust across experimental conditions, such as compressing the durations by up to 30\%, but lexical bias from monosyllabic words was more fragile and condition dependent.
The lexical bias effects shown by monosyllabic words only approached those of trisyllabic words when the participants were told to keep response times within a certain margin and given feedback when the response time fell outside the desired range, adding a cognitive load element to the task.
They argue that these affects arise from both the greater bottom-up information present in longer words and the greater lexical competition for shorter words.

%Position in the word
\citet{Pitt2012} used a lexical decision task with a continuum of fricatives from /s/ to /\textesh/ embedded in words differing in the position of a sibilant.  
They found that ambiguous fricatives later in the word, such as \emph{establish} or \emph{embarrass}, show greater lexical bias effects than the same ambiguous fricatives embedded earlier in the word, such as \emph{serenade} or \emph{chandelier}.
In the experiments and meta-analysis of phoneme identification results presented in \citet{Pitt1993}, they found that, for monosyllabic word frames, token-final targets produce more robust lexical bias effects than token-initial targets.
Between word length and position in the word, lexical bias appears to be strengthened over the course of the word.
As a listener hears more evidence for a particular word, their expectations for hearing the rest of that word increase.

%Samuel1981
Lexical bias has been found to affect phoneme restoration tasks as well \citep{Samuel1981}.  
In this paradigm, listeners hear words with noise added to or replacing sounds and are asked to identify which they had heard for each trial.  
Lower sensitivity to noise addition versus replacement and increased bias for responding that noise is added is indicative of phoneme restoration, where listeners are perceiving sounds not actually present in the signal.  
Several factors are identified in the study as increasing the likelihood of the phoneme restoration effect.
In the lexical domain, words are more likely than nonwords to have phoneme restorations, and this discrepancy strengthens when listeners are primed with a form without noise before the trial.  
More frequent words are also more likely to exhibit phoneme restoration effects, and longer words also show greater phoneme restoration effects.  
Position of the sound in the word also influences the decision, with non-initial positions showing greater phoneme restoration effects. 
The other influences on phoneme restoration discussed in \citet{Samuel1981}, namely the signal properties and sentential context will be discussed in subsequent sections.

Sounds at the beginnings of words show more fragile lexical bias effects than those later in the word.  In terms of salience as used in this dissertation, sounds at the beginning of words are more salient than sounds later in words, because they are used to narrow the set of potential words, and no particular expectations are available before the word begins.  As such, perception-oriented attentional sets is favored in the processing of word-initial sounds.  Lexically-guided perceptual learning typically maximizes the lexical bias at the modified category by placing it at the end of the word \citep{Norris2003}, but it has been found when the ambiguous stimuli is embedded earlier in the word, such as the onset of the final syllable \citep{Kraljic2005, Kraljic2008, Kraljic2008a} or the even the onset of the first syllable \citep{Clare2014}.  However, in light of the findings in \citet{Pitt2012}, we would expect less lexical biases the earlier in the word those ambiguous sounds were heard, and therefore, I hypothesize, lower endorsement rates and smaller perceptual learning effect sizes.  
Experiments 1 and 2  in Chapter~\ref{chap:lexdec} implement this manipulation.

\subsection{Semantic predictability}
\label{sec:semanticpredictability}

The second type of linguistic expectation manipulation used in this dissertation is known as semantic predictability \citep{Kalikow1977}.
Sentences are semantically predictable when they contain words prior to the final word that points almost definitively to the identity of that final word.  
For instance, the sentence fragment \emph{The cow gave birth to the...} from \citet{Kalikow1977} is almost guaranteed to be completed with the word \emph{calf}.  
On the other hand, a fragment like \emph{She is glad Jane called about the...} is far from having a guaranteed completion, other than having the category of noun.

Words that are predictable from the context are temporally and spectrally reduced as compared to words that are not predictable \cite{Scarborough2010, Clopper2008}. Despite this acoustic reduction, high predictability sentences are generally more intelligible.
Sentences that form a semantically coherent whole have higher word identification rates across varying signal-to-noise ratios \citep{Kalikow1977}, which has been found across children and adults \citep{Fallon2002}, and across native monolingual and early bilingual listeners, but not late bilingual listeners \citep{Mayo1997}.
Highly predictable sentences are more intelligible to native listeners in noise, even when signal enhancements are not made, though non-native listeners require both signal enhancements and high predictability together to see any benefit \cite{Bradlow2007}.
However, when words at the ends of predictive sentences are excised from their context, they tend to be less intelligible than words excised from non-predictive contexts \citep{Lieberman1963}.

Semantic predictability has been found to have similar effects on phoneme categorization as lexical bias \citep{Connine1987, Borsky1998}.  
In those studies, a continuum from one word to another, such as \emph{coat} to \emph{goat}, was embedded in a sentence frame that semantically cohered with either one of the endpoints or the other.  
Participants showed a shift in the category boundary based on the sentence frame.
If the sentence frame cued the voiced stop, more of the continuum was categorized as the voiced stop, and likewise for the voiceless.

In phoneme restoration, higher semantic predictability has been found to bias listeners toward interpreting the stimuli as intact with noise rather than replaced with noise \citep{Samuel1981}.
This increased bias towards interpreting the stimuli as an intact word was also coupled with an increase in sensitivity between the two types of stimuli, which \citet{Samuel1981} suggests is the result of a lower cognitive load in predictable contexts, and therefore greater phonetic encoding is available \citep[see also][]{Mattys2011}.

The previous literature on semantic predictability has shown largely similar effects as lexical bias in how sounds are categorized and restored.  
From this, I hypothesize that increasing the expectations for a word through semantic predictability will promote a comprehension-oriented attentional set, as perception of the modified sound category will not be strictly necessary for comprehension.
Listeners that are exposed to an /s/ category that is more /\textesh/-like only in words that are highly predictable from context should show larger perceptual learning effects than listeners exposed to the same category only in words that are unpredictable from context.
However, there may be an upper limit for listener expectations when both semantic predictability and lexical bias are high, as committing too much to a particular expectation could lead to garden path phenomena \citep{Levy2008}.
The effect of semantic predictability on perceptual learning will be explicitly tested in Chapter~\ref{chap:sent}.

\section{Attention and perceptual learning}
\label{sec:attention}

Attention is a large topic of research in its own right, and this section only reviews literature that is directly relevant to perceptual learning.
Attention has been found to have a role on perceptual learning in the psychophysics literature, indeed \citet{Gibson1953} identifies it as the sole prerequisite to perceptual learning.
For instance, \citet{Ahissar1993} found that, in general, attending to \emph{global} features for detection (i.e., discriminating different orientations of arrays of lines) does not make participants better at using \emph{local} features for detection (i.e., detection of a singleton that differs in angle in the same arrays of lines), and vice versa.  Perceptual learning, in this case, in this visual domain is limited to the task and stimuli on which a given participant was trained.  

Attentional sets can refer to the strategies that the perceiver uses to perform a task.  
For instance, in a visual search task, colour, orientation, motion and size are the predominant strategies \citep{Wolfe2004}.  
Two broad categories of attentional sets are generally used.  
Focused sets direct attention to components of the sensory input, such as seeing the trees, and diffuse sets direct attention to global properties of the sensory input, such as seeing the forest.  
In the visual search literature, an example of a focused attentional set is the "feature search mode", which gives priority to a single feature, such as the colour of the target, and an example of a diffuse attentional set is the "singleton detection mode", which gives priority to any salient features \citep{Bacon1994}. 
In the auditory streaming literature, two attentional sets have been identified as ``selective listening'', where the perceiver attempts to hear the components of two streams, and ``comprehensive listening'', where the perceiver tries to hear all components as a single stream  \citep{vanNoorden1975}.
Finally, in lexical decision tasks, the diffuse attentional set is where primary attention is on detecting words from nonwords, and the focused attentional set is where instructions direct participants attention to a potentially misleading sound \citep{Pitt2012}.
Attentional set selection is not necessarily optimal on the part of the perceiver, and it has been shown to be biased based on experience, with the amount of training performed influencing the length of time that perceivers will continue to use non-optimal sets after the task has changed \citep{Leber2006}.
In the terms of \citet{Cutler1987}, diffuse and focused attentional sets are roughly equivalent to comprehension-oriented and perception-oriented attentional sets.

Listeners can employ either of those attentional sets depending on the nature of the task.
For instance, listeners can attend to particular syllables or sounds in syllable- or phoneme-monitoring tasks \citep[and others]{Norris1988}, and even particular linguistically relevant temporal positions \citep{Pitt1990}.
However, even in these low-level, signal based tasks, lexical properties of the signal can exhibit some influence, if the stimuli are not monotonous enough to disengage comprehension \citep{Cutler1987}.  
Variation in speech in general seems to lead towards a more diffuse, comprehension-oriented attentional set, which is likely the default attentional set employed in everyday use of language, where the goal is firmly more comprehension than perception.

Attention has not been manipulated in previous work on perceptual learning in speech perception, but some work has been done on how individual differences in attention control can impact perceptual learning.
\citet{Scharenborg2014} presents a perceptual learning study of older Dutch listeners in the model of \citet{Norris2003}.  
In addition to the exposure and test phases, these older listeners completed tests for hearing loss, selective attention and attention-switching control.  
They found no evidence that perceptual learning was influenced by listeners' hearing loss or selective attention abilities, but they did find a significant relationship between a listener's attention-switching control and their perceptual learning.  
Listeners with worse attention-switching control showed greater perceptual learning effects, which the authors ascribed to an increased reliance on lexical information.  
Older listeners had previously been shown to have smaller perceptual learning effects as compared to younger listeners, but the differences were most prominent directly following exposure \citep{Scharenborg2013}.  
Younger listeners initially had a larger perceptual learning effect in the first block of testing, but the effect lessened over the subsequent blocks.  
Older listeners showed smaller initial perceptual learning effects, but no such decay.  
\citet{Scharenborg2013} also found that participants who endorsed more of the target items as words in the exposure phase showed significantly larger perceptual learning effects in the testing phase.

%Attention
The attentional set of listeners can influence the strength of lexical bias in word recognition tasks.
\citet{Pitt2012} investigated the role of attention in modulating lexical bias.  
When listeners were told that the speaker's /s/ and /\textesh/ were ambiguous and to listen carefully to ensure correct responses, they were less tolerant of noncanonical productions across all positions in the word.  
That is, participants attending to the speaker's sibilants were less likely to accept the modified production as a word than participants given no particular instructions about the sibilants.
Given that the task listeners performed was a lexical decision task, the default attentional set for the task, termed diffuse by \citet{Pitt2012}, would have attention distributed across both acoustic-phonetic and lexical domains.  Listeners given the instructions about the speaker's /s/ productions had a focused attentional set, with more weighting on the acoustic-phonetic domain than the diffuse attentional set.
Under higher cognitive load, such as performing a more difficult concurrent task, listeners show an increased lexical bias, as a result of weaker encoding of the auditory details \citep{Mattys2011}.  These results suggest that detailed encoding requires attentional resources.  In \citet{Mattys2011}, the primary task was a phoneme identification task, where a focused attentional set would likely be the default for participants, but a more diffuse attentional set, or one that weights lexical information more heavily, seems to be employed in the higher cognitive load conditions.

There is evidence that attentional sets in the visual domain become entrenched over time \cite{Leber2006}, but the fact that attention switching in older adults was a significant predictor of the size of perceptual learning effects \citep{Scharenborg2014} suggests that listeners are indeed switching sets through an experiment.
The task is oriented toward comprehension, so the primary attentional set is likely to be a diffuse one relying more on lexical information than acoustic.
Participants with worse attention-switching control would have adopted this attentional set for more of the exposure than those with better attention-switching control, and it is precisely those with worse attention-switching control that showed the larger perceptual learning effects.
The ability to switch attention to a more perception-oriented set could have allowed those listeners prevent over-generalization, leading to a smaller perceptual learning effect for participants with better attention-switching control. 

The predictive coding framework \citep{Clark2013} provides a gain-based attentional mechanism. 
In this view, attention causes greater weight to be attached to error signals, increasing their weight and their effect on expectations.  
However, as noted by \citet{Block2013}, this view of attention does not capture the full range of experimental results.  
For instance, in a texture segregation task, spatial attention to the periphery improves performance where spatial resolution is poor, but attention to central locations, where spatial resolution is high, actually harms performance \citep{Yeshurun1998}.  
This detrimental effect is an instance of missing the forest for the trees, with spatial resolution increased too much in the central locations to perceive the larger texture.  
The attentional mechanism that I propose for the predictive coding framework is one where attention is not simply gain, but is also a mechanism that affects where errors must be resolved and expectations updated.  
The more attention is oriented towards initial perception, the less general perceptual learning will be observed as a result of exposure.  
According to the mechanism proposed in \citet{Clark2013}, any increases in attention, perception-oriented or otherwise, should lead to greater perceptual learning.

In this dissertation, focusing a listener's attention on the signal through explicit instructions is hypothesized to lead to smaller perceptual learning effects.
In a focused perception-oriented attentional set, comprehension will be de-emphasized and expectations will be updated at a lower level, leading to a more exposure-specific learning that will not generalize to novel tokens as readily.
The work on phoneme restoration suggests that higher predictability sentences have a lower cognitive load than unpredictable sentences \citep{Samuel1981}, which may lead to greater encoding for the sounds to be learned, leading to larger perceptual learning effects.
However, if fewer attentional resources are required for comprehending the word, there could be more attention on perception in the predictable sentences, leading to a reduced perceptual effect.

\section{Category typicality and perceptual learning}
\label{sec:signal}

A primary finding across the perceptual learning literature is that learning effects are only found on testing items that are similar to the exposure items.  However, a less studied question is what properties of the exposure items cause different degrees of perceptual learning.  

Variability is a fundamental property of the speech signal, so sound categories must have some variance associated with them, and certain contexts can have increased degrees of variability.
\citet{Kraljic2008a} exposed participants to ambiguous sibilants between /s/ and /\textesh/ in two different contexts.  
In one, the ambiguous sibilants were intervocalic, and in the other, they occurred as part of a /str/ cluster in English words.  
Participants exposed to the ambiguous sound intervocalically showed a perceptual learning effect, while those exposed to the sibilants in /str/ environments did not.  
The sibilant in /str/ often surfaces closer to [\textesh] in many varieties of English, due to coarticulatory effects from the other consonants in the cluster, but the coarticulatory effects for merging /s/ and /\textesh/ are much weaker in intervocalic position.  
They argue that the interpretation of the ambiguous sound is done in context of the surrounding sounds, and only when the pronunciation variant is unexplainable from context is the variant learned and attributed to the speaker \citep[see also][]{Kraljic2008}.
A more /\textesh/-like /s/ category is typical in the context of the /str/ clusters, but is atypical in intervocalic position.
Interestingly, given the lack of learning present in in the /str/ context, some degree of salience is required to trigger perceptual learning.

\citet{Sumner2011} investigated whether differences in presentation order in a perceptual learning experiment led to different learning effects of the /b/-/p/ category boundary for a native French speaker of English.  
The presentation order that showed the greatest perceptual learning effects was the one where tokens started out close to what a listener would expect for the categories (English-like voice onset time for /b/ and /p/) and shifted over the course of the experiment to what the speaker's actual categories were (French-like voice onset time for /b/ and /p/), despite the fact that this presentation order is not anything like what a listener would normally encounter when interacting with a non-native speaker of English.  
The condition that mirrored the more normal course of non-native speaker pronunciation changes, starting as more French-like and ending as more English-like, did not produce significantly different behaviour than control participants.  
These results suggest that listeners constantly update their category following each successive input, rather than only relying on initial impressions.
This finding is mirrored in \citet{Vroomen2007}, where participants initially expand their category in response to a single, repeated modified input, but then entrench that category as subsequent input is the same.
The data in \citet{Vroomen2007} is modeled using a Bayesian framework with constant updating of beliefs.
However, the constantly shifting condition also had more perceptual learning than a random order of the same stimuli.
This aspect would be better captured by the exemplar model proposed by \citet{Pierrehumbert2001}, where only input similar to the learned distribution is used for updating that distribution, and input too ambiguous given learned distributions is discarded.

Additionally, in the phoneme restoration literature, the likelihood of restoring a sound increases with the similarity of that sound to the noise that replaced it \citep{Samuel1981}.  
When the replacement noise is white noise, fricatives and stops are more likely to be restored than vowels and liquids.
Acoustic signals that better match expectations are thus less likely to be noticed as atypical.

In this dissertation, the degree of typicality of the modified category is manipulated.  
In one case, the /s/ category for the speaker is maximally ambiguous between /s/ and /\textesh/, but in the other, the category is more like /\textesh/ than like /s/.
The maximally ambiguous category is hypothesized to be less salient than the more /\textesh/-like /s/ category.
This lessened salience will result in greater use of comprehension-oriented attentional sets.
I hypothesize that the more /\textesh/-like category will shift listeners' attentional sets to be more perception-oriented due to their greater atypicality, which will lead to lower perceptual learning.
As such, perceptual learning effects are predicted to be greater when the modified /s/ category is more ambiguous between the two categories than when it is closer to the expected /\textesh/ category.

\section{Current contribution}

Perceptual learning in speech perception generalizes to new forms and contexts far more than would be expected from a purely psychophysical perspective \citep{Norris2003,Gilbert2001}.
The two paradigms promote different attentional sets, with speech perception more focused on comprehension and psychophysics tasks more focused on perception.  
Indeed, visually-guided perceptual learning in speech perception, with its emphasis on perception, shows largely similar exposure-specificity effects as the psychophysics findings \citep{Reinisch2014}.
This dissertation expands the existing literature by modifying the exposure tasks to promote comprehension- or perception-oriented attentional sets.
Perceptual learning effects are hypothesized to be smaller in the conditions that promote perception-oriented attentional sets, as perception exposure tasks have shown greater exposure-specificity effects than comprehension exposure tasks.

