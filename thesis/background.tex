%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Introduction}

Listeners of a language are faced with a large degree of phonetic variability when interacting with their fellow language users.  
Speakers can have different sizes, different genders, and different backgrounds that make speech sound categories, at first blush, overlapping in distribution and hard to separate in acoustic domains.
In addition to properties of the speaker varying, a listener's attention or goals in an interaction can vary, such as listening hard to a non-native speaker or being distracted by planning upcoming utterances or by another task entirely.  
%Whether the listener has prior knowledge, or knowledge from another modality, about a speaker characteristic can affect their perception of that speaker's speech.
Despite variability on the part of both the listener and the speaker, listeners can maintain a large degree of perceptual constancy, interpreting disparate and variable productions as belonging to a single word type or sound category.
Exposure to a speaker affects a listener's perceptual system for that speaker, increasing their ability to understand that speaker or other speakers of similar background \citep{Bradlow2008}.
Broadly, perceptual learning or perceptual adaptation in the speech perception literature refers to the updating of distributions corresponding to a sound category for a particular speaker.

This dissertation investigates the interaction between linguistic, attentional and acoustic factors on perceptual learning.
The key hypothesis being tested is that the more evidence a listener has toward a specific word, be it acoustic, lexical or semantic/sentential, the stronger the link between an ambiguous sound embedded in that word and a sound category will be.
The more strong evidence that a listener accrues for a given sound category of a speaker, the more perceptual learning will take place.
This chapter will review the recent literature on perceptual learning in speech perception (Section~\ref{sec:perceptuallearning}), as well as literature on linguistic  (Section~\ref{sec:linguistic}), attentional (Section~\ref{sec:attention}), and signal (Section~\ref{sec:signal}) factors.
Chapter~\ref{chap:lexdec} will detail two experiments using a lexically-guided perceptual learning paradigm, each with manipulations to lexical bias and attention.  The two experiments differ in the acoustic properties of the exposure tokens.  Chapter~\ref{chap:sent} details an experiment using a novel perceptual learning paradigm that manipulates an additional linguistic factor, namely semantic predictability, to increase the linguistic expectations during exposure.
The perceptual learning literature has generally used consistent processing conditions to elicit perceptual learning effects, and the goal of this dissertation is to examine the robustness and degree of perceptual learning across different processing conditions.

\section{Perceptual learning}
\label{sec:perceptuallearning}


\citet{Norris2003} began the recent set of investigations into lexically-guided perceptual learning in speech.
\citet{Norris2003} exposed one group of Dutch listeners to a fricative halfway between /s/ and /f/ at the ends of words like \emph{olif} "olive" and \emph{radijs} "radish", and another group to the ambiguous fricative at the ends of nonwords, like \emph{blif} and \emph{blis}.
Following exposure, both groups of listeners were tested on their categorization on a fricative continuum from 100\% /s/ to 100\% /f/. 
Listeners exposed to the ambiguous fricative at the end of words shifted their categorization behaviour, while those exposed to them at the end of nonwords did not.  The exposure using words was further differentiated by the bias introduced by the words.  
Half the tokens ending in the ambiguous fricative formed a word if the fricative was interpreted as /s/ but not if it was interpreted as /f/, and the others were the reverse.  
Listeners exposed only to the /s/-biased tokens categorized more of the /f/-/s/ continuum as /s/, and listeners exposed to /f/-biased tokens categorized more of the continuum as /f/.  
The ambiguous fricative was associated with either /s/ or /f/ dependent on the bias of the word, which led to an expanded category for that fricative at the expense of the other category.

In addition to lexically-guided perceptual learning, unambiguous visual cues to sound identity can cause perceptual learning as well (referred to as perceptual recalibration in that literature).
In \citet{Bertelson2003}, an auditory continuum from /aba/ to /ada/ was synthesized and paired with a video of a speaker producing /aba/ and a video of /ada/.  
Participants first completed a pretest that identified the maximally ambiguous step of the /aba/-/ada/ auditory continuum. 
 In eight blocks, participants were randomly exposed to the ambiguous auditory token paired with video for /aba/ or with the video for /ada/.  Following each block, they completed a short categorization test.  
Participants showed perceptual learning effects, such that they were more likely to respond with /aba/ if they had been exposed to video of /aba/ paired with the ambiguous token in the preceding block, and likewise for /ada/.

\citet{vanLinden2007} compared the perceptual recalibration effects from the visual lipread paradigm \citep{Bertelson2003} to the standard lexically-guided perceptual learning paradigm \citep{Norris2003}.  
Lipread recalibration and perceptual learning effects had comparable size, lasted equally as long, were enhanced when presented with a contrasting sound, and were both unaffected by periods of silence between exposure and categorization.  
The effects did not last through prolonged testing in this study, unlike in other studies \citep{Kraljic2005,Eisner2006}.
In those studies, lexically-guided perceptual learning effects persisted through intermediate tasks, as long as the task did not involve any contradictory evidence of the trait learned \citep{Kraljic2005}, and also persisted across 12 hours \citep{Eisner2006}.

Perceptual learning is a well established phenomenon in the psychology and psychophysics literature. 
Training can improve a participants ability to discriminate in many disparate modalities, such as visual acuity, somatosensory spatial resolution, weight estimation, and discrimination of hue and acoustic pitch \citep[for review]{Gibson1953}. 
In this literature, perceptual learning is the improvement of a perceiver to judge the physical characteristics of objects in the world through training that assumes attention on the task, but doesn't require reinforcement, correction or reward.
This definition of perceptual learning corresponds more to what is termed ``selective adaptation'' in the speech perception literature rather than what is termed ``perceptual learning'', ``perceptual adaptation'' or ``perceptual recalibration.''  
In speech perception, selective adaptation is the phenomenon where listeners that are exposed repeatedly to a narrow distribution for a sound category, narrow their own perceptual category, resulting in a change in variance of the category, not of the mean of the category (along some acoustic-phonetic dimension) \citep{Eimas1973,Samuel1986,Vroomen2007}.
Perceptual learning or recalibration in the speech perception literature is a more broad updating of perceptual categories, either in mean or variance \citep{Norris2003, Vroomen2007}.

Perceptual learning in the psychophysics literature has shown a large degree of exposure-specificity, where observers only show learning effects on the same or very similar stimuli as those they were trained on. As such, perceptual learning has been argued to reside or affect the early sensory pathways, where stimuli are represented with the greatest detail \citep{Gilbert2001}.  Perceptual learning in speech perception has shown a large degree of exposure-specificity, where participants do not generalize cues across speech sounds \citep{Reinisch2014} or across speakers unless the sounds are similar across exposure and testing \citep{Eisner2005, Kraljic2005, Kraljic2007, Reinisch2013a}.  On the other hand, lexically-guided perceptual learning in speech has shown a greater degree of generalization than would be expected from a purely psychophysical standpoint.  The testing stimuli are generally quite different from the exposure stimuli, with participants exposed to multisyllabic words ending in an ambiguous sound and tested on monosyllabic words \citep{Reinisch2013} and nonwords \citep{Norris2003, Kraljic2005}, though exposure-specificity is found when exposure and testing use different positional allophones \citep{Mitterer2013}.

Perceptual learning can be captured well in terms of Bayesian belief updating \citep{Kleinschmidt2011} or as part of a predictive coding model of the brain \citep{Clark2013}.  
In Bayesian belief updating, the model categorizes the incoming stimuli based on multimodel cues, and then updates the distribution to reflect that categorization.  
This updated conditional distribution is then used for future categorizations in an iterative process.  
\citet{Kleinschmidt2011} model the results of the behavioural study in \citet{Vroomen2007}, with models fit to each participant capturing the perceptual recalibration and selective adaptation shown by the participants over the course of the experiment.  
A similar, but more broad, framework is that of the predictive brain \citep{Clark2013}. 
This framework uses a hierarchical generative model that aims to minimize prediction error between bottom-up sensory inputs and top-down expectations.  
Mismatches between the top-down expectations and the bottom-up signals generate error signals that are used to modify future expectations.  
Perceptual learning then is the result of modifying expectations to match learned input.

\section{Linguistic factors and perceptual learning}
\label{sec:linguistic}

The two linguistic factors manipulated in this dissertation are lexical bias and semantic predictability.  
Lexically-guided perceptual learning paradigms use lexical bias as the means to link an ambiguous sound to a sound category.
In Chapter~\ref{chap:sent}, a novel, sententially-guided perceptual learning paradigm is used to manipulate the listener's linguistic expectations in a different manner than manipulating lexical bias.

\subsection{Lexical bias}
\label{sec:lexicalbias}

Lexical bias is the primary way through which perceptual learning is induced in the experimental speech perception literature.
Lexical bias, also known as the Ganong Effect, refers to the tendency for listeners to interpret a speaker's (noncanonical) production as a particular, meaningful word rather than a nonsense word.  
For instance, given a continuum from a nonword like \emph{dask} to word like \emph{task} that differs only in one sound, listeners in general are more likely to interpret any step along the continuum as the word endpoint rather than the nonword endpoint \citep{Ganong1980}. 
This bias is exploited in perceptual learning studies to allow for noncanonical, ambiguous productions of a sound to be linked to pre-existing sound categories.
Given that ambiguous productions must be associated with a word to induce lexically-guided perceptual learning \citep{Norris2003}, differing degrees of lexical bias could lead to differing degrees of perceptual learning, a prediction which will be tested in Chapter~\ref{chap:lexdec}.  
The stronger the lexical bias, the stronger the link will be between the ambiguous sound and the sound category, as mediated by the word.

Studies looking at lexical bias generally use a phoneme categorization task, where participants identify a sound at the beginning or end of a word from two possible options that vary in one dimension.  
For instance, given a continuum from /t/ to /d/, participants are asked to identify the sound at the beginning or end as either /t/ or /d/. 
Lexical bias effects are calculated based on two continua, where words are formed at opposite ends. 
For the /t/ to /d/ continua, one would form a word at the /t/ end, such as \emph{task} and one would form a word at the other end, such as \emph{dash}.  
Lexical bias effects are then calculated from the different categorization behaviour for these two continua.

\citet{Connine1987a} established different reaction time profiles for ``perceptual'' and ``postperceptual'' processes in categorizing a continuum.  With lexical bias, response times to the end points of a /d/ to /t/ continuum show no difference whether the continuum forms a word at one end point (such as \emph{dice} to \emph{tice}) or at the other (such as \emph{dype} to \emph{type}).  However, at the boundary between /d/ and /t/, reaction times are faster when a subject responds consistent to the bias (i.e. interprets an ambiguous word \emph{?ice} as \emph{dice}).  For postperceptual processes, in this case a monetary payoff for responding either /d/ or /t/ along a continuum that has nonwords at both ends, the pattern is reversed, where reaction times were faster for responses consistent with the monetary bias at the end points of the continuum, but no such difference was found for the category boundary.  Both biases produced similar categorization patterns, such that the participants biased toward /d/, either lexically or monetarily, categorized more of the continuum as /d/, so the principle difference between the two biases was in the reaction time profile.

%Word length
Lexical bias varies in strength according to several factors.  
First, the length of the word in syllables has a large effect on lexical bias, with longer words showing stronger lexical bias than shorter words \citep{Pitt2006}.  
Continua formed using trisyllabic words, such as \emph{establish} and \emph{malpractice}, were found to show consistently large lexical bias effects than monosyllabic words, such as \emph{kiss} and \emph{fish}.  
\citet{Pitt2006} also found that lexical bias from trisyllabic words was robust across experimental conditions, but lexical bias from monosyllabic words was more fragile and condition dependent.  
They argue that these affects arise from both the greater bottom-up information present in longer words and the greater lexical competition for shorter words.

%Position in the word
\citet{Pitt2012} used a lexical decision task with a continuum of fricatives from /s/ to /\textesh/ embedded in words differing in the position of a sibilant.  
They found that ambiguous fricatives earlier in the word, such as \emph{serenade} or \emph{chandelier}, lead to greater nonword responses than the same ambiguous fricatives embedded later in a word, such as \emph{establish} or \emph{embarass}.  
In the experiments and meta-analysis of phoneme identification results presented in \citet{Pitt1993}, they found that, for monosyllabic word frames, token-final targets produce more robust lexical bias effects than token-initial targets.

In the stimuli used in \citet{Norris2003} and most other lexically-guided perceptual learning experiments, lexical bias tends to be maximized.  Exposure stimuli are multisyllabic words that end in the ambiguous sound.  Perceptual learning has also been found when the ambiguous stimuli is embedded earlier in the word, such as the onset of the final syllable \citep{Kraljic2005, Kraljic2008, Kraljic2008a} or the even the onset of the first syllable \citep{Clare2014}.  In light of the findings in \citet{Pitt2012}, we would expect different lexical biases at the point that those ambiguous sounds were heard, and therefore, I hypothesize, different endorsement rates and different perceptual learning effect sizes.  This prediction will be explicitly tested in Experiment 1 in Chapter~\ref{chap:lexdec}.

\subsection{Semantic predictability}
\label{sec:semanticpredictability}

The second type of linguistic expectation manipulation used in this dissertation is known as semantic predictability \citep{Kalikow1977}.
Sentences are semantically predictable when they contain words prior to the final word that points almost definitively to the identity of that final word.  
For instance, the sentence fragment \emph{The cow gave birth to the...} from \citet{Kalikow1977} is almost guaranteed to be completed with the word \emph{calf}.  
On the other hand, a fragment like \emph{She is glad Jane called about the...} is far from having a guaranteed completion, other than having the category of noun.

Perceptual learning paradigms for specific native speaker characteristics restrict themselves to lexical biases, but we predict compounding effects for perceptual learning, from studies done in speech production \citep{Clopper2008, Scarborough2010}.  
Studies looking at perceptual adaptation to nonnative accents have used exposure tasks that incorporate linguistic information beyond the lexical domain.  
For instance, \citet{Clarke2004} and \citet{Bradlow2008} trained listeners on foreign-accented English using sentence exposure items, and \citet{Trude2013} trained listeners on a merger of /i/ and /\textsci/ in French-accented English using a multi-sentence story.
In these studies, while the amount of linguistic information available is greater, so too are the number of characteristics that need to be learned, if the listener does not already have training or experience with the specific non-native accent.

In speech production, higher semantic predictability has been found to result in acoustically reduced word tokens.  
In \citet{Scarborough2010}, words produced in highly predictable frames tend to be shorter in duration and have less dispersed vowel realizations.  
This semantic predictability effect did not interact with neighbourhood density, a lexical factor that proxies for the amount of lexical competition a word has.  
Words with many neighbours, and therefore less lexical predictability, had longer durations and more dispersed vowel realizations.  
For both the lexical and the semantic predictability, high predictability led to less distinct word realizations, and low predictability led to more distinct word realizations, independently of each other.
In a study looking at semantic predictability across dialects, \citet{Clopper2008} found that not all dialects realize the effects of semantic predictability the same.  
For the Southern dialect of American English, the results were much the same as in \citet{Scarborough2010}, showing temporal and spectral reduction in high predictabilty environments.  
However, speakers in the Midland dialect showed no such effect, and speakers of the Northern dialect showed more extreme Northen Cities shifting in the the high predictability environment.

Despite the temporal and spectral reduction found in high predictability contexts, high predictability sentences are generally more intelligible.
Sentences that form a semantically coherent whole have higher word identification rates across varying signal-to-noise ratios \citep{Kalikow1977}, which has been found across children and adults \citep{Fallon2002}, and across native monolingual and early bilingual listeners, but not late bilingual listeners \citep{Mayo1997}.
However, when words at the ends of predictive sentences are excised from their context, they tend to be less intelligible than words excised from non-predictive contexts \citep{Lieberman1963}.
Highly predictable sentences are more intelligible to native listeners in noise, even when signal enhancements are not made, though non-native listeners require both signal enhancements and high predictability together to see any benefit \cite{Bradlow2007}.

Two studies looking at how similar lexical and bias and semantic predictability are, found conflicting results for their reaction time profiles on a phoneme categorization task \citep{Connine1987,Borsky1998}.
\citet{Connine1987} found evidence that semantic predictability operated similar to the ``postperceptual'' processes in \citet{Connine1987a}.  \citet{Borsky1998} attempted to replicate \citet{Connine1987} while removing potential confounds from the methodological design.  In contrast to \citet{Connine1987}, they used only one voicing continuum (from \emph{goat} to \emph{coat}), embedded the acoustic target in the middle of the sentence rather than at the end, and only presented one instance of each sentence to each participant rather than all continuum steps for each sentence to each participant.  The reaction time profile in their study more closely aligned to the profile of lexical bias in \citet{Connine1987a}.

The previous literature on semantic predictability has shown largely similar effects as lexical bias.  In the few instances where both are manipulated, they seem to be orthogonal effects that increase a listener's linguistic expectation.  However, those studies were done in production \citep{Scarborough2010, Clopper2008}, and perception does not necessarily mirror production.  It may be the case that there is a maximum to the linguistic expectation that a listener has for a particular utterance.  Committing too much to a particular expectation could lead to garden path phenomena \citep{Levy2008}.  The effect of semantic predictability on perceptual learning will be explicitly tested in Chapter~\ref{chap:sent}.

\section{Attentional factors and perceptual learning}
\label{sec:attention}

Attention is a large topic of research in its own right, and this review only reviews literature that is directly relevant to perceptual learning.
Attention has been found to have a role on perceptual learning in the psychophysics literature.  
For instance, \citet{Ahissar1993} found that in general, attending to global features for detection (i.e., discriminating different orientations of arrays of lines) does not make participants better at using local features for detection (i.e., detection of a singleton that differs in angle in the same arrays of lines), and vice versa.  However, there was a small degree of local detection learned from global detection, with the singleton popping out from its field as highly salient.

Attentional sets are a widely used term in the attention literature.  
In the visual domain, attentional sets can refer to the strategies that the perceiver uses to perform a task.  
For instance, in a visual search task, colour, orientation, motion and size are the predominant strategies \citep{Wolfe2004}.  
Two broad categories of attentional sets are generally used.  
Focused sets direct attention to components of the sensory input, and diffuse sets direct attention to global properties of the sensory input.  
In the visual search literature, a focused attentional is the feature search mode, which gives priority to a single feature, such as the colour of the target, and a diffuse attentional set is singleton detection mode, which gives priority to any salient features \citep{Bacon1994}. 
In the auditory streaming literature, two attentional sets have been identified as ``selective listening'', where the perceiver attempts to hear the components of two streams, and ``comprehensive listening'', where the perceiver tries to hear all components as a single stream  \citep{vanNoorden1975}.
Finally, in a lexical decision tasks, the diffuse attentional set is where primary attention is on detecting words from nonwords, and the focused attentional set is where instructions direct participants attention to a potentially misleading sound \citep{Pitt2012}.
Attentional set selection is not necessarily optimal on the part of the perceiver, and it has been shown to be biased based on experience, with the amount of training performed influencing the length of time that perceivers will continue to use non-optimal sets after the task has changed \citep{Leber2006}.  

Attention has not been manipulated in previous work on perceptual learning in speech perception, but some work has been done on how individual differences in attention control can impact perceptual learning.
\citet{Scharenborg2014} presents a perceptual learning study of older Dutch listeners in the model of \citet{Norris2003}.  
In addition to the exposure and test phases, these older listeners completed three tests for hearing loss, selective attention and attention-switching control.  
They found no evidence that perceptual learning was influenced by listeners' hearing loss or selective attention abilities, but they did find a significant relationship between a listener's attention-switching control and their perceptual learning.  
Listeners with worse attention-switching control showed greater perecptual learning effects, which the authors ascribed to an increased reliance on lexical information.  
Older listeners had previously been shown to have smaller perceptual learning effects as compared to younger listeners, but the differences were most prominent directly following exposure \citep{Scharenborg2013}.  
Younger listeners initially had a larger perceptual learning effect in the first block of testing, but the effect lessened over the subsequent blocks.  
Older listeners showed smaller initial perceptual learning effects, but no such decay.  
\citet{Scharenborg2013} also found that performance in the lexical decision task significantly affected the perceptual learning in the testing phase.

%Attention
Lexical bias is affected by attentional processing conditions.
\citet{Pitt2012} additionally investigated the role of attention in modulating lexical bias.  
When listeners were told that the speaker's /s/ and /\textesh/ were ambiguous and to listen carefully to ensure correct responses, they were less tolerant of noncanonical productions across all positions in the word.  
That is, participants attending to the speaker's sibilants were less likely to accept the modified production as a word than participants given no particular instructions about the sibilants.
Given that the task listeners performed was a lexical decision task, the default attentional set for the task, termed ``diffuse'' by \citet{Pitt2012}, would have attention distributed across both acoustic-phonetic and lexical domains.  Listeners given the instructions about the speaker's /s/ productions had a ``focused'' attentional set, with more weighting on the acoustic-phonetic domain than the ``diffuse'' attentional set.
Under higher cognitive load, such as performing a more difficult concurrent task, listeners show an increased lexical bias, as a result of weaker encoding of the auditory details \citep{Mattys2011}.  These results suggest that detailed encoding requires attentional resources.  In \citet{Mattys2011}, the primary task was a phoneme identification task, where a ``focused'' attentional set would likely be the default for participants, but a more ``diffuse'' attentional set, or one that weights lexical information more heavily, seems to be employed in the higher cognitive load conditions.

\section{Signal factors and perceptual learning}
\label{sec:signal}

A primary finding across the perceptual learning literature is that learning effects are only found on testing items that are similar to the exposure items.  However, a less studied question is what properties of the exposure items cause different degrees of perceptual learning.  

Variability is a fundamental property of the speech signal, so sound categories must have some variance associated with them, and certain contexts can have increased degrees of variability.
\citet{Kraljic2008a} exposed participants to ambiguous sibilants between /s/ and /\textesh/ in two different contexts.  
In one, the ambiguous sibilants were intervocalic, and in the other, they occurred as part of a /str/ cluster.  
Participants exposed to the ambiguous sound intervocalically showed a perceptual learning effect, while those exposed to the sibilants in /str/ environments did not.  
The sibilant in /str/ often surfaces closer to [\textesh] in many varieties of English, due to coarticulatory effects from the other consonants in the cluster, but the coarticulatory effects for merging /s/ and /\textesh/ are much weaker in intervocalic position.  
They argue that the interpretation of the ambiguous sound is done in context of the surrounding sounds, and only when the pronunciation variant is unexplainable from context is the variant learned and attributed to the speaker, see also \citet{Kraljic2008}.  
In addition to a continuum of /asi/ to /a\textesh i/, they also tested a continuum from /astri/ to /a\textesh tri/, and found comparable perceptual learning effects across both continua for those exposed to the intervocalic ambiguous sibilants, but no perceptual learning effects on either continua for the other condition, showing a context insentivity absent in other studies.

%
\citet{Sumner2011} investigated whether differences in presentation order in a perceptual learning experiment led to different learning effects of the /b/-/p/ category boundary for a native French speaker of English.  
The presentation order that showed the greatest perecptual learning effects was the one where tokens started out close to what a listener would expect for the categories (English-like voice onset time for /b/ and /p/) and shifted over the course of the experiment to what the speaker's actual categories were (French-like voice onset time for /b/ and /p/), despite the fact that this presentation order is not anything like what a listener would normally encounter when interacting with a non-native speaker of English.  
The condition that mirrored the more normal course of non-native speaker pronunciation changes, starting as more French-like and ending as more English-like, did not produce significantly different behaviour than control participants.  
One explanation for these results are that a small difference between the listener's expectations and the input provides more robust learning, and the future input uses the updated expectations for future input, allowing for greater shifts over time through smaller shifts per trial.  
In the exemplar model proposed by \citet{Pierrehumbert2002}, only input similar to the learned distribution is used for updating that distribution, and input too far away from learned distribution is discarded. <<Double check the reference>>

\section{Current contribution}

Perceptual learning effects require two important aspects to be involved in the exposure phase.  
There must be some ambiguous acoustic aspect to be learned, and there must be an unambiguous link between the ambiguous acoustics and a sound category. 
This link can be provided through sensory information, such as in the visual domain \citep{Bertelson2003}, or it can be through linguistic information, such as the lexical status of the tokens \citep{Norris2003}. 
Most of the literature within the domain of lexically-guided perceptual learning has been on the presence or absence of perceptual learning in the categorization phases, with manipulations to the categorization phase to test for generalization.  
The question that I am interested in is in the linking of ambiguous tokens to sound categories.  
Can manipulations in the exposure phase that reduce the reliability of the linking cause differences in perceptual learning?

In this dissertation,  I will induce manipulations of lexical bias and attention in Experiments 1 and 2 (Chapter~\ref{chap:lexdec}), and manipulations of sentence predictability and attention in Experiment 3 (Chapter~\ref{chap:sent}). 
Lexical bias has been shown to affect phoneme categorization tasks \citep{Ganong1980}, and can be manipulated by position of the ambiguous sound in the word and attention \citep{Pitt2012}.  
Sentence predictability, has likewise been found to affect phoneme categorization tasks similar to lexical bias \citep{Borsky1998}, and can be manipulated by the preceding words in the sentence \citep{Kalikow1977}.  
The interaction of sentence predictability with attention has not been explicitly studied, but the interaction should be similar to lexical bias, given findings that sentence predictability exerts similar effects on phone categorization as lexical bias \citep{Borsky1998}.  In both of these studies, attention will be either diffuse across the word or focused on the speaker's phonetic characteristic to be learned.  Attention to the speaker characteristic should lessen the acceptability of these productions as words as in previous work \citep{Pitt2012}, leading to less perceptual learning when comparing these participants to those with a diffuse attentional set.
